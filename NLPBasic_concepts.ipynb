{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "6c1c0f63"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luisbmonroyj/C4AG3E1_frontendVue/blob/main/NLPBasic_concepts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98d55f4a"
      },
      "source": [
        "## Paso 0: Acceso a los Datasets\n",
        "\n",
        "Antes de comenzar con el preprocesamiento y modelado, necesitamos asegurarnos de tener acceso a los dos datasets principales que utilizaremos en este ejercicio:\n",
        "\n",
        "1.  **Dataset de Reseñas de IMDb (`imdb_reviews`):**\n",
        "    *   **Fuente:** Este dataset se carga directamente desde el catálogo de TensorFlow Datasets (`tfds`). TensorFlow se encarga de descargar y preparar los datos por nosotros la primera vez que se solicita.\n",
        "    *   **Cómo lo accedemos:** Utilizamos la función `tfds.load('imdb_reviews', ...)`. Ya hemos ejecutado una celda (o la ejecutaremos si aún no lo has hecho) que descarga y carga este dataset en la variable `dataset`.\n",
        "    *   **Contenido principal:** Texto de la reseña y una etiqueta de sentimiento binaria (0 o 1).\n",
        "\n",
        "2.  **Dataset de Metadatos de Películas de IMDb (`movie_metadata.csv`):**\n",
        "    *   **Fuente:** Este dataset proviene de Kaggle (`kevalm/movie-imbd-dataset`). Lo descargamos a nuestro entorno de Colab utilizando la API de Kaggle.\n",
        "    *   **Cómo lo accedemos:** Primero, instalamos la librería `kaggle` y configuramos nuestras credenciales (idealmente guardadas en Google Drive). Luego, usamos el comando `!kaggle datasets download` para obtener el archivo zip, y finalmente lo descomprimimos. Ya hemos generado (o generaremos) las celdas para este proceso. Una vez descomprimido, el archivo `movie_metadata.csv` se carga en un DataFrame de pandas usando `pd.read_csv()`. Ya hemos ejecutado una celda que carga este archivo en la variable `df_movie_metadata`.\n",
        "    *   **Contenido principal:** Metadatos sobre películas como título, director, géneros, presupuesto, puntuación de IMDb, número de votos, etc. (pero no el texto completo de las reseñas individuales).\n",
        "\n",
        "Tener estos dos datasets disponibles en nuestras variables (`dataset` para las reseñas y `df_movie_metadata` para los metadatos) nos permite proceder con los siguientes pasos del plan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c1c0f63"
      },
      "source": [
        "### Cargar el dataset de reseñas de IMDb con TensorFlow Datasets (Paso 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4469e4b8"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Cargar el dataset de reseñas de IMDb\n",
        "# Esto descargará y preparará los datos si no se ha hecho antes.\n",
        "# Usamos split='train' y as_supervised=True para obtener (texto, etiqueta).\n",
        "try:\n",
        "    dataset, info = tfds.load('imdb_reviews', split='train', with_info=True, as_supervised=True)\n",
        "    print(\"Dataset 'imdb_reviews' cargado exitosamente en el Paso 0.\")\n",
        "    # Opcional: Mostrar información básica para confirmar\n",
        "    # print(info)\n",
        "    # print(\"\\nPrimeros 5 ejemplos del dataset:\")\n",
        "    # for example, label in dataset.take(1):\n",
        "    #     print(f\"Texto: {example.numpy()}\")\n",
        "    #     print(f\"Etiqueta: {label.numpy()}\")\n",
        "    #     print(\"-\" * 20)\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el dataset 'imdb_reviews' en el Paso 0: {e}\")\n",
        "    print(\"Asegúrate de que tienes conexión a internet y suficiente espacio, y vuelve a intentar ejecutar esta celda.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c237d8d"
      },
      "source": [
        "### Descargar dataset de Kaggle (Paso 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b265f790"
      },
      "source": [
        "# Instalar la biblioteca Kaggle (si no está instalada)\n",
        "!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e282f3e1"
      },
      "source": [
        "### Montar Google Drive y configurar credenciales de Kaggle (Paso 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1630055"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Montar Google Drive\n",
        "drive_path = '/content/drive'\n",
        "if not os.path.exists(drive_path):\n",
        "    print(\"Montando Google Drive...\")\n",
        "    drive.mount(drive_path)\n",
        "else:\n",
        "    print(\"Google Drive ya está montado.\")\n",
        "\n",
        "# Define la ruta donde tienes o donde quieres guardar tu archivo kaggle.json en Google Drive\n",
        "# Por ejemplo: '/content/drive/MyDrive/Kaggle_Credentials/kaggle.json'\n",
        "# Asegúrate de que este archivo existe en esta ubicación en tu Drive.\n",
        "kaggle_json_path_in_drive = '/content/drive/MyDrive/Universidad Distrital/Diplomado_AI/Semana1/kaggle.json'\n",
        "\n",
        "# Verifica si el archivo kaggle.json existe en la ruta especificada en Drive\n",
        "if not os.path.exists(kaggle_json_path_in_drive):\n",
        "    print(f\"Error: No se encontró el archivo kaggle.json en la ruta especificada: {kaggle_json_path_in_drive}\")\n",
        "    print(\"Por favor, asegúrate de que tu archivo kaggle.json está en esa ubicación en tu Google Drive.\")\n",
        "    # Puedes subirlo manualmente a esa carpeta en tu Drive o usar el siguiente código para subirlo AHORA a Colab y luego copiarlo a Drive\n",
        "    # from google.colab import files\n",
        "    # uploaded = files.upload()\n",
        "    # # Asumiendo que subiste 'kaggle.json'\n",
        "    # !mkdir -p $(dirname \"{kaggle_json_path_in_drive}\")\n",
        "    # !cp kaggle.json \"{kaggle_json_path_in_drive}\"\n",
        "else:\n",
        "    print(f\"Archivo kaggle.json encontrado en: {kaggle_json_path_in_drive}\")\n",
        "    # Configurar la variable de entorno KAGGLE_CONFIG_DIR\n",
        "    # Apuntamos KAGGLE_CONFIG_DIR al directorio que contiene kaggle.json\n",
        "    kaggle_config_dir = os.path.dirname(kaggle_json_path_in_drive)\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = kaggle_config_dir\n",
        "    print(f\"Variable de entorno KAGGLE_CONFIG_DIR configurada a: {os.environ.get('KAGGLE_CONFIG_DIR')}\")\n",
        "    print(\"Ahora Kaggle buscará las credenciales en tu Google Drive.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ce95d8d"
      },
      "source": [
        "### Descargar el dataset de Kaggle (Paso 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f129e0f"
      },
      "source": [
        "# Descargar el dataset usando el identificador\n",
        "# El comando buscará kaggle.json en el directorio especificado por KAGGLE_CONFIG_DIR\n",
        "!kaggle datasets download -d kevalm/movie-imbd-dataset\n",
        "\n",
        "# Listar los archivos descargados (debería ser un archivo zip en el directorio actual)\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c56865e4"
      },
      "source": [
        "### Descomprimir el archivo del dataset (Paso 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68f2565e"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Definir el nombre del archivo zip (ajustar si es diferente)\n",
        "# Esto asumirá que el archivo se descargó en el directorio actual\n",
        "zip_file_name = 'movie-imbd-dataset.zip'\n",
        "\n",
        "# Definir el directorio de destino para la descompresión\n",
        "destination_dir = 'imdb_dataset_from_kaggle'\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# Verificar si el archivo zip existe antes de intentar descomprimir\n",
        "if os.path.exists(zip_file_name):\n",
        "    # Descomprimir el archivo zip\n",
        "    with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
        "        zip_ref.extractall(destination_dir)\n",
        "\n",
        "    print(f\"Dataset descomprimido en la carpeta: {destination_dir}\")\n",
        "\n",
        "    # Listar los archivos descomprimidos\n",
        "    print(f\"Contenido de la carpeta '{destination_dir}':\")\n",
        "    !ls {destination_dir}\n",
        "else:\n",
        "    print(f\"Error: El archivo zip '{zip_file_name}' no fue encontrado en el directorio actual.\")\n",
        "    print(\"Por favor, verifica si la descarga de Kaggle fue exitosa.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99f93bcb"
      },
      "source": [
        "## Paso 1: Preparar el dataset `imdb_reviews` para entrenamiento\n",
        "\n",
        "El objetivo de este paso es convertir las reseñas de texto en un formato numérico que pueda ser utilizado por un modelo de Machine Learning. Esto implica varias sub-etapas de preprocesamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f5059cd"
      },
      "source": [
        "### Sub-etapa 1.1: Limpieza básica del texto\n",
        "\n",
        "Las reseñas de texto a menudo contienen ruido como etiquetas HTML (`<br />`), caracteres especiales, puntuación, etc., que no aportan valor para la clasificación de sentimiento e incluso pueden confundir al modelo. Vamos a realizar una limpieza básica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b448b791"
      },
      "source": [
        "import re\n",
        "import string\n",
        "import tensorflow as tf # Importamos TensorFlow para trabajar con el dataset\n",
        "\n",
        "# Asegurarnos de que el dataset esté cargado (si no lo estaba ya)\n",
        "# Si la celda anterior de carga falló o se interrumpió, ejecutar esta parte es crucial\n",
        "try:\n",
        "    dataset, info = tfds.load('imdb_reviews', split='train', with_info=True, as_supervised=True)\n",
        "    print(\"Dataset 'imdb_reviews' cargado exitosamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el dataset: {e}\")\n",
        "    print(\"Por favor, ejecuta la celda de carga del dataset 'imdb_reviews' si aún no lo has hecho o si falló.\")\n",
        "    # Si el dataset no se carga, las siguientes celdas darán error.\n",
        "    # En un escenario real, aquí podrías detener la ejecución o manejar el error de otra forma.\n",
        "\n",
        "\n",
        "# Función de limpieza de texto\n",
        "def clean_text(text):\n",
        "    # Convertir a minúsculas\n",
        "    text = tf.strings.lower(text)\n",
        "    # Eliminar etiquetas HTML (como <br />)\n",
        "    text = tf.strings.regex_replace(text, '<br />', ' ')\n",
        "    # Eliminar puntuación y mantener solo letras, números y espacios\n",
        "    text = tf.strings.regex_replace(text, '[%s]' % re.escape(string.punctuation), '')\n",
        "    # Eliminar números (opcional, a veces los números pueden ser relevantes)\n",
        "    # text = tf.strings.regex_replace(text, '[0-9]+', '')\n",
        "    # Eliminar espacios extra\n",
        "    text = tf.strings.regex_replace(text, '\\\\s+', ' ')\n",
        "    # Eliminar espacios al inicio y final\n",
        "    text = tf.strings.strip(text)\n",
        "    return text\n",
        "\n",
        "# Aplicar la función de limpieza a los ejemplos del dataset\n",
        "# Usaremos map para aplicar la función a cada elemento del dataset de TensorFlow\n",
        "dataset_cleaned = dataset.map(lambda text, label: (clean_text(text), label))\n",
        "\n",
        "# Mostrar algunos ejemplos limpios para verificar\n",
        "print(\"\\nPrimeros 5 ejemplos del dataset después de la limpieza:\")\n",
        "for example, label in dataset_cleaned.take(5):\n",
        "    print(f\"Texto limpio: {example.numpy()}\")\n",
        "    print(f\"Etiqueta: {label.numpy()}\")\n",
        "    print(\"-\" * 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "425741a7"
      },
      "source": [
        "### Sub-etapa 1.1: Limpieza básica del texto, Stemming y Lematización\n",
        "\n",
        "Las reseñas de texto a menudo contienen ruido como etiquetas HTML (`<br />`), caracteres especiales, puntuación, etc., que no aportan valor para la clasificación de sentimiento e incluso pueden confundir al modelo. Además, palabras con la misma raíz pero diferentes terminaciones (como \"corriendo\", \"corre\", \"corrió\") pueden tratarse como la misma palabra base para reducir la dimensionalidad y capturar mejor el significado. Para esto, usamos **Stemming** y **Lematización**.\n",
        "\n",
        "*   **Limpieza básica:** Eliminará ruido irrelevante.\n",
        "*   **Stemming:** Reduce las palabras a su raíz o \"stem\" (ej: \"running\" -> \"run\"). Es un proceso más rudimentar que corta sufijos.\n",
        "*   **Lematización:** Reduce las palabras a su forma base o \"lema\" (ej: \"running\" -> \"run\", \"better\" -> \"good\"). Es un proceso más sofisticado que considera el vocabulario y el análisis morfológico para llegar a la forma base correcta de una palabra."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7463991f"
      },
      "source": [
        "### Sub-etapa 1.1.1: Instalar NLTK y descargar recursos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b51b1179"
      },
      "source": [
        "# Instalar NLTK\n",
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8d180f3"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# Descargar recursos necesarios para Stemming y Lematización\n",
        "nltk.download('punkt') # Para tokenización\n",
        "nltk.download('wordnet') # Para lematización\n",
        "nltk.download('omw-1.4') # Open Multilingual Wordnet (complemento para wordnet)\n",
        "nltk.download('averaged_perceptron_tagger') # Para identificar la parte del habla (útil para lematización)\n",
        "nltk.download('punkt_tab') # Recurso adicional para tokenización, necesario según el error\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Recurso específico para POS tagging en inglés, necesario según el último error\n",
        "\n",
        "# NOTA: Ejecuta las líneas de nltk.download() una vez si no tienes los recursos.\n",
        "# Las dejo comentadas para evitar descargas repetidas si ya las hiciste."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca5de619"
      },
      "source": [
        "### Sub-etapa 1.1.2: Aplicar Stemming y Lematización (en una muestra)\n",
        "\n",
        "Vamos a demostrar cómo aplicar Stemming y Lematización. Ten en cuenta que aplicar esto directamente a todo el dataset de TensorFlow de manera eficiente requiere envolver las funciones de NLTK para que funcionen con `tf.py_function` o procesar el dataset de otra forma (por lotes convertidos a NumPy/listas). Para este ejemplo, lo haremos en una pequeña muestra para entender el concepto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78276323"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Inicializar Stemmer y Lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Helper function to get the part of speech tag for lemmatization\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN) # Default to Noun if tag not found\n",
        "\n",
        "\n",
        "# Ejemplo de texto limpio (tomado de la salida anterior)\n",
        "sample_text_bytes = b'this was an absolutely terrible movie dont be lured in by christopher walken or michael ironside both are great actors but this must simply be their worst role in history even their great acting could not redeem this movies ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the columbian rebels were making their cases for revolutions maria conchita alonso appeared phony and her pseudolove affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actors like christopher walkens good name i could barely sit through it'\n",
        "\n",
        "# Decodificar el texto de bytes a string\n",
        "sample_text = sample_text_bytes.decode('utf-8')\n",
        "\n",
        "print(f\"Texto original (limpio): {sample_text}\")\n",
        "\n",
        "# Tokenizar el texto\n",
        "tokens = word_tokenize(sample_text)\n",
        "print(f\"\\nTokens: {tokens[:20]}...\") # Mostrar los primeros tokens\n",
        "\n",
        "# Aplicar Stemming\n",
        "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "print(f\"\\nTokens después de Stemming: {stemmed_tokens[:20]}...\") # Mostrar los primeros tokens\n",
        "\n",
        "# Aplicar Lematización\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
        "print(f\"\\nTokens después de Lematización: {lemmatized_tokens[:20]}...\") # Mostrar los primeros tokens\n",
        "\n",
        "# Nota técnica:\n",
        "# - Stemming es más rápido pero menos preciso (solo corta).\n",
        "# - Lematización es más lento pero más preciso (usa vocabulario y POS).\n",
        "# - Para grandes datasets en TF, considera usar tf.py_function con NLTK o bibliotecas nativas de TF si están disponibles para estas tareas.\n",
        "# - Puedes combinar la limpieza básica con Stemming/Lematización en una sola función de procesamiento."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edc2ff8b"
      },
      "source": [
        "### Sub-etapa 1.1.3: Visualizar diferencias con Word Clouds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c15c53d"
      },
      "source": [
        "# Instalar la librería wordcloud\n",
        "!pip install wordcloud matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17c3e8d2"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # Necesario para manejar el texto en bytes de tf.data\n",
        "\n",
        "# Usaremos el mismo sample_text_bytes de la celda anterior para demostrar\n",
        "# sample_text_bytes ya está definido en la celda anterior (78276323)\n",
        "# Si ejecutas esta celda por separado, asegúrate de que sample_text_bytes esté definido.\n",
        "\n",
        "# Decodificar el texto de bytes a string (limpio, sin stemming/lemmatization aún)\n",
        "sample_text_cleaned = sample_text_bytes.decode('utf-8')\n",
        "\n",
        "# --- Aplicar Stemming y preparar texto para Word Cloud ---\n",
        "# Asegurarnos de que stemmer y word_tokenize estén inicializados (si no se han ejecutado antes)\n",
        "try:\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = word_tokenize(sample_text_cleaned)\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "    text_stemmed = \" \".join(stemmed_tokens)\n",
        "except NameError:\n",
        "     print(\"Stemmer o tokenizer no inicializados. Ejecuta la celda 1.1.2 primero.\")\n",
        "     text_stemmed = \"\" # Evitar error si no se puede generar\n",
        "\n",
        "\n",
        "# --- Aplicar Lematización y preparar texto para Word Cloud ---\n",
        "# Asegurarnos de que lemmatizer, word_tokenize, wordnet y nltk.pos_tag estén disponibles\n",
        "try:\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    # get_wordnet_pos function is defined in cell 78276323\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
        "    text_lemmatized = \" \".join(lemmatized_tokens)\n",
        "except NameError:\n",
        "    print(\"Lemmatizer o recursos de NLTK no inicializados. Ejecuta la celda 1.1.2 primero.\")\n",
        "    text_lemmatized = \"\" # Evitar error si no se puede generar\n",
        "except LookupError:\n",
        "    print(\"Recursos de NLTK para lematización no descargados. Ejecuta la celda 1.1.1 primero.\")\n",
        "    text_lemmatized = \"\"\n",
        "\n",
        "\n",
        "# --- Generar Word Clouds ---\n",
        "\n",
        "# Configuración básica para las word clouds\n",
        "wordcloud_config = {\n",
        "    'width': 800,\n",
        "    'height': 400,\n",
        "    'background_color': 'black', # Cambiado a oscuro\n",
        "    'max_words': 100,\n",
        "    'contour_color': 'steelblue'\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Word Cloud para Texto Limpio\n",
        "if sample_text_cleaned:\n",
        "    wordcloud_cleaned = WordCloud(**wordcloud_config).generate(sample_text_cleaned)\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(wordcloud_cleaned, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Texto Limpio')\n",
        "else:\n",
        "     plt.subplot(1, 3, 1)\n",
        "     plt.text(0.5, 0.5, 'No data', horizontalalignment='center', verticalalignment='center')\n",
        "     plt.axis('off')\n",
        "     plt.title('Texto Limpio (Error)')\n",
        "\n",
        "\n",
        "# Word Cloud para Stemming\n",
        "if text_stemmed:\n",
        "    wordcloud_stemmed = WordCloud(**wordcloud_config).generate(text_stemmed)\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(wordcloud_stemmed, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Texto con Stemming')\n",
        "else:\n",
        "     plt.subplot(1, 3, 2)\n",
        "     plt.text(0.5, 0.5, 'No data', horizontalalignment='center', verticalalignment='center')\n",
        "     plt.axis('off')\n",
        "     plt.title('Texto con Stemming (Error)')\n",
        "\n",
        "\n",
        "# Word Cloud para Lematización\n",
        "if text_lemmatized:\n",
        "    wordcloud_lemmatized = WordCloud(**wordcloud_config).generate(text_lemmatized)\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(wordcloud_lemmatized, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Texto con Lematización')\n",
        "else:\n",
        "     plt.subplot(1, 3, 3)\n",
        "     plt.text(0.5, 0.5, 'No data', horizontalalignment='center', verticalalignment='center')\n",
        "     plt.axis('off')\n",
        "     plt.title('Texto con Lematización (Error)')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Recomendación/Hack:\n",
        "# Las Word Clouds son excelentes para visualizaciones rápidas de la frecuencia de palabras.\n",
        "# Notarás que el stemming puede crear \"palabras\" que no son reales (ej. 'movi' en lugar de 'movie'),\n",
        "# mientras que la lematización intenta mantener la forma base real ('movie').\n",
        "# Esto ilustra la diferencia de precisión entre las dos técnicas."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eea2a58"
      },
      "source": [
        "### Sub-etapa 1.2: Tokenización y Vectorización (Usando TF-IDF)\n",
        "\n",
        "Después de limpiar y normalizar el texto (aplicando Lematización), el siguiente paso es dividir las reseñas en palabras individuales (Tokenización) y luego convertir estas palabras en vectores numéricos que puedan ser entendidos por un modelo. Una técnica común es TF-IDF (Frecuencia de Término - Frecuencia Inversa de Documento).\n",
        "\n",
        "**Tokenización:** El proceso de dividir una secuencia de texto en piezas más pequeñas llamadas tokens (normalmente palabras).\n",
        "**Vectorización (TF-IDF):** Asigna un peso a cada token basado en qué tan frecuentemente aparece en una reseña (Frecuencia de Término) y qué tan raro es el término en todo el conjunto de reseñas (Frecuencia Inversa de Documento). Esto ayuda a resaltar las palabras que son más importantes para una reseña específica en comparación con el corpus general."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4704de53"
      },
      "source": [
        "# Nota: Para aplicar la limpieza y lematización a todo el dataset de TF de manera eficiente,\n",
        "# necesitaríamos integrar las funciones de NLTK con tf.py_function o usar alternativas nativas de TF/Keras.\n",
        "# Para simplificar este ejemplo y continuar con la vectorización, vamos a simular\n",
        "# que tenemos un conjunto de textos limpios y lematizados listos para vectorizar.\n",
        "\n",
        "# En un flujo de trabajo completo con TF.data, aplicarías las funciones de limpieza/lematización\n",
        "# usando dataset.map() con tf.py_function o Keras Preprocessing layers (si usas una capa de lematización\n",
        "# personalizada o si usas Keras's built-in text processing which might not have native lemmatization).\n",
        "\n",
        "# --- Simulación de textos limpios y lematizados ---\n",
        "# Tomamos una muestra pequeña del dataset limpio/lematizado si ya ejecutaste esa parte,\n",
        "# o usamos los textos de ejemplo de la Sub-etapa 1.1.2 para demostrar.\n",
        "\n",
        "# Si ya tienes dataset_cleaned de la celda b448b791, puedes usarlo:\n",
        "# sample_texts = [example.numpy().decode('utf-8') for example, label in dataset_cleaned.take(10)]\n",
        "\n",
        "# Si no, usamos los textos de ejemplo de la sub-etapa 1.1.2 (celda 78276323):\n",
        "sample_text_cleaned_demo = b'this was an absolutely terrible movie dont be lured in by christopher walken or michael ironside both are great actors but this must simply be their worst role in history even their great acting could not redeem this movies ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the columbian rebels were making their cases for revolutions maria conchita alonso appeared phony and her pseudolove affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actors like christopher walkens good name i could barely sit through it'.decode('utf-8')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import numpy as np # Import numpy for array handling\n",
        "import pandas as pd # ¡Importar pandas!\n",
        "\n",
        "# Asegurarse de que los recursos de NLTK estén descargados si no lo están\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "    nltk.data.find('corpora/omw-1.4')\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "    # nltk.data.find('tokenizers/punkt_tab') # Optional based on previous errors\n",
        "    # nltk.data.find('taggers/averaged_perceptron_tagger_eng') # Optional based on previous errors\n",
        "except LookupError:\n",
        "    print(\"Recursos de NLTK no encontrados. Por favor, ejecuta la celda de descarga de recursos de NLTK (Sub-etapa 1.1.1) primero.\")\n",
        "\n",
        "\n",
        "# Helper function to get the part of speech tag for lemmatization\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN) # Default to Noun if tag not found\n",
        "\n",
        "# Combinar limpieza y lematización en una función (aplicada a un string)\n",
        "def clean_and_lemmatize(text):\n",
        "    # Limpieza básica (similar a la anterior pero para string de Python)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenización\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Lematización\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
        "\n",
        "    return \" \".join(lemmatized_tokens)\n",
        "\n",
        "\n",
        "# Aplicar la limpieza y lematización a una muestra de textos (convertidos a lista de strings)\n",
        "# Para este ejemplo, tomamos 10 textos del dataset original, los convertimos a string, limpiamos y lematizamos.\n",
        "# NOTA: Aplicar NLTK a todo el dataset de TF de esta forma NO es eficiente. Es solo para demostración.\n",
        "sample_texts_raw = [example.numpy().decode('utf-8') for example, label in dataset.take(10)] # Usamos el dataset original\n",
        "sample_texts_processed = [clean_and_lemmatize(text) for text in sample_texts_raw]\n",
        "\n",
        "\n",
        "# --- Vectorización usando TF-IDF ---\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Inicializar el vectorizador TF-IDF\n",
        "# max_features limita el vocabulario para manejar el tamaño\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000) # Considerar las 5000 palabras más frecuentes/importantes\n",
        "\n",
        "# Ajustar el vectorizador a los textos procesados y transformar los textos en una matriz TF-IDF\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(sample_texts_processed)\n",
        "\n",
        "# Obtener los nombres de las características (palabras)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Mostrar la matriz TF-IDF (para una muestra pequeña)\n",
        "print(\"\\nMatriz TF-IDF (primeros 10 textos, primeras 20 características):\")\n",
        "# Convertir la matriz dispersa a densa para mostrar (SOLO PARA MUESTRAS PEQUEÑAS)\n",
        "tfidf_matrix_dense = tfidf_matrix.toarray()\n",
        "display(pd.DataFrame(tfidf_matrix_dense[:10, :20], columns=feature_names[:20]))\n",
        "\n",
        "# En un escenario real, trabajarías con la matriz dispersa para ahorrar memoria.\n",
        "\n",
        "print(f\"\\nForma de la matriz TF-IDF (Número de textos, Número de características/palabras): {tfidf_matrix.shape}\")\n",
        "print(f\"Número de características (palabras únicas en el vocabulario limitado): {len(feature_names)}\")\n",
        "\n",
        "# Recomendación:\n",
        "# Para el dataset completo de IMDb (~25000 reseñas), TF-IDF con un vocabulario grande\n",
        "# puede generar una matriz muy grande. Considera usar un max_features razonable.\n",
        "# Alternativas como CountVectorizer también son posibles, pero TF-IDF suele dar mejores resultados\n",
        "# porque pondera la importancia de las palabras.\n",
        "# Embeddings de palabras (Word2Vec, GloVe, o de modelos como BERT) son una alternativa\n",
        "# más moderna y potente a TF-IDF para capturar significado semántico. Las exploraremos después."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e589d2b6"
      },
      "source": [
        "### Sub-etapa 1.1.4: Eliminar Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3bb285d"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# Descargar la lista de stop words\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "print(\"Lista de Stop words de NLTK descargada.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66317275"
      },
      "source": [
        "### Sub-etapa 1.1.5: Integrar Limpieza, Lematización y Eliminación de Stop Words\n",
        "\n",
        "Ahora vamos a modificar nuestra función de limpieza para incluir la eliminación de stop words y asegurarnos de que la lematización se aplique sobre los tokens que no son stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98b2a2e1"
      },
      "source": [
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk # Asegurarse de que nltk esté importado para get_wordnet_pos y descargas\n",
        "\n",
        "# Asegurarse de que el dataset esté cargado\n",
        "try:\n",
        "    dataset, info = tfds.load('imdb_reviews', split='train', with_info=True, as_supervised=True)\n",
        "    print(\"Dataset 'imdb_reviews' cargado exitosamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el dataset: {e}\")\n",
        "    print(\"Por favor, ejecuta la celda de carga del dataset 'imdb_reviews' si aún no lo has hecho o si falló.\")\n",
        "\n",
        "# Asegurarse de que los recursos de NLTK y stop words estén descargados si no lo están\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "    nltk.data.find('corpora/omw-1.4')\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    # Opcionales según errores anteriores:\n",
        "    # nltk.data.find('tokenizers/punkt_tab')\n",
        "    # nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n",
        "except LookupError:\n",
        "    print(\"Recursos de NLTK o stop words no encontrados. Por favor, ejecuta las celdas de descarga.\")\n",
        "\n",
        "\n",
        "# Obtener la lista de stop words en inglés\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Inicializar Lemmatizer y POS tagger (recursos ya descargados)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# get_wordnet_pos function is defined in a previous cell (78276323),\n",
        "# assuming it's available in the global scope or define it here again for clarity\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
        "    # Requires 'averaged_perceptron_tagger' and 'punkt' resources\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN) # Default to Noun if tag not found\n",
        "\n",
        "\n",
        "# Función de limpieza, tokenización, eliminación de stop words y lematización\n",
        "# Esta función trabajará en un string de Python, no directamente en un tf.Tensor\n",
        "# Para aplicarla a tf.data.Dataset, necesitaríamos usar tf.py_function\n",
        "def clean_tokenize_lemmatize_stopwords(text):\n",
        "    # Decodificar si es necesario (si el input es bytes)\n",
        "    if isinstance(text, tf.Tensor):\n",
        "        text = text.numpy().decode('utf-8')\n",
        "    elif isinstance(text, bytes):\n",
        "         text = text.decode('utf-8')\n",
        "\n",
        "    # Limpieza básica\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenización\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Eliminar Stop Words y Lematizar\n",
        "    processed_tokens = []\n",
        "    for word in tokens:\n",
        "        if word not in stop_words: # Eliminar stop words\n",
        "            # Aplicar lematización solo a palabras no stop words\n",
        "            lemma = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
        "            processed_tokens.append(lemma)\n",
        "\n",
        "    return \" \".join(processed_tokens)\n",
        "\n",
        "# --- Aplicar esta función a una muestra del dataset para demostración ---\n",
        "# NOTA: Aplicar NLTK/Python functions a TF.data.Dataset es más eficiente usando tf.py_function\n",
        "# o procesando por lotes. Esto es solo para demostrar la función.\n",
        "sample_texts_raw_demo = [example.numpy() for example, label in dataset.take(5)] # Tomar 5 ejemplos en bytes\n",
        "sample_texts_processed_demo = [clean_tokenize_lemmatize_stopwords(text) for text in sample_texts_raw_demo]\n",
        "\n",
        "print(\"Primeros 5 ejemplos después de limpieza, eliminación de stop words y lematización:\")\n",
        "for i, text in enumerate(sample_texts_processed_demo):\n",
        "    print(f\"Ejemplo {i+1}: {text}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "# Recomendación para usar con tf.data.Dataset:\n",
        "# Envuelve la función clean_tokenize_lemmatize_stopwords en tf.py_function\n",
        "# para aplicarla eficientemente al dataset completo:\n",
        "# dataset_processed = dataset.map(lambda text, label: (tf.py_function(func=clean_tokenize_lemmatize_stopwords, inp=[text], Tout=tf.string), label))\n",
        "# Sin embargo, tf.py_function puede ser más lento que las operaciones nativas de TF/Keras.\n",
        "# Para pipelines eficientes en TF, considera usar Keras Preprocessing layers si es posible,\n",
        "# aunque la lematización avanzada con POS tagging como la de NLTK no está nativamente en Keras."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e510211e"
      },
      "source": [
        "### Sub-etapa 1.3: Manejo de Datos Faltantes en `df_movie_metadata`\n",
        "\n",
        "El dataset de metadatos de Kaggle (`df_movie_metadata`) contiene valores faltantes (`NaN`) que necesitamos abordar antes de poder utilizarlo plenamente en los pasos posteriores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0968cf4"
      },
      "source": [
        "### Identificar valores faltantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b592abb"
      },
      "source": [
        "import pandas as pd # Asegurarse de que pandas esté importado\n",
        "\n",
        "# Inicializar df_movie_metadata a None para evitar NameError si no se carga previamente\n",
        "df_movie_metadata = None\n",
        "\n",
        "# Asegurarnos de que df_movie_metadata esté cargado\n",
        "try:\n",
        "    # Intentar usar la variable existente si ya fue cargada en una celda anterior\n",
        "    # Verificamos si la variable existe en el ámbito global y si es un DataFrame\n",
        "    if 'df_movie_metadata' in globals() and isinstance(df_movie_metadata, pd.DataFrame):\n",
        "         print(\"Utilizando el DataFrame df_movie_metadata cargado previamente.\")\n",
        "    else:\n",
        "        # Si la variable no existe o no es un DataFrame, intentar cargar el archivo CSV\n",
        "        print(\"La variable df_movie_metadata no fue encontrada o no es un DataFrame. Intentando cargar el archivo CSV.\")\n",
        "        csv_file_path_metadata = '/content/imdb_dataset_from_kaggle/movie_metadata.csv'\n",
        "        df_movie_metadata = pd.read_csv(csv_file_path_metadata)\n",
        "        print(f\"Dataset '{csv_file_path_metadata}' cargado exitosamente.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: El archivo CSV '{csv_file_path_metadata}' no fue encontrado.\")\n",
        "    print(\"Por favor, verifica la ruta y asegúrate de haber descomprimido el dataset de Kaggle.\")\n",
        "    df_movie_metadata = None # Asegurarse de que es None si la carga falla\n",
        "except Exception as e:\n",
        "    print(f\"Error inesperado al cargar df_movie_metadata: {e}\")\n",
        "    df_movie_metadata = None\n",
        "\n",
        "\n",
        "if df_movie_metadata is not None:\n",
        "    # Contar valores faltantes por columna\n",
        "    missing_values_count = df_movie_metadata.isnull().sum()\n",
        "\n",
        "    # Mostrar las columnas con valores faltantes (filtrando las que tienen 0)\n",
        "    missing_values_count = missing_values_count[missing_values_count > 0]\n",
        "\n",
        "    print(\"\\nNúmero de valores faltantes por columna en df_movie_metadata:\")\n",
        "    display(missing_values_count.sort_values(ascending=False))\n",
        "\n",
        "    # Opcional: Mostrar el porcentaje de valores faltantes\n",
        "    print(\"\\nPorcentaje de valores faltantes por columna:\")\n",
        "    missing_values_percent = (df_movie_metadata.isnull().sum() / len(df_movie_metadata)) * 100\n",
        "    missing_values_percent = missing_values_percent[missing_values_percent > 0]\n",
        "    display(missing_values_percent.sort_values(ascending=False))\n",
        "else:\n",
        "    print(\"\\nNo se pudo cargar el DataFrame df_movie_metadata. No se puede proceder con el análisis de valores faltantes.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ce1a0cd"
      },
      "source": [
        "### Manejar valores faltantes: Eliminar filas con datos nulos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a87f528"
      },
      "source": [
        "# Asegurarnos de que df_movie_metadata esté cargado\n",
        "try:\n",
        "    # Intentar usar la variable existente si ya fue cargada\n",
        "    print(\"Utilizando el DataFrame df_movie_metadata cargado previamente.\")\n",
        "except NameError:\n",
        "    # Si la variable no existe, intentar cargar el archivo CSV nuevamente\n",
        "    print(\"La variable df_movie_metadata no fue encontrada. Intentando cargar el archivo CSV.\")\n",
        "    try:\n",
        "        csv_file_path_metadata = '/content/imdb_dataset_from_kaggle/movie_metadata.csv'\n",
        "        df_movie_metadata = pd.read_csv(csv_file_path_metadata)\n",
        "        print(f\"Dataset '{csv_file_path_metadata}' cargado exitosamente.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: El archivo CSV '{csv_file_path_metadata}' no fue encontrado.\")\n",
        "        print(\"Por favor, verifica la ruta y asegúrate de haber descomprimido el dataset de Kaggle.\")\n",
        "        df_movie_metadata = None # Establecer como None para evitar errores posteriores\n",
        "\n",
        "if df_movie_metadata is not None:\n",
        "    print(f\"Tamaño original del DataFrame: {df_movie_metadata.shape}\")\n",
        "\n",
        "    # Eliminar filas que contengan *al menos un* valor faltante\n",
        "    df_movie_metadata_cleaned = df_movie_metadata.dropna()\n",
        "\n",
        "    print(f\"Tamaño del DataFrame después de eliminar filas con valores faltantes: {df_movie_metadata_cleaned.shape}\")\n",
        "\n",
        "    # Mostrar cuántas filas fueron eliminadas\n",
        "    rows_dropped = df_movie_metadata.shape[0] - df_movie_metadata_cleaned.shape[0]\n",
        "    print(f\"Número de filas eliminadas debido a valores faltantes: {rows_dropped}\")\n",
        "\n",
        "    # Mostrar si aún quedan valores faltantes (debería ser 0)\n",
        "    print(\"\\nVerificación de valores faltantes después de la eliminación:\")\n",
        "    display(df_movie_metadata_cleaned.isnull().sum().sum()) # Suma total de NaN en todo el DataFrame\n",
        "\n",
        "    # --- Alternativa (comentada): Imputación ---\n",
        "    # Si en lugar de eliminar, quisieras imputar, podrías hacer algo como esto:\n",
        "    # df_movie_metadata_imputed = df_movie_metadata.copy() # Trabajar en una copia\n",
        "    # # Imputar columnas numéricas con la mediana\n",
        "    # for col in df_movie_metadata_imputed.select_dtypes(include=np.number).columns:\n",
        "    #     if df_movie_metadata_imputed[col].isnull().any():\n",
        "    #         median_val = df_movie_metadata_imputed[col].median()\n",
        "    #         df_movie_metadata_imputed[col].fillna(median_val, inplace=True)\n",
        "    # # Imputar columnas categóricas con la moda (ejemplo básico, puede requerir más lógica)\n",
        "    # for col in df_movie_metadata_imputed.select_dtypes(include='object').columns:\n",
        "    #      if df_movie_metadata_imputed[col].isnull().any():\n",
        "    #          mode_val = df_movie_metadata_imputed[col].mode()[0] # mode() puede devolver múltiples valores\n",
        "    #          df_movie_metadata_imputed[col].fillna(mode_val, inplace=True)\n",
        "    # print(\"\\n(Alternativa) Verificación de valores faltantes después de imputación (en df_movie_metadata_imputed):\")\n",
        "    # display(df_movie_metadata_imputed.isnull().sum().sum())\n",
        "\n",
        "\n",
        "    # Ahora, continuaremos trabajando con df_movie_metadata_cleaned para los siguientes pasos\n",
        "    df_movie_metadata = df_movie_metadata_cleaned # Reemplazamos el DataFrame original con la versión limpia\n",
        "    print(\"\\nEl DataFrame 'df_movie_metadata' ahora contiene solo filas sin valores faltantes.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8e5bc41"
      },
      "source": [
        "### Sub-etapa 1.2.1: Tokenización y Estadísticas\n",
        "\n",
        "Después de la limpieza básica y lematización, el siguiente paso es tokenizar el texto y analizar algunas estadísticas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34b71f7f"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import collections # Para contar la frecuencia de los tokens\n",
        "import numpy as np # Importar numpy\n",
        "import pandas as pd # Importar pandas para mostrar resultados en tabla\n",
        "\n",
        "# Asegurar que los recursos de NLTK y stop words estén descargados\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "    nltk.data.find('corpora/omw-1.4')\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    # Optional based on previous errors:\n",
        "    # nltk.data.find('tokenizers/punkt_tab')\n",
        "    # nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n",
        "except LookupError:\n",
        "    print(\"Recursos de NLTK o stop words no encontrados. Ejecuta las celdas de descarga de la Sub-etapa 1.1.1 y 1.1.4.\")\n",
        "    # Esto no detiene la ejecución, pero la función clean_tokenize_lemmatize_stopwords fallará si no se descargan.\n",
        "\n",
        "\n",
        "# Asegurar que el dataset esté cargado\n",
        "try:\n",
        "    # Si la variable 'dataset' ya existe, la usamos.\n",
        "    print(\"Utilizando el dataset 'imdb_reviews' cargado previamente.\")\n",
        "except NameError:\n",
        "    # Si no, intentamos cargarlo.\n",
        "    print(\"Dataset 'imdb_reviews' no encontrado. Intentando cargarlo.\")\n",
        "    try:\n",
        "        dataset, info = tfds.load('imdb_reviews', split='train', with_info=True, as_supervised=True)\n",
        "        print(\"Dataset 'imdb_reviews' cargado exitosamente.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el dataset: {e}\")\n",
        "        print(\"Por favor, ejecuta la celda de carga del dataset 'imdb_reviews' (Paso 0).\")\n",
        "        dataset = None # Asegurar que dataset es None si la carga falla\n",
        "\n",
        "if dataset is not None:\n",
        "    # Obtener la lista de stop words en inglés\n",
        "    try:\n",
        "      stop_words = set(stopwords.words('english'))\n",
        "    except LookupError:\n",
        "      print(\"Recurso 'stopwords' de NLTK no encontrado. Ejecuta la celda de descarga de stop words (Sub-etapa 1.1.4).\")\n",
        "      stop_words = set() # Usar un set vacío para evitar errores si no se descargan\n",
        "\n",
        "\n",
        "    # Inicializar Lemmatizer y POS tagger\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    # Redefinir la función get_wordnet_pos para asegurar que esté disponible y maneje errores de recursos\n",
        "    def get_wordnet_pos(word):\n",
        "        \"\"\"Map POS tag to first character used by WordNetLemmatizer, handles LookupError\"\"\"\n",
        "        try:\n",
        "            tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "            tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "            return tag_dict.get(tag, wordnet.NOUN) # Default to Noun\n",
        "        except LookupError:\n",
        "            # Si falta el recurso 'averaged_perceptron_tagger', retornar Noun por defecto\n",
        "            print(\"Recurso 'averaged_perceptron_tagger' de NLTK no encontrado. La lematización será menos precisa.\")\n",
        "            return wordnet.NOUN\n",
        "        except IndexError:\n",
        "            # Manejar casos donde pos_tag no devuelve la estructura esperada (ej. palabra vacía)\n",
        "            return wordnet.NOUN\n",
        "\n",
        "\n",
        "    # Función de limpieza, tokenización, eliminación de stop words y lematización (para un string)\n",
        "    def clean_tokenize_lemmatize_stopwords(text_input):\n",
        "        # Si el input es un tensor de TF (como cuando se itera con .batch(1)),\n",
        "        # extraer el valor numpy y decodificar el primer (y único) elemento.\n",
        "        if isinstance(text_input, tf.Tensor):\n",
        "             # Asegurar que el tensor no esté vacío y contenga bytes\n",
        "             if tf.size(text_input) > 0 and text_input.dtype == tf.string:\n",
        "                 # Acceder al elemento (batch de tamaño 1) y decodificar\n",
        "                 text = text_input.numpy()[0].decode('utf-8', errors='ignore') # Usar errors='ignore' para evitar problemas con caracteres\n",
        "             else:\n",
        "                 return [] # Retornar lista vacía si el tensor está vacío o no es string\n",
        "        elif isinstance(text_input, bytes):\n",
        "             text = text_input.decode('utf-8', errors='ignore')\n",
        "        else: # Asumir que ya es un string\n",
        "             text = str(text_input) # Convertir a string explícitamente\n",
        "\n",
        "        # Limpieza básica\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'<br />', ' ', text)\n",
        "        text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Tokenización\n",
        "        try:\n",
        "            tokens = word_tokenize(text)\n",
        "        except LookupError:\n",
        "            print(\"Recurso 'punkt' de NLTK no encontrado. No se realizará tokenización.\")\n",
        "            return [] # Retornar lista vacía si falta el recurso de tokenización\n",
        "\n",
        "        # Eliminar Stop Words y Lematizar\n",
        "        processed_tokens = []\n",
        "        for word in tokens:\n",
        "            # Verificar si word es una cadena no vacía antes de procesar\n",
        "            if word and word not in stop_words: # Eliminar stop words\n",
        "                # Aplicar lematización solo a palabras no stop words\n",
        "                # get_wordnet_pos maneja su propio LookupError ahora\n",
        "                lemma = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
        "                processed_tokens.append(lemma)\n",
        "\n",
        "        return processed_tokens # Retornamos una lista de tokens\n",
        "\n",
        "    # --- Procesar el dataset completo (¡puede tardar!) ---\n",
        "    # NOTA: Aplicar NLTK/Python functions a un TF.data.Dataset completo de esta forma (iterando)\n",
        "    # NO es la forma más eficiente para pipelines de entrenamiento en TF.\n",
        "    # Para entrenamiento, idealmente usarías tf.py_function o Keras Preprocessing layers.\n",
        "    # Hacemos esto aquí solo para obtener las estadísticas globales.\n",
        "\n",
        "    all_tokens = []\n",
        "    total_reviews = 0\n",
        "\n",
        "    print(\"\\nProcesando dataset y recolectando tokens (esto puede tardar varios minutos)...\")\n",
        "\n",
        "    # Iterar sobre el dataset. Usamos .batch(1) para obtener tensores de tamaño 1.\n",
        "    # Convertir a numpy() para usar funciones Python/NLTK\n",
        "    # Aumentar el tamaño del lote para una ligera mejora de eficiencia al iterar\n",
        "    # Aunque para tf.py_function o Keras layers se usarían lotes más grandes.\n",
        "    batch_size_for_stats = 32 # Procesar 32 reseñas a la vez para las estadísticas\n",
        "\n",
        "    # Usar as_numpy_iterator() para iterar más eficientemente sobre lotes convertidos a numpy\n",
        "    for text_batch, label_batch in dataset.batch(batch_size_for_stats).as_numpy_iterator():\n",
        "        # text_batch es un array numpy de bytes strings\n",
        "        for text_bytes in text_batch:\n",
        "            tokens = clean_tokenize_lemmatize_stopwords(text_bytes) # Pasar los bytes directamente\n",
        "            all_tokens.extend(tokens)\n",
        "\n",
        "        total_reviews += len(text_batch) # Sumar el tamaño del lote al total\n",
        "        if total_reviews % 1000 == 0:\n",
        "            print(f\"Procesadas {total_reviews} reseñas...\")\n",
        "\n",
        "    print(f\"\\nProcesamiento completo. Total de reseñas procesadas: {total_reviews}\")\n",
        "\n",
        "    # --- Calcular Estadísticas ---\n",
        "    total_tokens = len(all_tokens)\n",
        "    unique_tokens = len(set(all_tokens))\n",
        "    token_counts = collections.Counter(all_tokens) # Frecuencia de cada token\n",
        "\n",
        "    print(f\"\\nEstadísticas del Dataset después de Limpieza, Lematización y Eliminación de Stop Words:\")\n",
        "    print(f\"Número total de tokens: {total_tokens}\")\n",
        "    print(f\"Número de tokens únicos (tamaño del vocabulario): {unique_tokens}\")\n",
        "\n",
        "    print(\"\\nTop 20 tokens más comunes:\")\n",
        "    display(pd.DataFrame(token_counts.most_common(20), columns=['Token', 'Frecuencia']))\n",
        "\n",
        "    # Recomendación: El tamaño del vocabulario es un factor clave para la vectorización.\n",
        "    # Un vocabulario muy grande puede hacer que las matrices TF-IDF sean enormes.\n",
        "    # Podemos limitar el vocabulario a las N palabras más frecuentes en el siguiente paso.\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo procesar el dataset porque no fue cargado exitosamente.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30ccc6f0"
      },
      "source": [
        "### Sub-etapa 1.2.2: Visualizar términos con alto TF-IDF en reseñas de muestra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1589fde2"
      },
      "source": [
        "# Usaremos el tfidf_vectorizer que ajustamos previamente (en la celda 4704de53)\n",
        "# sobre una muestra de textos procesados.\n",
        "\n",
        "# Si la variable tfidf_vectorizer no está definida (por ejemplo, si reiniciaste\n",
        "# el entorno sin ejecutar la celda 4704de53), necesitarás ejecutarla primero.\n",
        "# También necesitamos la función clean_and_lemmatize de la celda 4704de53\n",
        "# y el dataset 'dataset' de la celda 4469e4b8.\n",
        "\n",
        "# Asegurarnos de que las variables necesarias estén disponibles\n",
        "try:\n",
        "    tfidf_vectorizer\n",
        "    clean_and_lemmatize\n",
        "    dataset\n",
        "except NameError:\n",
        "    print(\"Advertencia: Variables 'tfidf_vectorizer', 'clean_and_lemmatize', o 'dataset' no encontradas.\")\n",
        "    print(\"Por favor, ejecuta las celdas correspondientes (Paso 0 y Sub-etapa 1.2) para definirlas.\")\n",
        "    # En un escenario real, aquí podrías detener la ejecución o recargar/redefinir las variables.\n",
        "\n",
        "\n",
        "# --- Seleccionar algunas reseñas de muestra ---\n",
        "# Tomamos las mismas 10 reseñas que usamos para ajustar el vectorizador\n",
        "sample_texts_raw_for_tfidf_viz = [example.numpy().decode('utf-8') for example, label in dataset.take(10)]\n",
        "sample_texts_processed_for_tfidf_viz = [clean_and_lemmatize(text) for text in sample_texts_raw_for_tfidf_viz]\n",
        "\n",
        "# --- Transformar las reseñas de muestra usando el vectorizador TF-IDF ajustado ---\n",
        "# Usamos transform, no fit_transform, porque el vectorizador ya fue ajustado\n",
        "tfidf_matrix_sample = tfidf_vectorizer.transform(sample_texts_processed_for_tfidf_viz)\n",
        "\n",
        "# --- Obtener los nombres de las características (palabras) ---\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# --- Visualizar los términos con TF-IDF más alto para cada reseña de muestra en tabla y gráfico ---\n",
        "\n",
        "print(\"\\nTérminos con mayor peso TF-IDF para algunas reseñas de muestra:\")\n",
        "\n",
        "# Seleccionar un número limitado de reseñas para mostrar\n",
        "num_reviews_to_show = 5\n",
        "num_top_terms = 10 # Mostrar los 10 términos con mayor TF-IDF por reseña\n",
        "\n",
        "# Aplicar un estilo oscuro a los gráficos\n",
        "plt.style.use('dark_background')\n",
        "\n",
        "for i in range(min(num_reviews_to_show, tfidf_matrix_sample.shape[0])):\n",
        "    print(f\"\\n--- Reseña de Muestra {i+1} ---\")\n",
        "    # Obtener el vector TF-IDF para la reseña actual\n",
        "    review_tfidf_vector = tfidf_matrix_sample[i]\n",
        "\n",
        "    # Convertir el vector disperso a un array denso para facilitar la indexación\n",
        "    review_tfidf_array = review_tfidf_vector.toarray().flatten()\n",
        "\n",
        "    # Obtener los índices de los términos con mayor peso TF-IDF\n",
        "    # Usamos argpartition para eficiencia, seguido de argsort para ordenar\n",
        "    top_term_indices = np.argpartition(review_tfidf_array, -num_top_terms)[-num_top_terms:]\n",
        "    # Ordenar los índices por el valor de TF-IDF descendente\n",
        "    top_term_indices = top_term_indices[np.argsort(-review_tfidf_array[top_term_indices])]\n",
        "\n",
        "    # Crear una lista de diccionarios para el DataFrame\n",
        "    top_terms_data = []\n",
        "    for index in top_term_indices:\n",
        "        term = feature_names[index]\n",
        "        tfidf_score = review_tfidf_array[index]\n",
        "        top_terms_data.append({'Término': term, 'Peso TF-IDF': tfidf_score})\n",
        "\n",
        "    # Crear y mostrar el DataFrame\n",
        "    df_top_terms = pd.DataFrame(top_terms_data)\n",
        "    display(df_top_terms)\n",
        "\n",
        "    # Crear y mostrar el gráfico de barras\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(df_top_terms['Término'], df_top_terms['Peso TF-IDF'])\n",
        "    plt.ylabel('Peso TF-IDF')\n",
        "    plt.title(f'Top {num_top_terms} Términos con mayor TF-IDF en Reseña {i+1}')\n",
        "    plt.xticks(rotation=45, ha='right') # Rotar etiquetas para mejor lectura\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Recomendación:\n",
        "# Nota cómo las palabras con TF-IDF más alto tienden a ser palabras clave\n",
        "# que describen el contenido específico o el sentimiento de esa reseña,\n",
        "# y no palabras comunes como \"the\", \"is\", \"and\" (porque las eliminamos\n",
        "# y porque IDF las penaliza)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a76afc4c"
      },
      "source": [
        "### Sub-etapa 1.4: Exploración de Temas y Extracción de Palabras Clave\n",
        "\n",
        "Antes de la vectorización final y el modelado de sentimiento, vamos a explorar los temas y términos más importantes en el dataset de reseñas de IMDb utilizando LDA y técnicas de extracción de palabras clave."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "071047ae"
      },
      "source": [
        "### Sub-etapa 1.4.1: Instalar librerías para Modelado de Temas y Extracción de Palabras Clave"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac0eca5e"
      },
      "source": [
        "# Instalar gensim para LDA\n",
        "!pip install gensim\n",
        "\n",
        "# Instalar una implementación de RAKE\n",
        "!pip install rake-nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c52951e2"
      },
      "source": [
        "### Sub-etapa 1.4.2: Preparar datos para LDA y RAKE\n",
        "\n",
        "LDA y RAKE generalmente trabajan con texto tokenizado (listas de palabras) más que con las representaciones TF-IDF directamente. Usaremos el resultado de nuestra limpieza y tokenización."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55dc36db"
      },
      "source": [
        "# Necesitamos los tokens limpios y lematizados para cada reseña.\n",
        "# Si ya ejecutaste la celda 34b71f7f (Tokenización y Estadísticas),\n",
        "# deberías tener 'all_tokens' que es una lista plana de todos los tokens.\n",
        "# Para LDA y RAKE, necesitamos los tokens POR DOCUMENTO.\n",
        "\n",
        "# Vamos a re-procesar una muestra del dataset para obtener los tokens por documento.\n",
        "# NOTA: Para el dataset completo, este paso puede ser intensivo en memoria si no se hace de forma eficiente.\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import numpy as np # Importar numpy\n",
        "import pandas as pd # Importar pandas\n",
        "\n",
        "# Asegurar que los recursos de NLTK y stop words estén descargados\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "    nltk.data.find('corpora/omw-1.4')\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    print(\"Recursos de NLTK o stop words no encontrados. Ejecuta las celdas de descarga de la Sub-etapa 1.1.1 y 1.1.4.\")\n",
        "\n",
        "# Asegurar que el dataset esté cargado\n",
        "try:\n",
        "    dataset, info = tfds.load('imdb_reviews', split='train', with_info=True, as_supervised=True)\n",
        "    print(\"Dataset 'imdb_reviews' cargado exitosamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el dataset: {e}\")\n",
        "    print(\"Por favor, ejecuta la celda de carga del dataset 'imdb_reviews' (Paso 0).\")\n",
        "    dataset = None\n",
        "\n",
        "if dataset is not None:\n",
        "    # Obtener la lista de stop words en inglés\n",
        "    try:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "    except LookupError:\n",
        "        print(\"Recurso 'stopwords' de NLTK no encontrado.\")\n",
        "        stop_words = set()\n",
        "\n",
        "    # Inicializar Lemmatizer y POS tagger\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    def get_wordnet_pos(word):\n",
        "        try:\n",
        "            tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "            tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "            return tag_dict.get(tag, wordnet.NOUN)\n",
        "        except LookupError:\n",
        "             # print(\"Recurso 'averaged_perceptron_tagger' de NLTK no encontrado.\")\n",
        "             return wordnet.NOUN\n",
        "        except IndexError:\n",
        "             return wordnet.NOUN\n",
        "\n",
        "    # Función de limpieza, tokenización, eliminación de stop words y lematización (retorna lista de tokens)\n",
        "    def clean_tokenize_lemmatize_stopwords_list(text_input):\n",
        "        if isinstance(text_input, tf.Tensor):\n",
        "             if tf.size(text_input) > 0 and text_input.dtype == tf.string:\n",
        "                 text = text_input.numpy()[0].decode('utf-8', errors='ignore')\n",
        "             else:\n",
        "                 return []\n",
        "        elif isinstance(text_input, bytes):\n",
        "             text = text_input.decode('utf-8', errors='ignore')\n",
        "        else:\n",
        "             text = str(text_input)\n",
        "\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'<br />', ' ', text)\n",
        "        text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        try:\n",
        "            tokens = word_tokenize(text)\n",
        "        except LookupError:\n",
        "            print(\"Recurso 'punkt' de NLTK no encontrado.\")\n",
        "            return []\n",
        "\n",
        "        processed_tokens = []\n",
        "        for word in tokens:\n",
        "            if word and word not in stop_words:\n",
        "                lemma = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
        "                processed_tokens.append(lemma)\n",
        "\n",
        "        return processed_tokens # Retorna LISTA de tokens\n",
        "\n",
        "\n",
        "    # --- Procesar una MUESTRA del dataset para LDA/RAKE (para evitar problemas de memoria/tiempo iniciales) ---\n",
        "    # Podemos aumentar el tamaño de la muestra si es necesario.\n",
        "    sample_size_for_topic_modeling = 1000 # Procesar 1000 reseñas para el modelado de temas\n",
        "\n",
        "    processed_documents_tokens = []\n",
        "    print(f\"\\nProcesando una muestra de {sample_size_for_topic_modeling} reseñas para Modelado de Temas y Extracción de Keywords...\")\n",
        "\n",
        "    # Iterar sobre una muestra del dataset\n",
        "    for text_tensor, _ in dataset.take(sample_size_for_topic_modeling).batch(1):\n",
        "         tokens = clean_tokenize_lemmatize_stopwords_list(text_tensor)\n",
        "         if tokens: # Solo agregar si la lista de tokens no está vacía\n",
        "             processed_documents_tokens.append(tokens)\n",
        "\n",
        "    print(f\"Procesamiento de muestra completo. Número de documentos procesados: {len(processed_documents_tokens)}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo procesar el dataset para modelado de temas porque no fue cargado exitosamente.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d872c242"
      },
      "source": [
        "### Sub-etapa 1.4.3: Modelado de Temas con LDA (Gensim)\n",
        "\n",
        "Aplicaremos LDA a nuestra muestra de reseñas procesadas para descubrir los temas ocultos. Esto requiere crear un diccionario y un corpus en formato Bag-of-Words que `gensim` pueda usar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "672c0d09"
      },
      "source": [
        "from gensim import corpora, models\n",
        "\n",
        "# Asegurarnos de que processed_documents_tokens esté disponible\n",
        "try:\n",
        "    processed_documents_tokens\n",
        "    print(f\"Número de documentos procesados para LDA: {len(processed_documents_tokens)}\")\n",
        "except NameError:\n",
        "    print(\"La variable 'processed_documents_tokens' no fue encontrada.\")\n",
        "    print(\"Por favor, ejecuta la celda de preparación de datos para LDA/RAKE (Sub-etapa 1.4.2) primero.\")\n",
        "    processed_documents_tokens = [] # Inicializar como vacío para evitar errores\n",
        "\n",
        "if processed_documents_tokens:\n",
        "    # Crear un diccionario a partir de los documentos procesados\n",
        "    # El diccionario mapea cada palabra única a un ID\n",
        "    dictionary = corpora.Dictionary(processed_documents_tokens)\n",
        "\n",
        "    # Opcional: Filtrar palabras raras o muy comunes\n",
        "    # NoFilter(no_below=5, no_above=0.5, keep_n=100000) # Ejemplo: ignorar palabras que aparecen en <5 docs o >50% docs\n",
        "    # dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
        "    # print(f\"Tamaño del vocabulario después de filtrar: {len(dictionary)}\")\n",
        "\n",
        "\n",
        "    # Crear el corpus en formato Bag-of-Words (BoW)\n",
        "    # Para cada documento, crea una lista de tuplas (word_id, word_frequency)\n",
        "    corpus_bow = [dictionary.doc2bow(doc) for doc in processed_documents_tokens]\n",
        "\n",
        "    print(f\"\\nCorpus BoW creado con {len(corpus_bow)} documentos.\")\n",
        "\n",
        "    # --- Entrenar el modelo LDA ---\n",
        "    # Especificar el número de temas\n",
        "    num_topics = 5 # Podemos ajustar este número\n",
        "\n",
        "    print(f\"\\nEntrenando modelo LDA con {num_topics} temas...\")\n",
        "\n",
        "    # Entrenar el modelo LDA\n",
        "    lda_model = models.LdaMulticore(corpus_bow,\n",
        "                                   num_topics=num_topics,\n",
        "                                   id2word=dictionary,\n",
        "                                   passes=10,     # Número de pasadas por el corpus durante el entrenamiento\n",
        "                                   workers=2)     # Número de procesos para entrenamiento paralelo (ajustar según CPU)\n",
        "\n",
        "    print(\"Entrenamiento LDA completo.\")\n",
        "\n",
        "    # --- Mostrar los temas encontrados ---\n",
        "    print(f\"\\nTemas identificados ({num_topics}):\")\n",
        "    # lda_model.print_topics(num_words=10) # Muestra los 10 términos más importantes para cada tema\n",
        "    for topic_id, topic_words in lda_model.print_topics(num_words=10):\n",
        "        print(f\"Tema #{topic_id + 1}: {topic_words}\")\n",
        "\n",
        "\n",
        "    # Recomendación:\n",
        "    # - El número de temas (num_topics) es un hiperparámetro. A menudo se prueba con diferentes valores\n",
        "    #   y se evalúa la coherencia de los temas o se usa métricas como la coherencia del modelo.\n",
        "    # - Los temas son distribuciones de palabras. Las palabras con mayor probabilidad en un tema\n",
        "    #   son las que definen ese tema.\n",
        "    # - Interpretar los temas requiere mirar las palabras principales y darles un nombre coherente.\n",
        "\n",
        "else:\n",
        "     print(\"\\nNo se pudo entrenar LDA porque no hay documentos procesados disponibles.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0abc94a0"
      },
      "source": [
        "### Reinstalando librerías para resolver conflicto de versiones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92e98e74"
      },
      "source": [
        "# Desinstalar versiones potencialmente conflictivas (opcional pero a veces ayuda)\n",
        "#!pip uninstall -y gensim numpy scipy\n",
        "\n",
        "# Reinstalar gensim y sus dependencias\n",
        "# Pip debería encontrar versiones compatibles con el entorno\n",
        "#!pip install gensim numpy scipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6d33674"
      },
      "source": [
        "**NOTA:** Después de ejecutar esta celda, **reinicia el entorno de ejecución** nuevamente (`Runtime -> Restart runtime`). Es crucial reiniciar después de reinstalar librerías para que los cambios surtan efecto y se carguen las versiones correctas sin conflictos.\n",
        "\n",
        "Luego, ejecuta las celdas en orden desde el principio (Paso 0), incluyendo las descargas de NLTK, y finalmente la celda del Modelo LDA (`672c0d09`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e2313c9"
      },
      "source": [
        "### Sub-etapa 1.4.4: Visualizar Temas de LDA con Word Clouds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dede33c"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd # Asegurarse de que pandas esté importado\n",
        "\n",
        "# Asegurarse de que lda_model y dictionary estén disponibles\n",
        "try:\n",
        "    lda_model\n",
        "    dictionary\n",
        "except NameError:\n",
        "    print(\"Advertencia: Las variables 'lda_model' o 'dictionary' no fueron encontradas.\")\n",
        "    print(\"Por favor, ejecuta la celda del modelo LDA (Sub-etapa 1.4.3) y las anteriores (Sub-etapa 1.4.2) para definirlas.\")\n",
        "    # Si las variables no existen, no podemos generar las word clouds\n",
        "\n",
        "\n",
        "if 'lda_model' in globals() and 'dictionary' in globals():\n",
        "    print(\"Generando Word Clouds para cada tema de LDA...\")\n",
        "\n",
        "    # Obtener los temas como listas de palabras y sus pesos\n",
        "    # lda_model.show_topics() devuelve una lista de tuplas (topic_id, topic_words)\n",
        "    # topic_words es un string como '\"word1\" * prob1 + \"word2\" * prob2 + ...'\n",
        "    all_topics = lda_model.show_topics(num_topics=lda_model.num_topics, num_words=20, formatted=False)\n",
        "\n",
        "    # Aplicar el estilo oscuro si ya fue configurado (en la celda 1589fde2)\n",
        "    # Si no, puedes descomentar la siguiente línea:\n",
        "    # plt.style.use('dark_background')\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(15, 8)) # Ajustar tamaño general de la figura\n",
        "\n",
        "    # Generar una Word Cloud para cada tema\n",
        "    for i, topic in all_topics:\n",
        "        # Convertir la lista de tuplas (word, probability) a un diccionario para WordCloud\n",
        "        topic_dict = dict(topic)\n",
        "\n",
        "        # Crear la Word Cloud\n",
        "        # Usamos background_color='black' explícitamente si el estilo oscuro no se aplica globalmente\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='black', colormap='viridis').generate_from_frequencies(topic_dict)\n",
        "\n",
        "        # Mostrar la Word Cloud\n",
        "        plt.subplot(2, (lda_model.num_topics + 1) // 2, i + 1) # Organizar en filas/columnas\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Tema #{i+1}')\n",
        "\n",
        "    plt.tight_layout() # Ajustar el layout para evitar solapamiento\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nWord Clouds de temas LDA generadas.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudieron generar las Word Clouds. Asegúrate de que el modelo LDA se entrenó correctamente.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73b0857e"
      },
      "source": [
        "### Sub-etapa 1.4.4: Modelado de Temas con LDA - Iteración 1 (con Filtrado de Vocabulario)\n",
        "\n",
        "En esta iteración, vamos a mejorar los resultados del modelado de temas aplicando un filtrado al diccionario para eliminar palabras excesivamente frecuentes o raras que pueden dominar los temas. También estableceremos un `seed` para reproducibilidad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9834bf1"
      },
      "source": [
        "# from gensim import corpora, models # No usaremos gensim en esta iteración\n",
        "from sklearn.feature_extraction.text import CountVectorizer # Para Bag-of-Words en scikit-learn\n",
        "from sklearn.decomposition import LatentDirichletAllocation # Implementación de LDA en scikit-learn\n",
        "import numpy as np # Necesario para np.array\n",
        "import pandas as pd # Para mostrar resultados en tabla\n",
        "\n",
        "# Necesitamos los tokens limpios y lematizados por documento.\n",
        "# Asegurarnos de que processed_documents_tokens esté disponible (de la celda 55dc36db)\n",
        "try:\n",
        "    processed_documents_tokens\n",
        "    print(f\"Número de documentos procesados para LDA: {len(processed_documents_tokens)}\")\n",
        "except NameError:\n",
        "    print(\"La variable 'processed_documents_tokens' no fue encontrada.\")\n",
        "    print(\"Por favor, ejecuta la celda de preparación de datos para LDA/RAKE (Sub-etapa 1.4.2) primero.\")\n",
        "    processed_documents_tokens = [] # Inicializar como vacío para evitar errores\n",
        "\n",
        "\n",
        "if processed_documents_tokens:\n",
        "    # --- Preparar datos para scikit-learn LDA ---\n",
        "    # scikit-learn LDA requiere una matriz de conteos (Bag-of-Words)\n",
        "    # Necesitamos unir los tokens de cada documento en un string para CountVectorizer\n",
        "    processed_documents_strings = [\" \".join(doc) for doc in processed_documents_tokens]\n",
        "\n",
        "    # Inicializar y ajustar el CountVectorizer\n",
        "    # Podemos aplicar filtrado similar al de gensim aquí\n",
        "    # min_df: Mínima frecuencia de documento (análogo a no_below)\n",
        "    # max_df: Máxima frecuencia de documento (análogo a no_above)\n",
        "    # max_features: Limitar el vocabulario (análogo a keep_n)\n",
        "    count_vectorizer = CountVectorizer(min_df=5, max_df=0.5, max_features=100000)\n",
        "\n",
        "    # Ajustar el vectorizador al corpus y transformar los documentos en una matriz de conteos\n",
        "    corpus_counts = count_vectorizer.fit_transform(processed_documents_strings)\n",
        "\n",
        "    # Obtener los nombres de las características (palabras/tokens)\n",
        "    feature_names_counts = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "    print(f\"\\nCorpus de conteos creado con forma: {corpus_counts.shape}\")\n",
        "    print(f\"Tamaño del vocabulario después de CountVectorizer: {len(feature_names_counts)}\")\n",
        "\n",
        "\n",
        "    # --- Entrenar el modelo LDA con scikit-learn ---\n",
        "    num_topics_sklearn = 5 # Mismo número de temas\n",
        "    lda_random_state = 42 # Establecer random_state para reproducibilidad en scikit-learn\n",
        "\n",
        "    print(f\"\\nEntrenando modelo LDA (scikit-learn) con {num_topics_sklearn} temas y random_state={lda_random_state}...\")\n",
        "\n",
        "    # Inicializar el modelo LDA de scikit-learn\n",
        "    # n_components: Número de temas\n",
        "    # random_state: Para reproducibilidad (¡funciona en scikit-learn!)\n",
        "    # learning_method='batch' o 'online': Batch es más preciso para datasets pequeños/medianos\n",
        "    # max_iter: Número de iteraciones\n",
        "    lda_model_sklearn = LatentDirichletAllocation(n_components=num_topics_sklearn,\n",
        "                                                  random_state=lda_random_state,\n",
        "                                                  learning_method='batch',\n",
        "                                                  max_iter=20) # Reducir iteraciones para velocidad inicial\n",
        "\n",
        "    # Entrenar el modelo LDA\n",
        "    # Esto puede tardar un poco\n",
        "    lda_model_sklearn.fit(corpus_counts)\n",
        "\n",
        "    print(\"Entrenamiento LDA (scikit-learn) completo.\")\n",
        "\n",
        "    # --- Mostrar los temas encontrados con el modelo scikit-learn ---\n",
        "    print(f\"\\nTemas identificados ({num_topics_sklearn}) con scikit-learn LDA:\")\n",
        "\n",
        "    # La salida de scikit-learn LDA es una matriz components_ (temas x palabras)\n",
        "    # donde cada fila es una distribución de palabras para un tema.\n",
        "    # Los valores son proporciones logarítmicas. Exponenciarlas da las probabilidades.\n",
        "\n",
        "    # Función para mostrar los principales términos por tema\n",
        "    def display_topics_sklearn(model, feature_names, no_top_words):\n",
        "        for topic_idx, topic in enumerate(model.components_):\n",
        "            print(f\"Tema #{topic_idx + 1}:\")\n",
        "            # Obtener los índices de las palabras más probables y sus nombres\n",
        "            top_words_indices = topic.argsort()[:-no_top_words - 1:-1]\n",
        "            top_words = [feature_names[i] for i in top_words_indices]\n",
        "            # Opcional: mostrar probabilidades (requiere exponenciar y normalizar)\n",
        "            # top_probs = np.exp(topic[top_words_indices]) / np.sum(np.exp(topic))\n",
        "            # print(\" \".join([f\"{word} ({prob:.2f})\" for word, prob in zip(top_words, top_probs)]))\n",
        "            print(\" \".join(top_words))\n",
        "            print(\"-\" * 20)\n",
        "\n",
        "    display_topics_sklearn(lda_model_sklearn, feature_names_counts, 10) # Mostrar top 10 palabras por tema\n",
        "\n",
        "\n",
        "    # Recomendación:\n",
        "    # - Compara estos temas con los de gensim. Deberían ser más estables y quizás más interpretables con el filtrado.\n",
        "    # - Puedes ajustar min_df, max_df en CountVectorizer y n_components, max_iter en LatentDirichletAllocation.\n",
        "\n",
        "else:\n",
        "     print(\"\\nNo se pudo entrenar LDA porque no hay documentos procesados disponibles.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83739a95"
      },
      "source": [
        "### Sub-etapa 1.4.5: Visualizar Temas de LDA (Filtrado) con Word Clouds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7080774"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd # Asegurarse de que pandas esté importado\n",
        "import numpy as np # Necesario para manejar arrays\n",
        "\n",
        "# Usaremos el modelo y diccionario/vectorizador de scikit-learn de la celda anterior (e9834bf1)\n",
        "try:\n",
        "    lda_model_sklearn # El modelo LDA de scikit-learn\n",
        "    feature_names_counts # Los nombres de las características del CountVectorizer\n",
        "except NameError:\n",
        "    print(\"Advertencia: Las variables 'lda_model_sklearn' o 'feature_names_counts' no fueron encontradas.\")\n",
        "    print(\"Por favor, ejecuta la celda de Modelado de Temas LDA - Iteración 1 (Sub-etapa 1.4.4, celda e9834bf1) primero.\")\n",
        "    # Si las variables no existen, no podemos generar las word clouds\n",
        "\n",
        "\n",
        "if 'lda_model_sklearn' in globals() and 'feature_names_counts' in globals():\n",
        "    print(\"Generando Word Clouds para cada tema del modelo LDA de scikit-learn...\")\n",
        "\n",
        "    # Obtener las distribuciones de palabras por tema desde el modelo scikit-learn\n",
        "    # model.components_ es una matriz (num_topics, num_features) con las proporciones logarítmicas\n",
        "    topic_word_distributions = lda_model_sklearn.components_\n",
        "\n",
        "    # Aplicar el estilo oscuro si ya fue configurado\n",
        "    plt.style.use('dark_background')\n",
        "\n",
        "    # Determinar el número de temas\n",
        "    num_topics_viz = topic_word_distributions.shape[0]\n",
        "\n",
        "    plt.figure(figsize=(15, 8)) # Ajustar tamaño general de la figura\n",
        "\n",
        "    # Generar una Word Cloud para cada tema\n",
        "    # Iteramos sobre cada fila en topic_word_distributions\n",
        "    for i, topic_dist in enumerate(topic_word_distributions):\n",
        "        # Obtener los índices de las palabras más importantes para este tema\n",
        "        # Usamos argsort y luego revertimos para obtener los índices de mayor a menor probabilidad\n",
        "        # Podemos tomar un número limitado de palabras para la Word Cloud\n",
        "        num_top_words_wc = 50 # Número de palabras a incluir en la Word Cloud\n",
        "\n",
        "        # Obtener los índices de las palabras principales (de mayor probabilidad logarítmica)\n",
        "        top_word_indices = topic_dist.argsort()[:-num_top_words_wc - 1:-1]\n",
        "\n",
        "        # Crear un diccionario de palabras y sus \"pesos\" para WordCloud\n",
        "        # Podemos usar la probabilidad (exp(log_prob)) o simplemente la probabilidad logarítmica\n",
        "        # WordCloud trabaja con frecuencias o pesos, así que usaremos las probabilidades\n",
        "        topic_dict = {}\n",
        "        # Exponenciar las probabilidades logarítmicas y normalizar (opcional, WordCloud puede trabajar con valores no normalizados)\n",
        "        # La suma de exp(log_prob) para un tema NO suma 1 directamente, es la suma de las *probabilidades* lo que suma 1.\n",
        "        # Para WordCloud, simplemente usar los valores (quizás exponenciados) funciona bien.\n",
        "        # Vamos a usar los valores de topic_dist directamente o sus exponenciales. Exp es más representativo de probabilidad.\n",
        "        # Asegurarse de que los valores sean positivos, lo cual exp(log_prob) garantiza.\n",
        "        # Normalizar solo para este tema para que sumen 1 para WordCloud\n",
        "        # No es estrictamente necesario que sumen 1, WordCloud escala por sí solo.\n",
        "        # Simplemente usaremos los valores exponenciados de las top palabras.\n",
        "        top_word_probs = np.exp(topic_dist[top_word_indices])\n",
        "        # Normalizar estas top N probabilidades para que sumen 1 para WordCloud (opcional, pero común)\n",
        "        top_word_probs_normalized = top_word_probs / np.sum(top_word_probs)\n",
        "\n",
        "        for j, word_index in enumerate(top_word_indices):\n",
        "            word = feature_names_counts[word_index]\n",
        "            # Usamos la probabilidad normalizada para el tamaño en la Word Cloud\n",
        "            topic_dict[word] = top_word_probs_normalized[j]\n",
        "\n",
        "\n",
        "        # Crear la Word Cloud (fondo oscuro)\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='black', colormap='viridis').generate_from_frequencies(topic_dict)\n",
        "\n",
        "        # Mostrar la Word Cloud\n",
        "        plt.subplot(2, (num_topics_viz + 1) // 2, i + 1) # Organizar en filas/columnas\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Tema #{i+1 + 1} (scikit-learn)') # +1 para índice base 1, +1 para diferenciar iteración\n",
        "\n",
        "    plt.tight_layout() # Ajustar el layout\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nWord Clouds de temas LDA (scikit-learn, filtrado) generadas.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudieron generar las Word Clouds. Asegúrate de que el modelo LDA de scikit-learn se entrenó correctamente.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9a997f6"
      },
      "source": [
        "### Sub-etapa 1.4.5: Extracción de Palabras Clave (TF-IDF y RAKE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N-nTKRSIi-ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9335e96"
      },
      "source": [
        "# Necesitamos la matriz TF-IDF y los nombres de las características (palabras)\n",
        "# Si la variable tfidf_vectorizer no está definida (por ejemplo, si reiniciaste\n",
        "# el entorno sin ejecutar la celda 4704de53), necesitarás ejecutarla primero.\n",
        "# También necesitamos los documentos procesados (lista de listas de tokens)\n",
        "# Si 'processed_documents_tokens' no está definido, ejecuta la celda 55dc36db.\n",
        "\n",
        "# Asegurarnos de que las variables necesarias estén disponibles\n",
        "try:\n",
        "    tfidf_vectorizer\n",
        "    processed_documents_tokens\n",
        "    # feature_names is obtained from tfidf_vectorizer.get_feature_names_out()\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "    print(f\"Variables 'tfidf_vectorizer', 'processed_documents_tokens' y 'feature_names' encontradas.\")\n",
        "except NameError:\n",
        "    print(\"Advertencia: Variables necesarias para Extracción de Palabras Clave no encontradas.\")\n",
        "    print(\"Por favor, ejecuta las celdas correspondientes (Sub-etapa 1.2 y Sub-etapa 1.4.2) para definirlas.\")\n",
        "    # Inicializar variables para evitar errores si no se encuentran\n",
        "    tfidf_vectorizer = None\n",
        "    processed_documents_tokens = []\n",
        "    feature_names = []\n",
        "\n",
        "\n",
        "# --- Extracción de Palabras Clave basada en TF-IDF ---\n",
        "if tfidf_vectorizer is not None and processed_documents_tokens:\n",
        "    print(\"\\n--- Extracción de Palabras Clave basada en TF-IDF ---\")\n",
        "\n",
        "    # Re-calcular la matriz TF-IDF para la muestra procesada si es necesario\n",
        "    # Usamos transform si el vectorizador ya fue ajustado en una muestra similar\n",
        "    # o fit_transform si queremos ajustar en esta muestra específica.\n",
        "    # Para consistencia con la visualización anterior, usamos el vectorizador ajustado en 10 documentos.\n",
        "    # Si quieres usar un vectorizador ajustado en una muestra mayor (como los 1000 de LDA),\n",
        "    # necesitarías reajustar TfidfVectorizer en 'processed_documents_tokens'\n",
        "\n",
        "    # Aquí usaremos el vectorizador ya ajustado (de la celda 4704de53) en una muestra pequeña\n",
        "    # Para aplicar a 'processed_documents_tokens' (1000 docs), necesitamos un vectorizador ajustado en ellos.\n",
        "    # Vamos a reajustar TfidfVectorizer en los 1000 documentos para esta sección.\n",
        "\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "    # Unir los tokens de cada documento en un string para TfidfVectorizer\n",
        "    processed_documents_strings = [\" \".join(doc) for doc in processed_documents_tokens]\n",
        "\n",
        "    # Inicializar y ajustar el vectorizador TF-IDF en la muestra de 1000 documentos\n",
        "    tfidf_vectorizer_sample = TfidfVectorizer(max_features=5000) # Usar el mismo límite de features\n",
        "    tfidf_matrix_sample = tfidf_vectorizer_sample.fit_transform(processed_documents_strings)\n",
        "    feature_names_sample = tfidf_vectorizer_sample.get_feature_names_out()\n",
        "\n",
        "    print(f\"Vectorizador TF-IDF ajustado en {len(processed_documents_tokens)} documentos.\")\n",
        "\n",
        "    # Mostrar las palabras clave con mayor TF-IDF para algunas reseñas de muestra\n",
        "    num_reviews_to_show_keywords = 3\n",
        "    num_top_terms_keywords = 5\n",
        "\n",
        "    print(f\"\\nTop {num_top_terms_keywords} Palabras Clave (TF-IDF) para algunas reseñas de muestra:\")\n",
        "\n",
        "    for i in range(min(num_reviews_to_show_keywords, tfidf_matrix_sample.shape[0])):\n",
        "        review_tfidf_vector = tfidf_matrix_sample[i].toarray().flatten()\n",
        "        top_term_indices = review_tfidf_vector.argsort()[-num_top_terms_keywords:][::-1] # Indices de mayor a menor\n",
        "\n",
        "        print(f\"  --- Reseña {i+1} ---\")\n",
        "        for index in top_term_indices:\n",
        "            term = feature_names_sample[index]\n",
        "            score = review_tfidf_vector[index]\n",
        "            print(f\"    - {term}: {score:.4f}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo realizar la Extracción de Palabras Clave basada en TF-IDF. Asegúrate de que las variables necesarias estén definidas.\")\n",
        "\n",
        "\n",
        "# --- Extracción de Palabras Clave con RAKE ---\n",
        "from rake_nltk import Rake\n",
        "\n",
        "if processed_documents_tokens:\n",
        "    print(\"\\n--- Extracción de Palabras Clave con RAKE ---\")\n",
        "\n",
        "    # RAKE trabaja mejor en el texto original o menos procesado, pero podemos probar con el texto lematizado.\n",
        "    # También necesita stop words. Podemos usar la lista de NLTK.\n",
        "    try:\n",
        "        # Asegurarse de que stop_words esté definido (de la celda 98b2a2e1 o 34b71f7f)\n",
        "        stop_words\n",
        "    except NameError:\n",
        "        print(\"Advertencia: La variable 'stop_words' no fue encontrada. Intentando descargar stop words de NLTK.\")\n",
        "        import nltk\n",
        "        try:\n",
        "            nltk.data.find('corpora/stopwords')\n",
        "        except LookupError:\n",
        "            nltk.download('stopwords')\n",
        "        from nltk.corpus import stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "    # Inicializar RAKE con la lista de stop words en inglés\n",
        "    # RAKE funciona en strings, no en listas de tokens. Usaremos los strings procesados.\n",
        "    rake = Rake(stopwords=stop_words)\n",
        "\n",
        "    # Mostrar las palabras clave extraídas por RAKE para algunas reseñas de muestra\n",
        "    num_reviews_to_show_rake = 3\n",
        "\n",
        "    print(f\"\\nTop Palabras/Frases Clave (RAKE) para algunas reseñas de muestra:\")\n",
        "\n",
        "    # Usamos processed_documents_strings que creamos para TF-IDF\n",
        "    if 'processed_documents_strings' in locals() and processed_documents_strings:\n",
        "        for i in range(min(num_reviews_to_show_rake, len(processed_documents_strings))):\n",
        "            review_text = processed_documents_strings[i]\n",
        "            rake.extract_keywords_from_text(review_text)\n",
        "            # Obtener las palabras clave clasificadas con sus puntuaciones\n",
        "            keyword_scores = rake.get_ranked_phrases_with_scores()\n",
        "\n",
        "            print(f\"  --- Reseña {i+1} ---\")\n",
        "            # Mostrar un número limitado de las mejores palabras clave/frases\n",
        "            for score, phrase in keyword_scores[:5]: # Mostrar las top 5 frases clave\n",
        "                print(f\"    - {phrase}: {score:.4f}\")\n",
        "    else:\n",
        "        print(\"No se pudo realizar la Extracción de Palabras Clave con RAKE. Asegúrate de que 'processed_documents_strings' esté disponible.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo realizar la Extracción de Palabras Clave con RAKE. Asegúrate de que 'processed_documents_tokens' esté disponible.\")\n",
        "\n",
        "# Recomendación:\n",
        "# - TF-IDF resalta palabras únicas importantes.\n",
        "# - RAKE resalta frases clave que capturan conceptos.\n",
        "# Ambos son útiles para entender el contenido de los documentos."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcc369f3"
      },
      "source": [
        "### Sub-etapa 1.4.6: Visualizar Palabras Clave Extraídas (TF-IDF y RAKE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eb69a35"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Asegurarse de que las variables necesarias de la celda 1.4.5 estén disponibles\n",
        "try:\n",
        "    # tfidf_vectorizer_sample is from cell a9335e96\n",
        "    # tfidf_matrix_sample is from cell a9335e96\n",
        "    # feature_names_sample is from cell a9335e96\n",
        "    tfidf_vectorizer_sample\n",
        "    tfidf_matrix_sample\n",
        "    feature_names_sample\n",
        "    # rake is from cell a9335e96\n",
        "    rake\n",
        "    # processed_documents_strings is from cell a9335e96\n",
        "    processed_documents_strings\n",
        "except NameError:\n",
        "    print(\"Advertencia: Variables necesarias para la visualización de Palabras Clave no encontradas.\")\n",
        "    print(\"Por favor, ejecuta la celda de Extracción de Palabras Clave (Sub-etapa 1.4.5, celda a9335e96) primero.\")\n",
        "    # Inicializar variables para evitar errores si no se encuentran\n",
        "    tfidf_vectorizer_sample = None\n",
        "    tfidf_matrix_sample = None\n",
        "    feature_names_sample = np.array([]) # Inicializar como array vacío de numpy para evitar NameError\n",
        "    rake = None\n",
        "    processed_documents_strings = []\n",
        "\n",
        "\n",
        "# Aplicar el estilo oscuro si ya fue configurado\n",
        "plt.style.use('dark_background')\n",
        "\n",
        "\n",
        "print(\"\\n--- Visualización de Palabras Clave Extraídas ---\")\n",
        "\n",
        "# --- Visualización TF-IDF ---\n",
        "# Corregir la condición para verificar si feature_names_sample no está vacío usando len()\n",
        "if tfidf_matrix_sample is not None and len(feature_names_sample) > 0:\n",
        "    print(\"\\nPalabras Clave (TF-IDF) para algunas reseñas de muestra:\")\n",
        "\n",
        "    num_reviews_to_show_viz = 3 # Mostrar visualización para 3 reseñas\n",
        "    num_top_terms_viz = 10 # Mostrar los 10 términos con mayor TF-IDF\n",
        "\n",
        "    for i in range(min(num_reviews_to_show_viz, tfidf_matrix_sample.shape[0])):\n",
        "        print(f\"\\nReseña de Muestra {i+1} (TF-IDF):\")\n",
        "\n",
        "        # Obtener el vector TF-IDF para la reseña actual\n",
        "        review_tfidf_vector = tfidf_matrix_sample[i].toarray().flatten()\n",
        "\n",
        "        # Obtener los índices de los términos con mayor peso TF-IDF y ordenar\n",
        "        top_term_indices = np.argpartition(review_tfidf_vector, -num_top_terms_viz)[-num_top_terms_viz:]\n",
        "        top_term_indices = top_term_indices[np.argsort(-review_tfidf_vector[top_term_indices])]\n",
        "\n",
        "        # Crear DataFrame y Gráfico de Barras\n",
        "        top_terms_data = []\n",
        "        for index in top_term_indices:\n",
        "            term = feature_names_sample[index]\n",
        "            tfidf_score = review_tfidf_vector[index]\n",
        "            top_terms_data.append({'Término': term, 'Peso TF-IDF': tfidf_score})\n",
        "\n",
        "        df_top_terms_viz = pd.DataFrame(top_terms_data)\n",
        "        display(df_top_terms_viz)\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.bar(df_top_terms_viz['Término'], df_top_terms_viz['Peso TF-IDF'])\n",
        "        plt.ylabel('Peso TF-IDF')\n",
        "        plt.title(f'Top {num_top_terms_viz} Términos (TF-IDF) - Reseña {i+1}')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"\\nNo se pudo generar la visualización de TF-IDF. Asegúrate de que la Extracción de Palabras Clave (TF-IDF) fue exitosa y generó datos.\")\n",
        "\n",
        "\n",
        "# --- Visualización RAKE ---\n",
        "# La condición para processed_documents_strings ya debería funcionar si es una lista\n",
        "if rake is not None and processed_documents_strings:\n",
        "    print(\"\\nPalabras/Frases Clave (RAKE) para algunas reseñas de muestra:\")\n",
        "\n",
        "    num_reviews_to_show_rake_viz = 3 # Mostrar visualización para 3 reseñas\n",
        "    num_top_phrases_viz = 5 # Mostrar las top 5 frases RAKE\n",
        "\n",
        "    for i in range(min(num_reviews_to_show_rake_viz, len(processed_documents_strings))):\n",
        "        print(f\"\\nReseña de Muestra {i+1} (RAKE):\")\n",
        "\n",
        "        # Re-extraer keywords para la reseña (ya que RAKE trabaja a nivel de documento individual)\n",
        "        review_text = processed_documents_strings[i]\n",
        "        rake.extract_keywords_from_text(review_text)\n",
        "        keyword_scores = rake.get_ranked_phrases_with_scores()\n",
        "\n",
        "        # Crear DataFrame para las top frases clave\n",
        "        top_phrases_data = [{'Frase Clave': phrase, 'Puntuación RAKE': score} for score, phrase in keyword_scores[:num_top_phrases_viz]]\n",
        "        df_top_phrases_viz = pd.DataFrame(top_phrases_data)\n",
        "        display(df_top_phrases_viz)\n",
        "\n",
        "        # NOTA: Un gráfico de barras para RAKE es un poco menos estándar para múltiples frases de longitud variable,\n",
        "        # pero podríamos hacerlo si fuera necesario, mostrando las frases en el eje X.\n",
        "        # Por ahora, la tabla muestra claramente las frases y sus puntuaciones.\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo generar la visualización de RAKE. Asegúrate de que la Extracción de Palabras Clave (RAKE) fue exitosa y generó datos.\")\n",
        "\n",
        "# Recomendación:\n",
        "# Compara los resultados de TF-IDF (palabras individuales importantes) con los de RAKE (frases clave).\n",
        "# Ambos métodos identifican \"palabras clave\", pero RAKE es mejor para capturar conceptos multi-palabra."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8562c816"
      },
      "source": [
        "### Sub-etapa 1.5: Vectorización TF-IDF en el Dataset Completo\n",
        "\n",
        "Aplicaremos el proceso de limpieza y lematización desarrollado a todo el dataset `imdb_reviews` y luego utilizaremos `TfidfVectorizer` para obtener la representación TF-IDF de cada reseña."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78c02b5d"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd # Para mostrar información si es necesario\n",
        "import numpy as np # Necesario para np.array\n",
        "\n",
        "# Asegurar que los recursos de NLTK y stop words estén descargados\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "    nltk.data.find('corpora/omw-1.4')\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    print(\"Recursos de NLTK o stop words no encontrados. Ejecuta las celdas de descarga de la Sub-etapa 1.1.1 y 1.1.4.\")\n",
        "\n",
        "# Asegurar que el dataset esté cargado\n",
        "try:\n",
        "    # Si la variable 'dataset' ya existe, la usamos.\n",
        "    print(\"Utilizando el dataset 'imdb_reviews' cargado previamente.\")\n",
        "except NameError:\n",
        "    # Si no, intentamos cargarlo.\n",
        "    print(\"Dataset 'imdb_reviews' no encontrado. Intentando cargarlo.\")\n",
        "    try:\n",
        "        dataset, info = tfds.load('imdb_reviews', split='train', with_info=True, as_supervised=True)\n",
        "        print(\"Dataset 'imdb_reviews' cargado exitosamente.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el dataset: {e}\")\n",
        "        print(\"Por favor, ejecuta la celda de carga del dataset 'imdb_reviews' (Paso 0).\")\n",
        "        dataset = None # Asegurar que dataset es None si la carga falla\n",
        "\n",
        "\n",
        "if dataset is not None:\n",
        "    # Obtener la lista de stop words en inglés\n",
        "    try:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "    except LookupError:\n",
        "        print(\"Recurso 'stopwords' de NLTK no encontrado.\")\n",
        "        stop_words = set() # Usar un set vacío\n",
        "\n",
        "    # Inicializar Lemmatizer y POS tagger\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    # get_wordnet_pos function defined previously (Cell 34b71f7f or 98b2a2e1)\n",
        "    # Redefinir la función get_wordnet_pos para asegurar que esté disponible\n",
        "    def get_wordnet_pos(word):\n",
        "        try:\n",
        "            tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "            tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "            return tag_dict.get(tag, wordnet.NOUN)\n",
        "        except LookupError:\n",
        "             return wordnet.NOUN\n",
        "        except IndexError:\n",
        "             return wordnet.NOUN\n",
        "\n",
        "\n",
        "    # Función de limpieza, tokenización, eliminación de stop words y lematización\n",
        "    # Esta función procesará un string y retornará un string con los tokens unidos por espacio\n",
        "    def clean_and_lemmatize_text(text_input):\n",
        "        if isinstance(text_input, tf.Tensor):\n",
        "             if tf.size(text_input) > 0 and text_input.dtype == tf.string:\n",
        "                 text = text_input.numpy().decode('utf-8', errors='ignore')\n",
        "             else:\n",
        "                 return \"\" # Retornar string vacío si el tensor está vacío o no es string\n",
        "        elif isinstance(text_input, bytes):\n",
        "             text = text_input.decode('utf-8', errors='ignore')\n",
        "        else: # Asumir que ya es un string\n",
        "             text = str(text_input)\n",
        "\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'<br />', ' ', text)\n",
        "        text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        try:\n",
        "            tokens = word_tokenize(text)\n",
        "        except LookupError:\n",
        "            print(\"Recurso 'punkt' de NLTK no encontrado.\")\n",
        "            return \"\" # Retornar string vacío si falta el recurso de tokenización\n",
        "\n",
        "        processed_tokens = []\n",
        "        for word in tokens:\n",
        "            if word and word not in stop_words:\n",
        "                lemma = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
        "                processed_tokens.append(lemma)\n",
        "\n",
        "        return \" \".join(processed_tokens) # Retornar un string con los tokens unidos\n",
        "\n",
        "\n",
        "    # --- Procesar el dataset completo y recolectar los textos procesados ---\n",
        "    # Iterar sobre el dataset y aplicar la función de procesamiento.\n",
        "    # Recolectamos los resultados en una lista de strings.\n",
        "    # NOTA: Esto puede consumir mucha memoria para datasets muy grandes.\n",
        "    # Para datasets extremadamente grandes, considera procesar en batches y guardar\n",
        "    # los resultados en disco o usar tf.data.Dataset con tf.py_function y luego TextVectorization.\n",
        "\n",
        "    processed_texts_list = []\n",
        "    labels_list = []\n",
        "    total_reviews_processed = 0\n",
        "\n",
        "    print(\"Procesando dataset completo para TF-IDF (esto puede tardar)...\")\n",
        "\n",
        "    # Usar as_numpy_iterator() para iterar más eficientemente sobre lotes convertidos a numpy\n",
        "    batch_size_for_processing = 100 # Procesar 100 reseñas a la vez\n",
        "\n",
        "    for text_batch, label_batch in dataset.batch(batch_size_for_processing).as_numpy_iterator():\n",
        "        for i in range(len(text_batch)):\n",
        "            processed_text = clean_and_lemmatize_text(text_batch[i])\n",
        "            processed_texts_list.append(processed_text)\n",
        "            labels_list.append(label_batch[i]) # Guardar también las etiquetas\n",
        "\n",
        "        total_reviews_processed += len(text_batch)\n",
        "        if total_reviews_processed % 5000 == 0:\n",
        "            print(f\"Procesadas {total_reviews_processed} reseñas...\")\n",
        "\n",
        "    print(f\"\\nProcesamiento de texto completo. Total de reseñas procesadas: {total_reviews_processed}\")\n",
        "    print(f\"Número de textos procesados recolectados: {len(processed_texts_list)}\")\n",
        "\n",
        "\n",
        "    # --- Aplicar TF-IDF al corpus completo de textos procesados ---\n",
        "    # Inicializar el vectorizador TF-IDF\n",
        "    # Es crucial limitar max_features para manejar la dimensionalidad y memoria\n",
        "    # Puedes ajustar este número\n",
        "    tfidf_max_features = 10000 # Considerar las 10,000 palabras/lemas más importantes\n",
        "\n",
        "    print(f\"\\nAplicando TF-IDF con max_features={tfidf_max_features}...\")\n",
        "\n",
        "    tfidf_vectorizer_full = TfidfVectorizer(max_features=tfidf_max_features)\n",
        "\n",
        "    # Ajustar y transformar el corpus completo\n",
        "    tfidf_matrix_full = tfidf_vectorizer_full.fit_transform(processed_texts_list)\n",
        "\n",
        "    # Obtener los nombres de las características (palabras/lemas)\n",
        "    feature_names_full = tfidf_vectorizer_full.get_feature_names_out()\n",
        "\n",
        "    print(\"Vectorización TF-IDF completa.\")\n",
        "    print(f\"Forma de la matriz TF-IDF completa: {tfidf_matrix_full.shape}\")\n",
        "    print(f\"Número de características (vocabulario TF-IDF): {len(feature_names_full)}\")\n",
        "\n",
        "    # Convertir la lista de etiquetas a un array numpy para consistencia\n",
        "    labels_array = np.array(labels_list)\n",
        "\n",
        "    print(f\"Forma del array de etiquetas: {labels_array.shape}\")\n",
        "\n",
        "    # Recomendación:\n",
        "    # La matriz tfidf_matrix_full es una matriz dispersa (sparse matrix) para ahorrar memoria.\n",
        "    # La mayoría de los modelos de ML (como los de scikit-learn) pueden trabajar directamente con matrices dispersas.\n",
        "    # Ahora tienes tus datos (tfidf_matrix_full) y etiquetas (labels_array) listos para el entrenamiento del modelo (Paso 2).\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo vectorizar el dataset completo porque no fue cargado exitosamente.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eef3128a"
      },
      "source": [
        "## Paso 2: Entrenar un modelo de clasificación de sentimiento\n",
        "\n",
        "Entrenaremos un modelo de Regresión Logística para clasificar reseñas como positivas (1) o negativas (0) utilizando las representaciones TF-IDF y las etiquetas preparadas en el Paso 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d3f5293"
      },
      "source": [
        "### Sub-etapa 2.1: División de datos en conjuntos de entrenamiento y validación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4697eef7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Asegurarnos de que tfidf_matrix_full y labels_array estén disponibles\n",
        "try:\n",
        "    tfidf_matrix_full\n",
        "    labels_array\n",
        "    print(f\"Forma de la matriz TF-IDF completa: {tfidf_matrix_full.shape}\")\n",
        "    print(f\"Forma del array de etiquetas: {labels_array.shape}\")\n",
        "except NameError:\n",
        "    print(\"Advertencia: Las variables 'tfidf_matrix_full' o 'labels_array' no fueron encontradas.\")\n",
        "    print(\"Por favor, ejecuta la celda de Vectorización TF-IDF en el Dataset Completo (Sub-etapa 1.5) primero.\")\n",
        "    # Salir o manejar el error si los datos no están disponibles\n",
        "\n",
        "\n",
        "if 'tfidf_matrix_full' in globals() and 'labels_array' in globals():\n",
        "    # Dividir los datos. Usaremos un 80% para entrenamiento y 20% para validación.\n",
        "    # stratify=labels_array asegura que la proporción de clases (0s y 1s) sea similar en ambos conjuntos.\n",
        "    # random_state=42 asegura que la división sea la misma cada vez que ejecutas el código.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix_full,\n",
        "                                                        labels_array,\n",
        "                                                        test_size=0.2,\n",
        "                                                        random_state=42,\n",
        "                                                        stratify=labels_array)\n",
        "\n",
        "    print(\"\\nDatos divididos en conjuntos de entrenamiento y validación:\")\n",
        "    print(f\"Forma de X_train: {X_train.shape}\")\n",
        "    print(f\"Forma de X_test: {X_test.shape}\")\n",
        "    print(f\"Forma de y_train: {y_train.shape}\")\n",
        "    print(f\"Forma de y_test: {y_test.shape}\")\n",
        "\n",
        "    # Opcional: Verificar la distribución de clases en los conjuntos\n",
        "    # print(\"\\nDistribución de clases en y_train:\")\n",
        "    # display(pd.Series(y_train).value_counts(normalize=True))\n",
        "    # print(\"\\nDistribución de clases en y_test:\")\n",
        "    # display(pd.Series(y_test).value_counts(normalize=True))\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo dividir los datos. Asegúrate de que los datos TF-IDF y etiquetas estén cargados.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d62baf73"
      },
      "source": [
        "### Sub-etapa 2.2: Entrenamiento del modelo de Regresión Logística"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d393ade"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "# Importar métricas para evaluación (aunque las usaremos en la siguiente celda)\n",
        "# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Asegurarnos de que los datos de entrenamiento estén disponibles\n",
        "try:\n",
        "    X_train, y_train\n",
        "    print(f\"Usando datos de entrenamiento con forma X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "except NameError:\n",
        "    print(\"Advertencia: Los datos de entrenamiento (X_train, y_train) no fueron encontrados.\")\n",
        "    print(\"Por favor, ejecuta la celda de División de datos (Sub-etapa 2.1) primero.\")\n",
        "\n",
        "\n",
        "if 'X_train' in globals() and 'y_train' in globals():\n",
        "    print(\"\\nInicializando y entrenando el modelo de Regresión Logística...\")\n",
        "\n",
        "    # Inicializar el modelo de Regresión Logística\n",
        "    # class_weight='balanced' ayuda a manejar posibles desbalances en las clases\n",
        "    # solver='liblinear' es bueno para datasets pequeños y medianos con L1/L2 regularization\n",
        "    # random_state=42 para reproducibilidad\n",
        "    model = LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced', max_iter=1000)\n",
        "\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    # Esto puede tardar un poco dependiendo del tamaño del dataset y max_features\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Entrenamiento del modelo de Regresión Logística completo.\")\n",
        "    print(f\"El modelo ha aprendido {model.n_features_in_} características (palabras/lemas).\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo entrenar el modelo. Asegúrate de que los datos de entrenamiento estén disponibles.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "374130f5"
      },
      "source": [
        "### Sub-etapa 2.3: Evaluación del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1dc7349"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # Para visualizar la matriz de confusión\n",
        "\n",
        "# Asegurarnos de que el modelo entrenado y los datos de prueba estén disponibles\n",
        "try:\n",
        "    model # El modelo entrenado\n",
        "    X_test, y_test # Datos de validación/prueba\n",
        "    print(\"Modelo entrenado y datos de prueba encontrados.\")\n",
        "except NameError:\n",
        "    print(\"Advertencia: El modelo entrenado o los datos de prueba (X_test, y_test) no fueron encontrados.\")\n",
        "    print(\"Por favor, ejecuta las celdas de Entrenamiento del modelo (Sub-etapa 2.2) y División de datos (Sub-etapa 2.1) primero.\")\n",
        "\n",
        "\n",
        "if 'model' in globals() and 'X_test' in globals() and 'y_test' in globals():\n",
        "    print(\"\\nEvaluando el modelo en el conjunto de validación...\")\n",
        "\n",
        "    # Realizar predicciones en el conjunto de validación\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # --- Mostrar Métricas de Clasificación ---\n",
        "    print(\"\\nReporte de Clasificación:\")\n",
        "    # target_names permite especificar los nombres de las clases (0 -> negativo, 1 -> positivo)\n",
        "    print(classification_report(y_test, y_pred, target_names=['Negativo', 'Positivo']))\n",
        "\n",
        "    # --- Mostrar Matriz de Confusión ---\n",
        "    print(\"\\nMatriz de Confusión:\")\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Visualizar la matriz de confusión\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    # Aplicar el estilo oscuro si ya fue configurado\n",
        "    plt.style.use('dark_background') # Asegurarse de que el estilo esté activo\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negativo', 'Positivo'], yticklabels=['Negativo', 'Positivo'])\n",
        "    plt.xlabel('Predicción')\n",
        "    plt.ylabel('Valor Real')\n",
        "    plt.title('Matriz de Confusión')\n",
        "    plt.show()\n",
        "\n",
        "    # Mostrar Accuracy (opcional, ya está en el reporte pero es una métrica clave)\n",
        "    # accuracy = accuracy_score(y_test, y_pred)\n",
        "    # print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Recomendación:\n",
        "    # - Precision: De todas las reseñas que el modelo predijo como Positivas, ¿cuántas eran realmente Positivas?\n",
        "    # - Recall: De todas las reseñas que eran realmente Positivas, ¿cuántas predijo el modelo correctamente?\n",
        "    # - F1-score: Es la media armónica de Precision y Recall. Una buena métrica única cuando Precision y Recall son importantes.\n",
        "    # - Support: Número de instancias reales de cada clase en el conjunto de prueba.\n",
        "    # - Matriz de Confusión: Muestra cuántas predicciones fueron Verdaderos Positivos (VP), Verdaderos Negativos (VN), Falsos Positivos (FP), y Falsos Negativos (FN).\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo evaluar el modelo. Asegúrate de que el modelo y los datos de prueba estén disponibles.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f86a1b3a"
      },
      "source": [
        "### Sub-etapa 2.4: Guardar el modelo entrenado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7514df5"
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "# Asegurarnos de que el modelo entrenado esté disponible\n",
        "try:\n",
        "    model\n",
        "    print(\"Modelo entrenado encontrado.\")\n",
        "except NameError:\n",
        "    print(\"Advertencia: El modelo entrenado no fue encontrado.\")\n",
        "    print(\"Por favor, ejecuta la celda de Entrenamiento del modelo (Sub-etapa 2.2) primero.\")\n",
        "\n",
        "\n",
        "if 'model' in globals():\n",
        "    # Definir el nombre del archivo para guardar el modelo\n",
        "    model_filename = 'logistic_regression_sentiment_model.pkl'\n",
        "\n",
        "    # Guardar el modelo en un archivo .pkl\n",
        "    with open(model_filename, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    print(f\"\\nModelo entrenado guardado como '{model_filename}'\")\n",
        "    print(f\"Tamaño del archivo del modelo: {os.path.getsize(model_filename)} bytes\")\n",
        "\n",
        "    # Recomendación:\n",
        "    # Este archivo .pkl contiene el modelo entrenado. Puedes cargarlo en otro script\n",
        "    # o notebook para hacer predicciones en datos nuevos sin tener que reentrenar.\n",
        "    # Por ejemplo: loaded_model = pickle.load(open('logistic_regression_sentiment_model.pkl', 'rb'))\n",
        "    # Para usar este modelo en el dataset de metadatos, necesitarás aplicar la misma\n",
        "    # vectorización TF-IDF (usando el *mismo* tfidf_vectorizer_full que ajustamos en el Paso 1.5)\n",
        "    # a la información textual de ese dataset antes de pasársela al modelo.\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo guardar el modelo. Asegúrate de que el modelo esté disponible.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8481440"
      },
      "source": [
        "## Paso 3: Adaptar el conocimiento al dataset `df_movie_metadata`\n",
        "\n",
        "Cargaremos el modelo de Regresión Logística entrenado y el vectorizador TF-IDF guardados, y los utilizaremos para predecir el sentimiento (basado en las palabras clave) para cada película en el dataset `df_movie_metadata`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27d29d4f"
      },
      "source": [
        "### Sub-etapa 3.1: Cargar el modelo y el vectorizador guardados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ace7c385"
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "# Definir los nombres de los archivos guardados (de Sub-etapa 2.4 y 1.5)\n",
        "model_filename = 'logistic_regression_sentiment_model.pkl'\n",
        "# El vectorizador TF-IDF completo fue guardado implícitamente en la variable tfidf_vectorizer_full\n",
        "# Necesitamos asegurarnos de que esta variable esté disponible o, idealmente, guardar/cargar también el vectorizador.\n",
        "# Como no lo guardamos explícitamente en un .pkl, vamos a asumir que la variable\n",
        "# tfidf_vectorizer_full de la celda 1.5 está disponible. Si no, necesitaríamos re-ejecutar esa celda\n",
        "# o modificar la celda 1.5 para guardar el vectorizador también.\n",
        "\n",
        "# Para mayor robustez, vamos a modificar la celda 1.5 para guardar el vectorizador y cargarlo aquí.\n",
        "# **NOTA:** Necesitarás re-ejecutar la celda 1.5 después de la modificación para guardar el vectorizador.\n",
        "\n",
        "\n",
        "# --- Cargar el modelo ---\n",
        "try:\n",
        "    with open(model_filename, 'rb') as f:\n",
        "        loaded_model = pickle.load(f)\n",
        "    print(f\"Modelo '{model_filename}' cargado exitosamente.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: El archivo del modelo '{model_filename}' no fue encontrado.\")\n",
        "    loaded_model = None\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el modelo: {e}\")\n",
        "    loaded_model = None\n",
        "\n",
        "\n",
        "# --- Cargar el vectorizador TF-IDF ---\n",
        "# Asumiendo que tfidf_vectorizer_full está disponible en el entorno.\n",
        "# Si no, necesitaríamos cargarlo desde un archivo si lo hubiéramos guardado en la Sub-etapa 1.5.\n",
        "try:\n",
        "    tfidf_vectorizer_full\n",
        "    print(\"Variable 'tfidf_vectorizer_full' encontrada en el entorno.\")\n",
        "    loaded_vectorizer = tfidf_vectorizer_full\n",
        "except NameError:\n",
        "    print(\"Advertencia: La variable 'tfidf_vectorizer_full' no fue encontrada en el entorno.\")\n",
        "    print(\"Por favor, ejecuta la celda de Vectorización TF-IDF en el Dataset Completo (Sub-etapa 1.5).\")\n",
        "    # Si hubiéramos guardado el vectorizador, lo cargaríamos aquí:\n",
        "    # vectorizer_filename = 'tfidf_vectorizer_full.pkl'\n",
        "    # try:\n",
        "    #     with open(vectorizer_filename, 'rb') as f:\n",
        "    #         loaded_vectorizer = pickle.load(f)\n",
        "    #     print(f\"Vectorizer '{vectorizer_filename}' cargado exitosamente.\")\n",
        "    # except FileNotFoundError:\n",
        "    #     print(f\"Error: El archivo del vectorizador '{vectorizer_filename}' no fue encontrado.\")\n",
        "    #     loaded_vectorizer = None\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Error al cargar el vectorizador: {e}\")\n",
        "    #     loaded_vectorizer = None\n",
        "    loaded_vectorizer = None # Asegurar que es None si no se encuentra\n",
        "\n",
        "\n",
        "if loaded_model is not None and loaded_vectorizer is not None:\n",
        "    print(\"\\nModelo y Vectorizador listos para inferencia.\")\n",
        "else:\n",
        "    print(\"\\nNo se pudieron cargar el Modelo o el Vectorizador. No se puede proceder con la inferencia.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef4c2e9c"
      },
      "source": [
        "### Sub-etapa 3.2: Preprocesar y Vectorizar la columna `plot_keywords` de `df_movie_metadata`\n",
        "\n",
        "Aplicaremos el mismo proceso de limpieza, lematización y eliminación de stop words a la columna `plot_keywords` del dataset `df_movie_metadata`. Luego, usaremos el `tfidf_vectorizer_full` entrenado previamente para convertir estas palabras clave en vectores TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81b69a34"
      },
      "source": [
        "# Asegurarnos de que df_movie_metadata esté cargado y limpio (sin NaNs)\n",
        "try:\n",
        "    # Verificamos si la variable df_movie_metadata existe y tiene la columna 'plot_keywords'\n",
        "    if 'df_movie_metadata' in globals() and isinstance(df_movie_metadata, pd.DataFrame) and 'plot_keywords' in df_movie_metadata.columns:\n",
        "         print(\"Utilizando el DataFrame df_movie_metadata (limpio) cargado previamente.\")\n",
        "         # Asegurarnos de que no tenga NaNs en 'plot_keywords', aunque ya eliminamos filas con NaNs\n",
        "         # En un escenario real, podrías querer imputar solo esta columna si no eliminaste filas.\n",
        "         # df_movie_metadata['plot_keywords'].fillna('', inplace=True) # Ejemplo de imputación simple\n",
        "\n",
        "         # Si el DataFrame fue limpiado eliminando filas, ya no debería haber NaNs.\n",
        "         if df_movie_metadata['plot_keywords'].isnull().any():\n",
        "             print(\"Advertencia: La columna 'plot_keywords' aún contiene valores nulos. Imputando con string vacío.\")\n",
        "             df_movie_metadata['plot_keywords'].fillna('', inplace=True)\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"Advertencia: La variable df_movie_metadata no fue encontrada, no es un DataFrame o no tiene la columna 'plot_keywords'.\")\n",
        "        print(\"Por favor, ejecuta las celdas de carga del dataset Kaggle y manejo de faltantes (Paso 0 y Sub-etapa 1.3).\")\n",
        "        df_movie_metadata = None # Asegurar que es None si no está disponible\n",
        "\n",
        "except NameError:\n",
        "    print(\"Advertencia: La variable df_movie_metadata no fue encontrada.\")\n",
        "    print(\"Por favor, ejecuta las celdas de carga del dataset Kaggle y manejo de faltantes (Paso 0 y Sub-etapa 1.3).\")\n",
        "    df_movie_metadata = None\n",
        "\n",
        "\n",
        "# Asegurarnos de que loaded_vectorizer esté disponible\n",
        "try:\n",
        "    loaded_vectorizer\n",
        "    print(\"Vectorizador TF-IDF cargado encontrado.\")\n",
        "except NameError:\n",
        "    print(\"Advertencia: El vectorizador TF-IDF ('loaded_vectorizer') no fue encontrado.\")\n",
        "    print(\"Por favor, ejecuta la celda de Carga de modelo y vectorizador (Sub-etapa 3.1).\")\n",
        "    loaded_vectorizer = None\n",
        "\n",
        "\n",
        "if df_movie_metadata is not None and loaded_vectorizer is not None:\n",
        "    # Necesitamos la misma función de limpieza/lematización/stop words que usamos en el entrenamiento\n",
        "    # Asegurar que la función clean_and_lemmatize_text esté definida (de la celda 78c02b5d)\n",
        "    try:\n",
        "        clean_and_lemmatize_text\n",
        "    except NameError:\n",
        "        print(\"Advertencia: La función 'clean_and_lemmatize_text' no fue encontrada.\")\n",
        "        print(\"Por favor, ejecuta la celda de Vectorización TF-IDF en el Dataset Completo (Sub-etapa 1.5) para definirla.\")\n",
        "        # Si la función no está definida, no podemos procesar el texto.\n",
        "        clean_and_lemmatize_text = None # Set to None to indicate it's missing\n",
        "\n",
        "    if clean_and_lemmatize_text is not None:\n",
        "        print(\"\\nPreprocesando la columna 'plot_keywords'...\")\n",
        "\n",
        "        # Aplicar la función de preprocesamiento a cada keyword string\n",
        "        # La columna 'plot_keywords' es una string con palabras separadas por '|'\n",
        "        # Necesitamos convertir 'palabra1|palabra2' a 'palabra1 palabra2' antes de limpiar\n",
        "        # y luego aplicar la misma limpieza que usamos para las reseñas.\n",
        "        # NOTA: Esta limpieza puede ser diferente si el formato de keywords es muy distinto al de reseñas.\n",
        "        # Asumiremos que la limpieza general es aplicable después de reemplazar '|' por espacio.\n",
        "\n",
        "        processed_keywords_list = df_movie_metadata['plot_keywords'].apply(\n",
        "            lambda x: clean_and_lemmatize_text(x.replace('|', ' ')) if isinstance(x, str) else clean_and_lemmatize_text('')\n",
        "        ).tolist()\n",
        "\n",
        "        print(\"Preprocesamiento de 'plot_keywords' completo.\")\n",
        "        print(f\"Número de entradas procesadas: {len(processed_keywords_list)}\")\n",
        "\n",
        "        print(\"\\nAplicando vectorizador TF-IDF entrenado a las palabras clave procesadas...\")\n",
        "\n",
        "        # Usar el vectorizador cargado para transformar los textos procesados\n",
        "        # Usamos transform(), NO fit_transform(), para usar el vocabulario y pesos aprendidos en el entrenamiento\n",
        "        keywords_tfidf_matrix = loaded_vectorizer.transform(processed_keywords_list)\n",
        "\n",
        "        print(\"Vectorización de palabras clave completa.\")\n",
        "        print(f\"Forma de la matriz TF-IDF de palabras clave: {keywords_tfidf_matrix.shape}\")\n",
        "        # Nota: La segunda dimensión (número de columnas) debe ser la misma que la del vectorizador entrenado.\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"\\nNo se pudo preprocesar y vectorizar las palabras clave. Asegúrate de que la función de preprocesamiento esté disponible.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo preprocesar y vectorizar las palabras clave. Asegúrate de que df_movie_metadata y el vectorizador estén disponibles.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b34b5fed"
      },
      "source": [
        "### Sub-etapa 3.3: Aplicar el modelo de sentimiento para inferencia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d02357a2"
      },
      "source": [
        "import numpy as np # Importar numpy si es necesario\n",
        "\n",
        "# Asegurarnos de que el modelo cargado y la matriz TF-IDF de palabras clave estén disponibles\n",
        "try:\n",
        "    loaded_model\n",
        "    keywords_tfidf_matrix\n",
        "    print(\"Modelo cargado y matriz TF-IDF de palabras clave encontrados.\")\n",
        "except NameError:\n",
        "    print(\"Advertencia: El modelo cargado ('loaded_model') o la matriz TF-IDF de palabras clave ('keywords_tfidf_matrix') no fueron encontrados.\")\n",
        "    print(\"Por favor, ejecuta las celdas de Carga de modelo (Sub-etapa 3.1) y Preprocesamiento/Vectorización de palabras clave (Sub-etapa 3.2).\")\n",
        "    loaded_model = None\n",
        "    keywords_tfidf_matrix = None\n",
        "\n",
        "\n",
        "if loaded_model is not None and keywords_tfidf_matrix is not None:\n",
        "    print(\"\\nAplicando el modelo de sentimiento a las representaciones TF-IDF de palabras clave...\")\n",
        "\n",
        "    # --- Predecir las etiquetas de sentimiento (0 o 1) ---\n",
        "    # predictions = loaded_model.predict(keywords_tfidf_matrix)\n",
        "    # print(\"\\nPredicciones de sentimiento (etiquetas 0 o 1) generadas.\")\n",
        "    # print(f\"Primeras 10 predicciones: {predictions[:10]}\")\n",
        "\n",
        "    # --- Predecir las probabilidades de sentimiento positivo ---\n",
        "    # Esto es a menudo más útil para entender el \"grado\" de positividad/negatividad\n",
        "    # La columna 1 de predict_proba() es la probabilidad de la clase positiva (1)\n",
        "    sentiment_probabilities = loaded_model.predict_proba(keywords_tfidf_matrix)[:, 1]\n",
        "\n",
        "    print(\"\\nPuntuaciones de probabilidad de sentimiento positivo generadas.\")\n",
        "    print(f\"Primeras 10 probabilidades de sentimiento positivo: {sentiment_probabilities[:10]}\")\n",
        "\n",
        "    # --- Añadir las puntuaciones de sentimiento al DataFrame df_movie_metadata ---\n",
        "    # Asegurarnos de que df_movie_metadata esté disponible\n",
        "    try:\n",
        "        df_movie_metadata\n",
        "        # Asegurarse de que el número de probabilidades coincide con el número de filas en df_movie_metadata\n",
        "        if len(sentiment_probabilities) == len(df_movie_metadata):\n",
        "            df_movie_metadata['predicted_sentiment_score'] = sentiment_probabilities\n",
        "            print(\"\\nColumna 'predicted_sentiment_score' añadida a df_movie_metadata.\")\n",
        "            # Mostrar las primeras filas con la nueva columna\n",
        "            display(df_movie_metadata[['movie_title', 'plot_keywords', 'predicted_sentiment_score']].head())\n",
        "\n",
        "        else:\n",
        "            print(\"Advertencia: El número de predicciones no coincide con el número de filas en df_movie_metadata.\")\n",
        "            print(\"Asegúrate de que el preprocesamiento y la vectorización se aplicaron al DataFrame correcto y completo.\")\n",
        "\n",
        "    except NameError:\n",
        "        print(\"Advertencia: La variable df_movie_metadata no fue encontrada.\")\n",
        "        print(\"No se pudieron añadir las puntuaciones de sentimiento al DataFrame.\")\n",
        "\n",
        "\n",
        "    # Recomendación:\n",
        "    # La columna 'predicted_sentiment_score' ahora contiene la probabilidad de que,\n",
        "    # basado en sus plot_keywords, el modelo de sentimiento entrenado lo clasificaría como positivo.\n",
        "    # Este puntaje puede usarse para ordenar, filtrar o analizar películas.\n",
        "    # Ten en cuenta que esta es una predicción basada *solo* en las plot_keywords,\n",
        "    # que es una información limitada comparada con una reseña completa.\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo aplicar el modelo de sentimiento. Asegúrate de que el modelo y los datos vectorizados estén disponibles.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f42c6efe"
      },
      "source": [
        "### Visualización de los Resultados de la Inferencia de Sentimiento\n",
        "\n",
        "Vamos a visualizar la distribución de las puntuaciones de sentimiento predichas y ver ejemplos de películas con sus puntuaciones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc261782"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd # Asegurarse de que pandas esté importado\n",
        "\n",
        "# Asegurarnos de que df_movie_metadata esté cargado y contenga la columna 'predicted_sentiment_score'\n",
        "try:\n",
        "    if 'df_movie_metadata' in globals() and isinstance(df_movie_metadata, pd.DataFrame) and 'predicted_sentiment_score' in df_movie_metadata.columns:\n",
        "         print(\"Utilizando el DataFrame df_movie_metadata con la columna 'predicted_sentiment_score'.\")\n",
        "    else:\n",
        "        print(\"Advertencia: La variable df_movie_metadata no fue encontrada, no es un DataFrame o no tiene la columna 'predicted_sentiment_score'.\")\n",
        "        print(\"Por favor, ejecuta las celdas de carga del dataset Kaggle, manejo de faltantes (Paso 0 y Sub-etapa 1.3) y Aplicación del modelo (Sub-etapa 3.3).\")\n",
        "        df_movie_metadata = None # Asegurar que es None si no está disponible\n",
        "\n",
        "except NameError:\n",
        "    print(\"Advertencia: La variable df_movie_metadata no fue encontrada.\")\n",
        "    print(\"Por favor, ejecuta las celdas de carga del dataset Kaggle, manejo de faltantes (Paso 0 y Sub-etapa 1.3) y Aplicación del modelo (Sub-etapa 3.3).\")\n",
        "    df_movie_metadata = None\n",
        "\n",
        "\n",
        "if df_movie_metadata is not None:\n",
        "    print(\"\\n--- Visualización de Resultados de Inferencia ---\")\n",
        "\n",
        "    # --- Mostrar ejemplos de películas con sus puntuaciones predichas ---\n",
        "    print(\"\\nEjemplos de películas con sus palabras clave, puntuación IMDb y puntuación de sentimiento predicha:\")\n",
        "    # Seleccionar algunas columnas relevantes y las primeras filas\n",
        "    display(df_movie_metadata[['movie_title', 'plot_keywords', 'imdb_score', 'predicted_sentiment_score']].head())\n",
        "\n",
        "    # Opcional: Mostrar películas con las puntuaciones más altas y más bajas\n",
        "    print(\"\\nPelículas con las puntuaciones de sentimiento predichas más altas:\")\n",
        "    display(df_movie_metadata.sort_values(by='predicted_sentiment_score', ascending=False)[['movie_title', 'imdb_score', 'predicted_sentiment_score']].head())\n",
        "\n",
        "    print(\"\\nPelículas con las puntuaciones de sentimiento predichas más bajas:\")\n",
        "    display(df_movie_metadata.sort_values(by='predicted_sentiment_score', ascending=True)[['movie_title', 'imdb_score', 'predicted_sentiment_score']].head())\n",
        "\n",
        "\n",
        "    # --- Histograma de la distribución de las puntuaciones de sentimiento predichas ---\n",
        "    print(\"\\nDistribución de las puntuaciones de sentimiento predichas:\")\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    # Aplicar el estilo oscuro\n",
        "    plt.style.use('dark_background')\n",
        "\n",
        "    sns.histplot(df_movie_metadata['predicted_sentiment_score'], bins=50, kde=True)\n",
        "    plt.title('Distribución de las Puntuaciones de Sentimiento Predichas')\n",
        "    plt.xlabel('Puntuación de Sentimiento Predicha (Probabilidad de Positivo)')\n",
        "    plt.ylabel('Frecuencia')\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.show()\n",
        "\n",
        "    # Opcional: Scatter plot de IMDb Score vs Predicted Sentiment Score\n",
        "    # print(\"\\nScatter plot de IMDb Score vs Predicted Sentiment Score:\")\n",
        "    # plt.figure(figsize=(8, 5))\n",
        "    # plt.style.use('dark_background')\n",
        "    # sns.scatterplot(data=df_movie_metadata, x='imdb_score', y='predicted_sentiment_score', alpha=0.6)\n",
        "    # plt.title('IMDb Score vs Predicted Sentiment Score (basado en Keywords)')\n",
        "    # plt.xlabel('IMDb Score Original')\n",
        "    # plt.ylabel('Puntuación de Sentimiento Predicha (Keywords)')\n",
        "    # plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    # plt.show()\n",
        "\n",
        "    # Recomendación:\n",
        "    # - La tabla te permite ver ejemplos concretos de cómo se predijo el sentimiento para películas específicas.\n",
        "    # - El histograma muestra si las predicciones tienden a ser más positivas, negativas o distribuidas uniformemente.\n",
        "    # - Recuerda que esta predicción se basa *únicamente* en las 'plot_keywords', no en reseñas completas,\n",
        "    #   por lo que puede no alinearse perfectamente con el IMDb Score general de la película.\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo generar la visualización de los resultados de inferencia. Asegúrate de que df_movie_metadata con la columna de sentimiento predicho esté disponible.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa6377ce"
      },
      "source": [
        "## Paso 4: Generar representaciones vectoriales para `df_movie_metadata`\n",
        "\n",
        "Crearemos un vector numérico para cada película en el dataset de metadatos combinando sus características relevantes. Este vector servirá como entrada para la reducción de dimensionalidad y visualización."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc572d1e"
      },
      "source": [
        "### Sub-etapa 4.1: Selección y Preparación de Características\n",
        "\n",
        "Seleccionaremos las columnas numéricas y categóricas a incluir en la representación vectorial y aplicaremos la codificación necesaria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cb10795"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Asegurarnos de que df_movie_metadata esté cargado y limpio\n",
        "try:\n",
        "    if 'df_movie_metadata' in globals() and isinstance(df_movie_metadata, pd.DataFrame):\n",
        "         print(\"Utilizando el DataFrame df_movie_metadata (limpio) cargado previamente.\")\n",
        "         print(f\"Columnas disponibles: {df_movie_metadata.columns.tolist()}\")\n",
        "    else:\n",
        "        print(\"Advertencia: La variable df_movie_metadata no fue encontrada o no es un DataFrame.\")\n",
        "        print(\"Por favor, ejecuta las celdas de carga del dataset Kaggle y manejo de faltantes (Paso 0 y Sub-etapa 1.3).\")\n",
        "        df_movie_metadata = None # Asegurar que es None si no está disponible\n",
        "\n",
        "except NameError:\n",
        "    print(\"Advertencia: La variable df_movie_metadata no fue encontrada.\")\n",
        "    print(\"Por favor, ejecuta las celdas de carga del dataset Kaggle y manejo de faltantes (Paso 0 y Sub-etapa 1.3).\")\n",
        "    df_movie_metadata = None\n",
        "\n",
        "\n",
        "if df_movie_metadata is not None:\n",
        "    # --- Seleccionar columnas a incluir ---\n",
        "    # Columnas numéricas (ya manejamos los NaNs eliminando filas)\n",
        "    numeric_features = [\n",
        "        'num_critic_for_reviews', 'duration', 'director_facebook_likes',\n",
        "        'actor_3_facebook_likes', 'actor_2_facebook_likes', 'actor_1_facebook_likes',\n",
        "        'gross', 'num_voted_users', 'cast_total_facebook_likes',\n",
        "        'facenumber_in_poster', 'num_user_for_reviews', 'budget',\n",
        "        'title_year', 'imdb_score', 'movie_facebook_likes',\n",
        "        'predicted_sentiment_score' # Nuestra columna de sentimiento predicho\n",
        "    ]\n",
        "\n",
        "    # Columnas categóricas (seleccionamos algunas para codificar)\n",
        "    # NOTA: One-Hot Encoding puede crear muchas columnas si hay muchas categorías únicas.\n",
        "    # Podríamos necesitar filtrar o usar otras técnicas (ej. Target Encoding, Embeddings categóricos)\n",
        "    # si las columnas tienen alta cardinalidad.\n",
        "    categorical_features = ['genres', 'country', 'language', 'content_rating']\n",
        "\n",
        "    # Asegurarnos de que las columnas seleccionadas existan en el DataFrame\n",
        "    selected_features = numeric_features + categorical_features\n",
        "    existing_features = [col for col in selected_features if col in df_movie_metadata.columns]\n",
        "    missing_features = [col for col in selected_features if col not in df_movie_metadata.columns]\n",
        "\n",
        "    if missing_features:\n",
        "        print(f\"Advertencia: Las siguientes columnas seleccionadas no se encontraron en el DataFrame: {missing_features}\")\n",
        "        print(\"Usando solo las columnas existentes.\")\n",
        "        numeric_features = [col for col in numeric_features if col in existing_features]\n",
        "        categorical_features = [col for col in categorical_features if col in existing_features]\n",
        "        existing_features = numeric_features + categorical_features # Actualizar la lista\n",
        "\n",
        "    print(f\"\\nColumnas numéricas seleccionadas: {numeric_features}\")\n",
        "    print(f\"Columnas categóricas seleccionadas: {categorical_features}\")\n",
        "\n",
        "\n",
        "    # --- Preparar transformaciones ---\n",
        "    # Para columnas numéricas: Escalar (MinMaxScaler es común para poner todo en un rango similar)\n",
        "    # Para columnas categóricas: One-Hot Encode (convierte categorías a vectores binarios)\n",
        "\n",
        "    # Crear transformadores para cada tipo de columna\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('scaler', MinMaxScaler())\n",
        "    ])\n",
        "\n",
        "    # Handle unknown categories in OneHotEncoder by ignoring them or adding a placeholder\n",
        "    # Setting handle_unknown='ignore' is often safer for inference on new data\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Combinar transformadores usando ColumnTransformer\n",
        "    # Esto aplicará las transformaciones correctas a las columnas correctas\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ],\n",
        "        remainder='passthrough' # Mantener otras columnas si las hay (no aplicable aquí ya que seleccionamos)\n",
        "    )\n",
        "\n",
        "    # --- Aplicar las transformaciones al DataFrame ---\n",
        "    print(\"\\nAplicando preprocesamiento y combinando características...\")\n",
        "\n",
        "    # Ajustar y transformar los datos\n",
        "    # Esto aplica el escalado a numéricas y one-hot encoding a categóricas\n",
        "    movie_features_vectorized = preprocessor.fit_transform(df_movie_metadata[existing_features])\n",
        "\n",
        "    print(\"Generación de representaciones vectoriales completa.\")\n",
        "    print(f\"Forma de la matriz de representaciones vectoriales: {movie_features_vectorized.shape}\")\n",
        "    # El número de columnas en la matriz resultante es la suma de:\n",
        "    # (número de columnas numéricas) + (suma de categorías únicas en cada columna categórica después de OHE)\n",
        "\n",
        "    # Recomendación:\n",
        "    # - La matriz movie_features_vectorized ahora contiene el vector numérico para cada película.\n",
        "    # - Las columnas originales 'movie_title' y 'movie_imdb_link' no se incluyeron en el vector,\n",
        "    #   pero son útiles como metadatos para la visualización. Podemos guardarlas por separado\n",
        "    #   o asegurarnos de que se mantengan alineadas con los vectores.\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudieron generar las representaciones vectoriales. Asegúrate de que df_movie_metadata esté cargado y limpio.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38f88a65"
      },
      "source": [
        "## Paso 5: Aplicar Reducción de Dimensionalidad y Visualización\n",
        "\n",
        "Reduciremos las representaciones vectoriales de las películas a 2 o 3 dimensiones y prepararemos los archivos para visualizarlas en el TensorFlow Projector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6716cb6e"
      },
      "source": [
        "### Sub-etapa 5.1: Aplicar Reducción de Dimensionalidad (t-SNE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e108d5db"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "\n",
        "# Asegurarnos de que movie_features_vectorized esté disponible\n",
        "try:\n",
        "    movie_features_vectorized\n",
        "    # Corregir el typo en el mensaje de print\n",
        "    print(f\"Utilizando la matriz de representaciones vectoriales con forma: {movie_features_vectorized.shape}\")\n",
        "except NameError:\n",
        "    print(\"Advertencia: La variable 'movie_features_vectorized' no fue encontrada.\")\n",
        "    print(\"Por favor, ejecuta la celda de Generación de representaciones vectoriales (Sub-etapa 4.1) primero.\")\n",
        "    movie_features_vectorized = None # Asegurar que es None si no está disponible\n",
        "\n",
        "\n",
        "if movie_features_vectorized is not None:\n",
        "    print(\"\\nAplicando t-SNE para reducir la dimensionalidad a 2 componentes (esto puede tardar)...\")\n",
        "\n",
        "    # Inicializar el modelo t-SNE\n",
        "    # n_components: Número de dimensiones de salida (2 para visualización 2D)\n",
        "    # random_state: Para reproducibilidad\n",
        "    # perplexity: Parámetro sensible\n",
        "    # max_iter: Número de iteraciones\n",
        "    # init=\"random\": Usar inicialización aleatoria para matrices dispersas\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=300, init=\"random\")\n",
        "\n",
        "    # Aplicar t-SNE a los datos\n",
        "    movie_tsne_2d = tsne.fit_transform(movie_features_vectorized)\n",
        "\n",
        "    print(\"Reducción de dimensionalidad con t-SNE (2D) completa.\")\n",
        "    print(f\"Forma de la matriz de t-SNE 2D: {movie_tsne_2d.shape}\")\n",
        "\n",
        "    # Si quieres 3D para el proyector:\n",
        "    print(\"\\nAplicando t-SNE para reducir la dimensionalidad a 3 componentes (esto puede tardar)...\")\n",
        "    # n_components: Número de dimensiones de salida (3 para visualización 3D)\n",
        "    # random_state: Para reproducibilidad\n",
        "    # perplexity: Parámetro sensible\n",
        "    # max_iter: Número de iteraciones\n",
        "    # init=\"random\": Usar inicialización aleatoria\n",
        "    tsne_3d = TSNE(n_components=3, random_state=42, perplexity=30, max_iter=300, init=\"random\")\n",
        "    movie_tsne_3d = tsne_3d.fit_transform(movie_features_vectorized)\n",
        "    print(\"Reducción de dimensionalidad con t-SNE (3D) completa.\")\n",
        "    print(f\"Forma de la matriz de t-SNE 3D: {movie_tsne_3d.shape}\")\n",
        "\n",
        "    # Decidimos qué representación usaremos para el proyector (ej. 3D)\n",
        "    movie_embeddings_for_projector = movie_tsne_3d\n",
        "    print(f\"\\nSe usará la representación 3D de t-SNE para el Projector: {movie_embeddings_for_projector.shape}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo aplicar t-SNE. Asegúrate de que las representaciones vectoriales estén disponibles.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e84299f3"
      },
      "source": [
        "### Sub-etapa 5.2: Preparar archivos .tsv para TensorFlow Projector\n",
        "\n",
        "Necesitamos dos archivos:\n",
        "\n",
        "1.  **`vectors.tsv`:** Contiene las coordenadas 2D o 3D para cada punto (película) después de la reducción de dimensionalidad. Cada fila es un punto, cada columna es una dimensión.\n",
        "2.  **`metadata.tsv`:** Contiene los metadatos para cada punto (película). Cada fila corresponde a una película en `vectors.tsv`. La primera fila es el encabezado. Incluiremos columnas interesantes como título de la película, género, puntuación IMDb, y nuestra puntuación de sentimiento predicha."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5933356b"
      },
      "source": [
        "import os\n",
        "import numpy as np # Asegurarse de que numpy esté importado\n",
        "import pandas as pd # Asegurarse de que pandas esté importado\n",
        "\n",
        "# Asegurarnos de que movie_embeddings_for_projector y df_movie_metadata estén disponibles\n",
        "try:\n",
        "    movie_embeddings_for_projector # La matriz de t-SNE 3D o 2D\n",
        "    if 'df_movie_metadata' in globals() and isinstance(df_movie_metadata, pd.DataFrame):\n",
        "        print(f\"Matriz de embeddings para Projector con forma: {movie_embeddings_for_projector.shape}\")\n",
        "        print(\"DataFrame df_movie_metadata encontrado.\")\n",
        "    else:\n",
        "        print(\"Advertencia: La variable df_movie_metadata no fue encontrada o no es un DataFrame.\")\n",
        "        print(\"Por favor, ejecuta las celdas de carga del dataset Kaggle y manejo de faltantes (Paso 0 y Sub-etapa 1.3).\")\n",
        "        df_movie_metadata = None # Asegurar que es None si no está disponible\n",
        "\n",
        "except NameError:\n",
        "    print(\"Advertencia: La variable 'movie_embeddings_for_projector' o 'df_movie_metadata' no fueron encontradas.\")\n",
        "    print(\"Por favor, ejecuta las celdas de Reducción de Dimensionalidad (Sub-etapa 5.1) y carga/limpieza de df_movie_metadata.\")\n",
        "    movie_embeddings_for_projector = None\n",
        "    df_movie_metadata = None\n",
        "\n",
        "\n",
        "# --- Definir la ruta de destino en Google Drive ---\n",
        "# Asegúrate de que Google Drive esté montado (ejecutando la celda del Paso 0)\n",
        "drive_output_path = '/content/drive/MyDrive/Universidad Distrital/Diplomado_AI/Semana1'\n",
        "\n",
        "# Crear la carpeta de destino si no existe\n",
        "if not os.path.exists(drive_output_path):\n",
        "    try:\n",
        "        os.makedirs(drive_output_path)\n",
        "        print(f\"\\nCarpeta de destino en Drive creada: {drive_output_path}\")\n",
        "    except OSError as e:\n",
        "        print(f\"\\nError: No se pudo crear la carpeta en Drive {drive_output_path}. Asegúrate de que Drive esté montado y la ruta sea válida.\")\n",
        "        drive_output_path = None # Anular la ruta si falla la creación\n",
        "\n",
        "if movie_embeddings_for_projector is not None and df_movie_metadata is not None and drive_output_path is not None:\n",
        "\n",
        "    # --- Preparar y guardar el archivo vectors.tsv ---\n",
        "    vectors_file_name = 'vectors.tsv'\n",
        "    vectors_file_path = os.path.join(drive_output_path, vectors_file_name)\n",
        "    print(f\"\\nCreando archivo de vectores en Drive: {vectors_file_path}\")\n",
        "\n",
        "    try:\n",
        "        # Guardar la matriz de embeddings en formato .tsv en Drive\n",
        "        np.savetxt(vectors_file_path, movie_embeddings_for_projector, delimiter='\\t')\n",
        "        print(f\"Archivo '{vectors_file_name}' guardado en Drive.\")\n",
        "        print(f\"Tamaño del archivo de vectores: {os.path.getsize(vectors_file_path)} bytes\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al guardar el archivo de vectores en Drive: {e}\")\n",
        "\n",
        "\n",
        "    # --- Preparar y guardar el archivo metadata.tsv ---\n",
        "    metadata_file_name = 'metadata.tsv'\n",
        "    metadata_file_path = os.path.join(drive_output_path, metadata_file_name)\n",
        "    print(f\"\\nCreando archivo de metadatos en Drive: {metadata_file_path}\")\n",
        "\n",
        "    # Seleccionar las columnas de metadatos a incluir\n",
        "    metadata_columns = ['movie_title', 'genres', 'country', 'language', 'content_rating', 'imdb_score', 'title_year', 'predicted_sentiment_score']\n",
        "    existing_metadata_columns = [col for col in metadata_columns if col in df_movie_metadata.columns]\n",
        "    missing_metadata_columns = [col for col in metadata_columns if col not in df_movie_metadata.columns]\n",
        "\n",
        "    if missing_metadata_columns:\n",
        "        print(f\"Advertencia: Las siguientes columnas de metadatos seleccionadas no se encontraron en df_movie_metadata: {missing_metadata_columns}\")\n",
        "        print(\"Solo se incluirán las columnas existentes.\")\n",
        "        metadata_columns_to_save = existing_metadata_columns\n",
        "    else:\n",
        "        metadata_columns_to_save = metadata_columns\n",
        "\n",
        "    if not metadata_columns_to_save:\n",
        "        print(\"Error: No hay columnas de metadatos válidas para guardar.\")\n",
        "    else:\n",
        "        try:\n",
        "            # Guardar las columnas de metadatos en formato .tsv en Drive\n",
        "            df_movie_metadata[metadata_columns_to_save].to_csv(metadata_file_path, sep='\\t', index=False)\n",
        "            print(f\"Archivo '{metadata_file_name}' guardado en Drive.\")\n",
        "            print(f\"Tamaño del archivo de metadatos: {os.path.getsize(metadata_file_path)} bytes\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al guardar el archivo de metadatos en Drive: {e}\")\n",
        "\n",
        "\n",
        "    # Recomendación para TensorFlow Projector:\n",
        "    # Los archivos ahora están en tu Google Drive en la ruta especificada.\n",
        "    # Puedes ir a Google Drive, navegar a esa carpeta, y descargar los archivos desde allí\n",
        "    # para cargarlos en https://projector.tensorflow.org/\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudieron crear los archivos .tsv. Asegúrate de que las variables necesarias estén disponibles y la ruta de Drive sea válida.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "049c9148"
      },
      "source": [
        "## Vista Unificada del Proceso: Datos y Relaciones\n",
        "\n",
        "A lo largo de este ejercicio, hemos trabajado con diferentes conjuntos de datos y hemos generado nuevas representaciones y características. Podemos pensar en ellos como \"tablas\" conceptuales que se relacionan entre sí:\n",
        "\n",
        "1.  **`imdb_reviews_original`**: El dataset original de reseñas de IMDb (texto y etiqueta de sentimiento).\n",
        "    *   Columnas clave: `text`, `label`.\n",
        "    *   Relación: Cada fila es una reseña.\n",
        "2.  **`imdb_reviews_processed`**: El texto de las reseñas de IMDb después de limpieza, tokenización, eliminación de stop words y lematización.\n",
        "    *   Columnas clave: `processed_text` (string de tokens lematizados).\n",
        "    *   Relación: 1 a 1 con `imdb_reviews_original`.\n",
        "3.  **`imdb_reviews_vectorized`**: La representación TF-IDF de las reseñas de IMDb procesadas.\n",
        "    *   Representación: Matriz dispersa (ej. `tfidf_matrix_full`).\n",
        "    *   Columnas conceptuales: Una columna por cada palabra/lema en el vocabulario TF-IDF.\n",
        "    *   Relación: 1 a 1 con `imdb_reviews_processed`.\n",
        "4.  **`sentiment_model`**: El modelo de clasificación de sentimiento entrenado (Regresión Logística).\n",
        "    *   Representación: Objeto serializado (`.pkl`).\n",
        "    *   Relación: Se entrena usando `imdb_reviews_vectorized` y `imdb_reviews_original['label']`. Se aplica a `df_movie_metadata_processed_keywords`.\n",
        "5.  **`tfidf_vectorizer`**: El vectorizador TF-IDF entrenado.\n",
        "    *   Representación: Objeto serializado (conceptual, aunque lo usamos como variable `tfidf_vectorizer_full`).\n",
        "    *   Relación: Se ajusta en `imdb_reviews_processed` y se usa para transformar `imdb_reviews_processed` y `df_movie_metadata_processed_keywords`.\n",
        "6.  **`df_movie_metadata_original`**: El dataset original de metadatos de Kaggle.\n",
        "    *   Columnas clave: `movie_title`, `genres`, `imdb_score`, `plot_keywords`, etc.\n",
        "    *   Relación: Cada fila es una película.\n",
        "7.  **`df_movie_metadata_cleaned`**: El dataset de metadatos después del manejo de datos faltantes.\n",
        "    *   Relación: Subconjunto de `df_movie_metadata_original` (si se eliminaron filas) o versión imputada. 1 a 1 con las filas restantes/modificadas del original. Lo representamos con la variable `df_movie_metadata`.\n",
        "8.  **`df_movie_metadata_processed_keywords`**: La columna `plot_keywords` del dataset de metadatos después de limpieza y lematización.\n",
        "    *   Columnas clave: `processed_plot_keywords` (string de tokens lematizados).\n",
        "    *   Relación: 1 a 1 con `df_movie_metadata_cleaned`.\n",
        "9.  **`df_movie_metadata_sentiment_inferred`**: El dataset de metadatos con la puntuación de sentimiento predicha añadida.\n",
        "    *   Columnas clave: Todas las de `df_movie_metadata_cleaned` + `predicted_sentiment_score`.\n",
        "    *   Relación: 1 a 1 con `df_movie_metadata_cleaned`. Lo representamos con la variable `df_movie_metadata` después de añadir la columna.\n",
        "10. **`df_movie_metadata_vectorized_features`**: La representación vectorial combinada de las características seleccionadas del dataset de metadatos (numéricas escaladas, categóricas codificadas, sentimiento predicho).\n",
        "    *   Representación: Matriz numérica (ej. `movie_features_vectorized`).\n",
        "    *   Columnas conceptuales: Una columna por cada característica después del preprocesamiento (escalado, OHE).\n",
        "    *   Relación: 1 a 1 con `df_movie_metadata_sentiment_inferred`.\n",
        "11. **`df_movie_metadata_reduced_embeddings`**: La representación vectorial de las películas después de la reducción de dimensionalidad (t-SNE).\n",
        "    *   Representación: Matriz numérica 2D o 3D (ej. `movie_embeddings_for_projector`).\n",
        "    *   Columnas: Coordenadas en el espacio 2D/3D (ej. `Dim1`, `Dim2`, `Dim3`).\n",
        "    *   Relación: 1 a 1 con `df_movie_metadata_vectorized_features` y `df_movie_metadata_sentiment_inferred`.\n",
        "\n",
        "**Relación Clave para Visualización:**\n",
        "\n",
        "Para la visualización en el TensorFlow Projector, la relación principal es entre los **embeddings reducidos (`df_movie_metadata_reduced_embeddings`)** y los **metadatos (`df_movie_metadata_sentiment_inferred`)**. Cada fila en la matriz de embeddings corresponde a la misma película en la misma fila del DataFrame de metadatos.\n",
        "\n",
        "---\n",
        "\n",
        "Ahora, crearemos un DataFrame de pandas que combine las columnas de metadatos clave con las coordenadas 2D/3D de t-SNE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1043115f"
      },
      "source": [
        "### Crear una Tabla Unificada (DataFrame) con Metadatos Clave y Embeddings Reducidos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e8ebbd5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Asegurarnos de que df_movie_metadata y movie_embeddings_for_projector estén disponibles\n",
        "try:\n",
        "    if 'df_movie_metadata' in globals() and isinstance(df_movie_metadata, pd.DataFrame) and 'predicted_sentiment_score' in df_movie_metadata.columns:\n",
        "         print(\"Utilizando el DataFrame df_movie_metadata con la columna 'predicted_sentiment_score'.\")\n",
        "    else:\n",
        "        print(\"Advertencia: La variable df_movie_metadata no fue encontrada, no es un DataFrame o no tiene la columna 'predicted_sentiment_score'.\")\n",
        "        print(\"Por favor, ejecuta las celdas de carga del dataset Kaggle, manejo de faltantes (Paso 0 y Sub-etapa 1.3) y Aplicación del modelo (Sub-etapa 3.3).\")\n",
        "        df_movie_metadata = None # Asegurar que es None si no está disponible\n",
        "\n",
        "    movie_embeddings_for_projector # La matriz de t-SNE 3D o 2D\n",
        "    print(f\"Matriz de embeddings para Projector con forma: {movie_embeddings_for_projector.shape}\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"Advertencia: La variable 'movie_embeddings_for_projector' no fue encontrada.\")\n",
        "    print(\"Por favor, ejecuta las celdas de Reducción de Dimensionalidad (Sub-etapa 5.1) primero.\")\n",
        "    movie_embeddings_for_projector = None\n",
        "\n",
        "\n",
        "if df_movie_metadata is not None and movie_embeddings_for_projector is not None:\n",
        "    # Asegurarse de que el número de filas en el DataFrame y en los embeddings coincida\n",
        "    if len(df_movie_metadata) == movie_embeddings_for_projector.shape[0]:\n",
        "        print(\"\\nCombinando metadatos clave y embeddings reducidos...\")\n",
        "\n",
        "        # Seleccionar columnas de metadatos clave para la tabla unificada\n",
        "        metadata_cols_for_unified_table = ['movie_title', 'genres', 'imdb_score', 'predicted_sentiment_score', 'director_name', 'actor_1_name', 'country', 'language', 'content_rating', 'title_year']\n",
        "        # Asegurarse de que existan en el DataFrame\n",
        "        existing_metadata_cols = [col for col in metadata_cols_for_unified_table if col in df_movie_metadata.columns]\n",
        "        missing_metadata_cols = [col for col in metadata_cols_for_unified_table if col not in df_movie_metadata.columns]\n",
        "\n",
        "        if missing_metadata_cols:\n",
        "             print(f\"Advertencia: Las siguientes columnas de metadatos seleccionadas para la tabla unificada no se encontraron: {missing_metadata_cols}\")\n",
        "             print(\"Solo se incluirán las columnas existentes.\")\n",
        "\n",
        "\n",
        "        # Crear un DataFrame con las coordenadas de los embeddings\n",
        "        # Asumimos 3D para el Projector, ajustar si se usó 2D\n",
        "        if movie_embeddings_for_projector.shape[1] == 3:\n",
        "             df_embeddings = pd.DataFrame(movie_embeddings_for_projector, columns=['TSNE_Dim1', 'TSNE_Dim2', 'TSNE_Dim3'])\n",
        "        elif movie_embeddings_for_projector.shape[1] == 2:\n",
        "             df_embeddings = pd.DataFrame(movie_embeddings_for_projector, columns=['TSNE_Dim1', 'TSNE_Dim2'])\n",
        "        else:\n",
        "             print(f\"Advertencia: Los embeddings tienen {movie_embeddings_for_projector.shape[1]} dimensiones. No se crearon columnas de TSNE con nombres estándar.\")\n",
        "             df_embeddings = pd.DataFrame(movie_embeddings_for_projector) # Crear DataFrame sin nombres de columna específicos\n",
        "\n",
        "        # Combinar las columnas de metadatos seleccionadas con el DataFrame de embeddings\n",
        "        # Usamos el índice para asegurar la alineación correcta\n",
        "        df_unified = pd.concat([df_movie_metadata[existing_metadata_cols].reset_index(drop=True), df_embeddings.reset_index(drop=True)], axis=1)\n",
        "\n",
        "        print(\"Tabla unificada (DataFrame) creada.\")\n",
        "        print(f\"Forma de la tabla unificada: {df_unified.shape}\")\n",
        "\n",
        "        # Mostrar las primeras filas de la tabla unificada\n",
        "        print(\"\\nPrimeras filas de la tabla unificada:\")\n",
        "        display(df_unified.head())\n",
        "\n",
        "        # Recomendación:\n",
        "        # Este DataFrame 'df_unified' contiene la información clave de cada película\n",
        "        # junto con sus coordenadas en el espacio reducido. Es útil para análisis posteriores\n",
        "        # o para generar visualizaciones personalizadas fuera del TensorFlow Projector.\n",
        "\n",
        "    else:\n",
        "        print(\"\\nError: El número de filas en df_movie_metadata y movie_embeddings_for_projector no coincide.\")\n",
        "        print(\"Asegúrate de que el preprocesamiento, vectorización y reducción de dimensionalidad se aplicaron al mismo conjunto de datos.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo crear la tabla unificada. Asegúrate de que df_movie_metadata y movie_embeddings_for_projector estén disponibles.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1366563b"
      },
      "source": [
        "### Exportar DataFrames a CSV en Google Drive\n",
        "\n",
        "Guardaremos las principales tablas (DataFrames de pandas) generadas durante el proceso como archivos CSV en la carpeta especificada en Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42af5c04"
      },
      "source": [
        "import os\n",
        "import pandas as pd # Asegurarse de que pandas esté importado\n",
        "\n",
        "# Definir la ruta de destino en Google Drive\n",
        "# Asegúrate de que Google Drive esté montado (ejecutando la celda del Paso 0)\n",
        "drive_output_path = '/content/drive/MyDrive/Universidad Distrital/Diplomado_AI/Semana1'\n",
        "\n",
        "# Crear la carpeta de destino si no existe (ya deberíamos haberla creado antes, pero es seguro verificar)\n",
        "if not os.path.exists(drive_output_path):\n",
        "    try:\n",
        "        os.makedirs(drive_output_path)\n",
        "        print(f\"\\nCarpeta de destino en Drive creada: {drive_output_path}\")\n",
        "    except OSError as e:\n",
        "        print(f\"\\nError: No se pudo crear la carpeta en Drive {drive_output_path}. Asegúrate de que Drive esté montado y la ruta sea válida.\")\n",
        "        drive_output_path = None # Anular la ruta si falla la creación\n",
        "\n",
        "\n",
        "if drive_output_path is not None:\n",
        "    print(f\"\\nExportando DataFrames a CSV en: {drive_output_path}\")\n",
        "\n",
        "    # --- Exportar df_movie_metadata (con score de sentimiento) ---\n",
        "    try:\n",
        "        if 'df_movie_metadata' in globals() and isinstance(df_movie_metadata, pd.DataFrame):\n",
        "            metadata_output_filename = 'df_movie_metadata_with_sentiment.csv'\n",
        "            metadata_output_path = os.path.join(drive_output_path, metadata_output_filename)\n",
        "            print(f\"Guardando '{metadata_output_filename}'...\")\n",
        "            df_movie_metadata.to_csv(metadata_output_path, index=False)\n",
        "            print(f\"'{metadata_output_filename}' guardado exitosamente.\")\n",
        "        else:\n",
        "            print(\"Advertencia: La variable df_movie_metadata no fue encontrada o no es un DataFrame. No se exportará.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al exportar df_movie_metadata: {e}\")\n",
        "\n",
        "\n",
        "    # --- Exportar la tabla unificada (df_unified) ---\n",
        "    try:\n",
        "        if 'df_unified' in globals() and isinstance(df_unified, pd.DataFrame):\n",
        "            unified_output_filename = 'df_unified_metadata_embeddings.csv'\n",
        "            unified_output_path = os.path.join(drive_output_path, unified_output_filename)\n",
        "            print(f\"Guardando '{unified_output_filename}'...\")\n",
        "            df_unified.to_csv(unified_output_path, index=False)\n",
        "            print(f\"'{unified_output_filename}' guardado exitosamente.\")\n",
        "        else:\n",
        "            print(\"Advertencia: La variable df_unified no fue encontrada o no es un DataFrame. No se exportará.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al exportar df_unified: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudieron exportar los DataFrames porque la ruta de Drive no es válida.\")\n",
        "\n",
        "# Recomendación:\n",
        "# Ahora puedes encontrar estos archivos CSV en tu Google Drive en la ruta especificada.\n",
        "# Estos archivos contienen los datos procesados y las características generadas."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fc38383"
      },
      "source": [
        "### Exportar Todos los Artefactos Clave a Google Drive\n",
        "\n",
        "Guardaremos varios resultados intermedios y finales del proceso en una subcarpeta específica en Google Drive para su posterior análisis o uso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11d5b5c3"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle # Para guardar objetos Python como el vectorizador\n",
        "\n",
        "# Definir la ruta base de destino en Google Drive\n",
        "# Asegúrate de que Google Drive esté montado (ejecutando la celda del Paso 0)\n",
        "drive_base_path = '/content/drive/MyDrive/Universidad Distrital/Diplomado_AI/Semana1'\n",
        "output_data_subdir = 'DataSets' # La nueva subcarpeta\n",
        "drive_output_path = os.path.join(drive_base_path, output_data_subdir)\n",
        "\n",
        "# Crear la carpeta de destino si no existe\n",
        "if not os.path.exists(drive_output_path):\n",
        "    try:\n",
        "        os.makedirs(drive_output_path)\n",
        "        print(f\"\\nCarpeta de destino en Drive creada: {drive_output_path}\")\n",
        "    except OSError as e:\n",
        "        print(f\"\\nError: No se pudo crear la carpeta en Drive {drive_output_path}. Asegúrate de que Drive esté montado y la ruta base sea válida.\")\n",
        "        drive_output_path = None # Anular la ruta si falla la creación\n",
        "\n",
        "\n",
        "if drive_output_path is not None:\n",
        "    print(f\"\\nExportando artefactos de datos a: {drive_output_path}\")\n",
        "\n",
        "    # --- Artefactos relacionados con imdb_reviews ---\n",
        "\n",
        "    # 1. Texto procesado de las reseñas (imdb_reviews_processed)\n",
        "    try:\n",
        "        # Necesita processed_texts_list de Sub-etapa 1.5 (celda 78c02b5d)\n",
        "        if 'processed_texts_list' in globals() and isinstance(processed_texts_list, list):\n",
        "            processed_reviews_filename = 'imdb_reviews_processed.csv' # Guardar como CSV\n",
        "            processed_reviews_path = os.path.join(drive_output_path, processed_reviews_filename)\n",
        "            print(f\"Guardando '{processed_reviews_filename}'...\")\n",
        "            # Convertir a DataFrame para guardar fácilmente como CSV\n",
        "            df_processed_reviews = pd.DataFrame({'processed_text': processed_texts_list})\n",
        "            # Opcional: Añadir etiquetas si también las recolectamos en Sub-etapa 1.5 (labels_list)\n",
        "            # if 'labels_list' in globals() and len(labels_list) == len(processed_texts_list):\n",
        "            #     df_processed_reviews['label'] = labels_list\n",
        "            df_processed_reviews.to_csv(processed_reviews_path, index=False)\n",
        "            print(f\"'{processed_reviews_filename}' guardado exitosamente.\")\n",
        "        else:\n",
        "            print(\"Advertencia: La variable processed_texts_list no fue encontrada o no es una lista. No se exportará el texto procesado de reseñas.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al exportar el texto procesado de reseñas: {e}\")\n",
        "\n",
        "\n",
        "    # 2. Matriz TF-IDF completa de las reseñas (imdb_reviews_vectorized)\n",
        "    try:\n",
        "        # Necesita tfidf_matrix_full de Sub-etapa 1.5 (celda 78c02b5d)\n",
        "        if 'tfidf_matrix_full' in globals(): # tfidf_matrix_full es una matriz dispersa\n",
        "             tfidf_matrix_filename = 'imdb_reviews_vectorized_tfidf_matrix.npz' # Formato para matrices dispersas\n",
        "             tfidf_matrix_path = os.path.join(drive_output_path, tfidf_matrix_filename)\n",
        "             print(f\"Guardando '{tfidf_matrix_filename}'...\")\n",
        "             # Guardar matriz dispersa usando scipy.sparse.save_npz (necesita importar scipy)\n",
        "             from scipy.sparse import save_npz\n",
        "             save_npz(tfidf_matrix_path, tfidf_matrix_full)\n",
        "             print(f\"'{tfidf_matrix_filename}' guardado exitosamente.\")\n",
        "        else:\n",
        "            print(\"Advertencia: La variable tfidf_matrix_full no fue encontrada. No se exportará la matriz TF-IDF completa.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al exportar la matriz TF-IDF: {e}\")\n",
        "\n",
        "\n",
        "    # 3. Modelo de sentimiento entrenado (sentiment_model)\n",
        "    # Ya lo guardamos como 'logistic_regression_sentiment_model.pkl' en la Sub-etapa 2.4\n",
        "    # Podemos copiarlo o simplemente asegurarnos de que esté en la ruta correcta si lo re-guardamos.\n",
        "    # Vamos a re-guardarlo en la nueva subcarpeta para consistencia.\n",
        "    try:\n",
        "        # Necesita model de Sub-etapa 2.2 (celda 4d393ade) o loaded_model de Sub-etapa 3.1 (celda ace7c385)\n",
        "        # Usaremos la variable 'model' que es el modelo entrenado\n",
        "        if 'model' in globals():\n",
        "            model_filename_pkl = 'sentiment_model_logistic_regression.pkl'\n",
        "            model_path_pkl = os.path.join(drive_output_path, model_filename_pkl)\n",
        "            print(f\"Guardando '{model_filename_pkl}'...\")\n",
        "            with open(model_path_pkl, 'wb') as f:\n",
        "                pickle.dump(model, f)\n",
        "            print(f\"'{model_filename_pkl}' guardado exitosamente.\")\n",
        "        else:\n",
        "             print(\"Advertencia: La variable 'model' (modelo entrenado) no fue encontrada. No se exportará el modelo.\")\n",
        "    except Exception as e:\n",
        "         print(f\"Error al exportar el modelo: {e}\")\n",
        "\n",
        "\n",
        "    # 4. Vectorizador TF-IDF entrenado (tfidf_vectorizer)\n",
        "    try:\n",
        "        # Necesita tfidf_vectorizer_full de Sub-etapa 1.5 (celda 78c02b5d)\n",
        "        if 'tfidf_vectorizer_full' in globals():\n",
        "            vectorizer_filename_pkl = 'tfidf_vectorizer_full.pkl'\n",
        "            vectorizer_path_pkl = os.path.join(drive_output_path, vectorizer_filename_pkl)\n",
        "            print(f\"Guardando '{vectorizer_filename_pkl}'...\")\n",
        "            with open(vectorizer_path_pkl, 'wb') as f:\n",
        "                pickle.dump(tfidf_vectorizer_full, f)\n",
        "            print(f\"'{vectorizer_filename_pkl}' guardado exitosamente.\")\n",
        "        else:\n",
        "            print(\"Advertencia: La variable tfidf_vectorizer_full no fue encontrada. No se exportará el vectorizador.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al exportar el vectorizador TF-IDF: {e}\")\n",
        "\n",
        "\n",
        "    # --- Artefactos relacionados con df_movie_metadata ---\n",
        "\n",
        "    # 5. Texto procesado de las palabras clave (df_movie_metadata_processed_keywords)\n",
        "    try:\n",
        "        # Necesita processed_keywords_list de Sub-etapa 3.2 (celda 81b69a34)\n",
        "        if 'processed_keywords_list' in globals() and isinstance(processed_keywords_list, list):\n",
        "            processed_keywords_filename = 'movie_keywords_processed.csv' # Guardar como CSV\n",
        "            processed_keywords_path = os.path.join(drive_output_path, processed_keywords_filename)\n",
        "            print(f\"Guardando '{processed_keywords_filename}'...\")\n",
        "            df_processed_keywords = pd.DataFrame({'processed_keywords': processed_keywords_list})\n",
        "            df_processed_keywords.to_csv(processed_keywords_path, index=False)\n",
        "            print(f\"'{processed_keywords_filename}' guardado exitosamente.\")\n",
        "        else:\n",
        "            print(\"Advertencia: La variable processed_keywords_list no fue encontrada o no es una lista. No se exportará el texto procesado de keywords.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al exportar el texto procesado de keywords: {e}\")\n",
        "\n",
        "\n",
        "    # 6. DataFrame de metadatos con puntuación de sentimiento (df_movie_metadata_sentiment_inferred)\n",
        "    # Ya lo guardamos como 'df_movie_metadata_with_sentiment.csv' en la celda 42af5c04\n",
        "    # Vamos a re-guardarlo en la nueva subcarpeta.\n",
        "    try:\n",
        "        # Necesita df_movie_metadata de Sub-etapa 3.3 (celda d02357a2), que incluye la columna 'predicted_sentiment_score'\n",
        "        if 'df_movie_metadata' in globals() and isinstance(df_movie_metadata, pd.DataFrame) and 'predicted_sentiment_score' in df_movie_metadata.columns:\n",
        "            metadata_sentiment_filename_csv = 'df_movie_metadata_with_sentiment.csv'\n",
        "            metadata_sentiment_path_csv = os.path.join(drive_output_path, metadata_sentiment_filename_csv)\n",
        "            print(f\"Guardando '{metadata_sentiment_filename_csv}'...\")\n",
        "            df_movie_metadata.to_csv(metadata_sentiment_path_csv, index=False)\n",
        "            print(f\"'{metadata_sentiment_filename_csv}' guardado exitosamente.\")\n",
        "        else:\n",
        "            print(\"Advertencia: La variable df_movie_metadata (con score de sentimiento) no fue encontrada. No se exportará el DataFrame de metadatos con sentimiento.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al exportar df_movie_metadata con sentimiento: {e}\")\n",
        "\n",
        "\n",
        "    # 7. Matriz de características vectorizadas de metadatos (df_movie_metadata_vectorized_features)\n",
        "    try:\n",
        "        # Necesita movie_features_vectorized de Sub-etapa 4.1 (celda 6cb10795)\n",
        "        if 'movie_features_vectorized' in globals(): # movie_features_vectorized es una matriz dispersa\n",
        "            metadata_features_filename = 'movie_features_vectorized.npz' # Formato para matrices dispersas\n",
        "            metadata_features_path = os.path.join(drive_output_path, metadata_features_filename)\n",
        "            print(f\"Guardando '{metadata_features_filename}'...\")\n",
        "            from scipy.sparse import save_npz # Importar de nuevo por si acaso\n",
        "            save_npz(metadata_features_path, movie_features_vectorized)\n",
        "            print(f\"'{metadata_features_filename}' guardado exitosamente.\")\n",
        "        else:\n",
        "            print(\"Advertencia: La variable movie_features_vectorized no fue encontrada. No se exportará la matriz de características vectorizadas de metadatos.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al exportar la matriz de características vectorizadas de metadatos: {e}\")\n",
        "\n",
        "\n",
        "    # 8. Embeddings reducidos de metadatos (df_movie_metadata_reduced_embeddings)\n",
        "    # Ya lo guardamos como 'vectors.tsv' en la celda 5933356b\n",
        "    # Vamos a re-guardarlo en la nueva subcarpeta.\n",
        "    try:\n",
        "        # Necesita movie_embeddings_for_projector de Sub-etapa 5.1 (celda e108d5db)\n",
        "        if 'movie_embeddings_for_projector' in globals(): # Es un array numpy denso\n",
        "             embeddings_filename_tsv = 'movie_embeddings_reduced.tsv' # Mantener formato tsv para Projector\n",
        "             embeddings_path_tsv = os.path.join(drive_output_path, embeddings_filename_tsv)\n",
        "             print(f\"Guardando '{embeddings_filename_tsv}'...\")\n",
        "             np.savetxt(embeddings_path_tsv, movie_embeddings_for_projector, delimiter='\\t')\n",
        "             print(f\"'{embeddings_filename_tsv}' guardado exitosamente.\")\n",
        "\n",
        "             # Opcional: También guardar el metadata.tsv correspondiente en la misma subcarpeta\n",
        "             # Necesita df_movie_metadata de Sub-etapa 3.3 (celda d02357a2)\n",
        "             if 'df_movie_metadata' in globals() and isinstance(df_movie_metadata, pd.DataFrame):\n",
        "                metadata_projector_filename = 'metadata.tsv' # Mantener nombre para Projector\n",
        "                metadata_projector_path = os.path.join(drive_output_path, metadata_projector_filename)\n",
        "                print(f\"Guardando '{metadata_projector_filename}' para Projector...\")\n",
        "                # Seleccionar las columnas de metadatos a incluir (mismas que en Sub-etapa 5.2)\n",
        "                metadata_columns_projector = ['movie_title', 'genres', 'country', 'language', 'content_rating', 'imdb_score', 'title_year', 'predicted_sentiment_score']\n",
        "                existing_metadata_columns_projector = [col for col in metadata_columns_projector if col in df_movie_metadata.columns]\n",
        "                if existing_metadata_columns_projector:\n",
        "                    df_movie_metadata[existing_metadata_columns_projector].to_csv(metadata_projector_path, sep='\\t', index=False)\n",
        "                    print(f\"'{metadata_projector_filename}' guardado exitosamente.\")\n",
        "                else:\n",
        "                    print(\"Advertencia: No hay columnas de metadatos válidas para guardar metadata.tsv para Projector.\")\n",
        "             else:\n",
        "                print(\"Advertencia: df_movie_metadata no está disponible para guardar metadata.tsv para Projector.\")\n",
        "\n",
        "        else:\n",
        "            print(\"Advertencia: La variable movie_embeddings_for_projector no fue encontrada. No se exportarán los embeddings reducidos.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al exportar los embeddings reducidos o metadata.tsv: {e}\")\n",
        "\n",
        "\n",
        "    # 9. Tabla unificada (df_unified)\n",
        "    # Ya lo guardamos como 'df_unified_metadata_embeddings.csv' en la celda 42af5c04\n",
        "    # Vamos a re-guardarlo en la nueva subcarpeta.\n",
        "    try:\n",
        "        # Necesita df_unified de la celda 0e8ebbd5\n",
        "        if 'df_unified' in globals() and isinstance(df_unified, pd.DataFrame):\n",
        "            unified_filename_csv = 'df_unified_metadata_embeddings.csv'\n",
        "            unified_path_csv = os.path.join(drive_output_path, unified_filename_csv)\n",
        "            print(f\"Guardando '{unified_filename_csv}'...\")\n",
        "            df_unified.to_csv(unified_path_csv, index=False)\n",
        "            print(f\"'{unified_filename_csv}' guardado exitosamente.\")\n",
        "        else:\n",
        "            print(\"Advertencia: La variable df_unified no fue encontrada o no es un DataFrame. No se exportará la tabla unificada.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al exportar la tabla unificada: {e}\")\n",
        "\n",
        "\n",
        "    print(\"\\nProceso de exportación de artefactos de datos completado.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudieron exportar los artefactos de datos porque la ruta de Drive no es válida.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a68ce72"
      },
      "source": [
        "# Nivel 1: Uso de abstracciones con conciencia de lo subyacente\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch # PyTorch es comúnmente usado con Hugging Face Transformers\n",
        "import numpy as np # Para manejar los embeddings resultantes\n",
        "import pandas as pd # Para cargar el archivo CSV desde Drive\n",
        "import os # Para unir rutas\n",
        "\n",
        "# Definir la ruta del archivo CSV con el texto procesado en Google Drive\n",
        "drive_data_path = '/content/drive/MyDrive/Universidad Distrital/Diplomado_AI/Semana1/DataSets'\n",
        "processed_reviews_filename = 'imdb_reviews_processed.csv'\n",
        "processed_reviews_filepath = os.path.join(drive_data_path, processed_reviews_filename)\n",
        "\n",
        "# --- Cargar el dataset de texto procesado desde Google Drive ---\n",
        "print(f\"Cargando texto procesado desde: {processed_reviews_filepath}\")\n",
        "try:\n",
        "    # Asegurarse de que Google Drive esté montado (ejecutar la celda del Paso 0 si es necesario)\n",
        "    # y que el archivo exista en la ruta especificada.\n",
        "    df_processed_reviews = pd.read_csv(processed_reviews_filepath)\n",
        "    print(f\"Dataset '{processed_reviews_filename}' cargado exitosamente.\")\n",
        "    print(f\"Forma del DataFrame cargado: {df_processed_reviews.shape}\")\n",
        "\n",
        "    # Extraer la columna de texto procesado\n",
        "    # Asumimos que la columna se llama 'processed_text' como la guardamos\n",
        "    if 'processed_text' in df_processed_reviews.columns:\n",
        "        all_processed_texts = df_processed_reviews['processed_text'].tolist()\n",
        "        print(f\"Extraídos {len(all_processed_texts)} textos procesados.\")\n",
        "        # Manejar posibles valores NaN que read_csv podría haber introducido si la columna tenía vacíos\n",
        "        all_processed_texts = [text if isinstance(text, str) else \"\" for text in all_processed_texts]\n",
        "\n",
        "    else:\n",
        "        print(f\"Error: La columna 'processed_text' no fue encontrada en '{processed_reviews_filename}'.\")\n",
        "        all_processed_texts = [] # Usar lista vacía para evitar errores posteriores\n",
        "        df_processed_reviews = None # Set df to None to indicate data issue\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: El archivo '{processed_reviews_filepath}' no fue encontrado.\")\n",
        "    print(\"Asegúrate de que Google Drive esté montado y el archivo exista en esa ruta.\")\n",
        "    all_processed_texts = []\n",
        "    df_processed_reviews = None\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar o procesar el archivo CSV: {e}\")\n",
        "    all_processed_texts = []\n",
        "    df_processed_reviews = None\n",
        "\n",
        "\n",
        "if all_processed_texts:\n",
        "    # --- Cargar un modelo y tokenizador pre-entrenados ---\n",
        "    # Elegimos un modelo Transformer pequeño y rápido para demostración\n",
        "    model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "    print(f\"\\nCargando tokenizador y modelo Transformer: {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "    print(\"Tokenizador y modelo cargados.\")\n",
        "\n",
        "    # --- Preparar una muestra de texto para obtener embeddings ---\n",
        "    # Usaremos una muestra del texto cargado para evitar procesar todo el dataset grande\n",
        "    sample_size_for_embeddings = 10 # Reducir el tamaño de la muestra para una ejecución más rápida\n",
        "    if len(all_processed_texts) > sample_size_for_embeddings:\n",
        "        sample_texts_for_embeddings = all_processed_texts[:sample_size_for_embeddings]\n",
        "        print(f\"\\nUsando una muestra de {sample_size_for_embeddings} textos para obtener embeddings.\")\n",
        "    else:\n",
        "        sample_texts_for_embeddings = all_processed_texts\n",
        "        print(f\"\\nUsando todos los {len(all_processed_texts)} textos cargados para obtener embeddings.\")\n",
        "\n",
        "\n",
        "    print(f\"Ejemplos de textos para obtener embeddings ({len(sample_texts_for_embeddings)}):\")\n",
        "    for i, review in enumerate(sample_texts_for_embeddings):\n",
        "        print(f\"Texto {i+1}: {review[:200]}...\") # Mostrar solo los primeros 200 caracteres\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "\n",
        "    # --- Tokenizar los textos ---\n",
        "    # `padding='max_length'` asegura que todas las secuencias tengan la misma longitud\n",
        "    # `truncation=True` corta las secuencias si son más largas que el largo máximo del modelo\n",
        "    # `return_tensors=\"pt\"` devuelve tensores de PyTorch\n",
        "    print(\"\\nTokenizando textos...\")\n",
        "    # Handle potential empty strings in the sample_texts_for_embeddings list\n",
        "    inputs = tokenizer(sample_texts_for_embeddings, padding=True, truncation=True, return_tensors=\"pt\") # Use padding=True for dynamic padding\n",
        "\n",
        "\n",
        "    print(\"Tokenización completa. Forma de los inputs (input_ids):\", inputs['input_ids'].shape)\n",
        "\n",
        "\n",
        "    # --- Obtener los embeddings del modelo ---\n",
        "    # Mover el modelo a la GPU si está disponible\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()} # Mover inputs a la GPU\n",
        "\n",
        "    print(f\"\\nObteniendo embeddings del modelo Transformer en {device}...\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Los embeddings de la última capa suelen estar en outputs.last_hidden_state\n",
        "    # Obtener el embedding del token [CLS] para cada secuencia\n",
        "    cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "    # Convertir los embeddings de PyTorch a un array de NumPy (mover de nuevo a CPU si estaba en GPU)\n",
        "    cls_embeddings_np = cls_embeddings.cpu().numpy()\n",
        "\n",
        "    print(\"Obtención de embeddings completa.\")\n",
        "    print(f\"Forma de los embeddings [CLS]: {cls_embeddings_np.shape}\") # (Número de textos, Tamaño del embedding)\n",
        "\n",
        "    print(\"\\nPrimer embedding [CLS] (primeros 20 valores):\")\n",
        "    print(cls_embeddings_np[0, :20])\n",
        "    print(\"...\")\n",
        "\n",
        "\n",
        "    # Recomendación para la Capa 1:\n",
        "    # Ahora tienes representaciones vectoriales de alta dimensión (embeddings) para cada texto procesado de la muestra cargada.\n",
        "    # Puedes usar estos embeddings como características para entrenar un clasificador (ej. Regresión Logística, SVM, etc.).\n",
        "    # Para la Capa 2, podrías profundizar en:\n",
        "    # - Cómo funcionan los tokenizadores (Subword tokenization).\n",
        "    # - La arquitectura interna del modelo Transformer (Atención, Capas Feed-Forward).\n",
        "    # - El proceso de pre-entrenamiento (Masked Language Modeling, Next Sentence Prediction).\n",
        "    # - Cómo se obtienen diferentes tipos de embeddings (promedio de palabras, [CLS] token, etc.).\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo obtener embeddings porque no se cargaron textos procesados desde Google Drive.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc7090af"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd # Para manejar etiquetas\n",
        "\n",
        "# Asegurarnos de que los embeddings del Transformer estén disponibles\n",
        "try:\n",
        "    # cls_embeddings_np is from the last executed cell (7a68ce72)\n",
        "    cls_embeddings_np\n",
        "    print(f\"Embeddings del Transformer encontrados con forma: {cls_embeddings_np.shape}\")\n",
        "except NameError:\n",
        "    print(\"Advertencia: La variable 'cls_embeddings_np' (embeddings del Transformer) no fue encontrada.\")\n",
        "    print(\"Por favor, ejecuta la celda anterior (la que usa Hugging Face Transformers) para generarlos.\")\n",
        "    cls_embeddings_np = None # Asegurar que es None si no está disponible\n",
        "\n",
        "\n",
        "# Asegurarnos de tener acceso a las etiquetas para la muestra (asumiendo que el dataset original está cargado)\n",
        "# NOTA: Esto asume que el orden de los embeddings en cls_embeddings_np corresponde al orden de los primeros\n",
        "# 'sample_size_for_embeddings' elementos del dataset original. Esto puede no ser robusto si el preprocesamiento\n",
        "# alteró el orden o si se usó una muestra diferente. Para un flujo de trabajo robusto, las etiquetas deberían\n",
        "# guardarse junto con los textos procesados.\n",
        "try:\n",
        "    dataset # Variable del dataset original de tfds (celda 4469e4b8)\n",
        "    # Tomar las etiquetas correspondientes a la muestra\n",
        "    sample_size = cls_embeddings_np.shape[0] # Usar el tamaño real de la muestra de embeddings\n",
        "    sample_labels = [label.numpy() for text, label in dataset.take(sample_size)]\n",
        "    print(f\"Etiquetas correspondientes a la muestra obtenidas ({len(sample_labels)} etiquetas).\")\n",
        "    sample_labels_array = np.array(sample_labels)\n",
        "\n",
        "except NameError:\n",
        "    print(\"Advertencia: La variable 'dataset' (dataset original de tfds) no fue encontrada.\")\n",
        "    print(\"No se pudieron obtener las etiquetas correspondientes a los embeddings de la muestra.\")\n",
        "    sample_labels_array = None\n",
        "except Exception as e:\n",
        "    print(f\"Error al obtener las etiquetas de la muestra: {e}\")\n",
        "    sample_labels_array = None\n",
        "\n",
        "\n",
        "if cls_embeddings_np is not None:\n",
        "    print(\"\\nAplicando PCA para reducción de dimensionalidad...\")\n",
        "\n",
        "    # --- Aplicar PCA a 2 dimensiones ---\n",
        "    pca_2d = PCA(n_components=2, random_state=42)\n",
        "    embeddings_pca_2d = pca_2d.fit_transform(cls_embeddings_np)\n",
        "\n",
        "    print(f\"PCA a 2D completo. Forma: {embeddings_pca_2d.shape}\")\n",
        "    # print(f\"Varianza explicada por 2 componentes: {pca_2d.explained_variance_ratio_.sum():.4f}\")\n",
        "\n",
        "\n",
        "    # --- Aplicar PCA a 3 dimensiones ---\n",
        "    pca_3d = PCA(n_components=3, random_state=42)\n",
        "    embeddings_pca_3d = pca_3d.fit_transform(cls_embeddings_np)\n",
        "\n",
        "    print(f\"PCA a 3D completo. Forma: {embeddings_pca_3d.shape}\")\n",
        "    # print(f\"Varianza explicada por 3 componentes: {pca_3d.explained_variance_ratio_.sum():.4f}\")\n",
        "\n",
        "\n",
        "    # --- Visualización en Colab (2D y 3D) ---\n",
        "    print(\"\\nVisualizando embeddings reducidos (muestra pequeña)...\")\n",
        "\n",
        "    # Aplicar el estilo oscuro si ya fue configurado\n",
        "    plt.style.use('dark_background')\n",
        "\n",
        "    # Visualización 2D\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    # Si tenemos etiquetas, podemos usar colores para diferenciar clases\n",
        "    if sample_labels_array is not None:\n",
        "        # Cambiar colormap a uno que contraste mejor con fondo oscuro\n",
        "        scatter = plt.scatter(embeddings_pca_2d[:, 0], embeddings_pca_2d[:, 1], c=sample_labels_array, cmap='viridis') # Usar 'viridis'\n",
        "        plt.colorbar(scatter, label='Etiqueta (0: Negativo, 1: Positivo)')\n",
        "    else:\n",
        "        plt.scatter(embeddings_pca_2d[:, 0], embeddings_pca_2d[:, 1], color='skyblue') # Color por defecto si no hay etiquetas\n",
        "    plt.title('PCA 2D de Embeddings del Transformer (Muestra)')\n",
        "    plt.xlabel('Componente Principal 1')\n",
        "    plt.ylabel('Componente Principal 2')\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nExplicación de lo que verías con un dataset más grande (Visualización 2D):\")\n",
        "    print(\"Con un conjunto de datos más grande, esperarías ver grupos (clusters) de puntos.\")\n",
        "    print(\"Los puntos que están cerca unos de otros en este espacio 2D/3D representan reseñas que el modelo Transformer considera semánticamente similares.\")\n",
        "    print(\"Si usas colores basados en la etiqueta de sentimiento (0 o 1), podrías observar si el modelo Transformer ha separado bien las reseñas positivas de las negativas en este espacio.\")\n",
        "    print(\"PCA intenta preservar la varianza global de los datos, por lo que los ejes (Componente Principal 1 y 2) representan las direcciones de mayor variabilidad en los embeddings originales.\")\n",
        "\n",
        "\n",
        "    # Visualización 3D (requiere instalar matplotlib con soporte 3D si no está ya)\n",
        "    # from mpl_toolkits.mplot3d import Axes3D # Descomentar si es necesario\n",
        "\n",
        "    print(\"\\nVisualizando embeddings reducidos en 3D (muestra pequeña)...\")\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    if sample_labels_array is not None:\n",
        "         # Cambiar colormap a uno que contraste mejor con fondo oscuro\n",
        "         scatter = ax.scatter(embeddings_pca_3d[:, 0], embeddings_pca_3d[:, 1], embeddings_pca_3d[:, 2], c=sample_labels_array, cmap='viridis') # Usar 'viridis'\n",
        "         fig.colorbar(scatter, label='Etiqueta (0: Negativo, 1: Positivo)')\n",
        "    else:\n",
        "         ax.scatter(embeddings_pca_3d[:, 0], embeddings_pca_3d[:, 1], embeddings_pca_3d[:, 2], color='skyblue') # Color por defecto\n",
        "    ax.set_title('PCA 3D de Embeddings del Transformer (Muestra)')\n",
        "    ax.set_xlabel('Componente Principal 1')\n",
        "    ax.set_ylabel('Componente Principal 2')\n",
        "    ax.set_zlabel('Componente Principal 3')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nExplicación de lo que verías con un dataset más grande (Visualización 3D):\")\n",
        "    print(\"Similar a la vista 2D, la vista 3D te permite explorar la agrupación de reseñas semánticamente similares.\")\n",
        "    print(\"Al poder rotar el gráfico, a veces se pueden identificar separaciones o estructuras que no son visibles en 2D.\")\n",
        "    print(\"Los clusters en 3D también indican grupos de reseñas con características semánticas compartidas según el modelo Transformer.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo realizar la reducción de dimensionalidad y visualización porque los embeddings del Transformer no están disponibles.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18fe25c3"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Definir la ruta de destino en Google Drive\n",
        "# Asegúrate de que Google Drive esté montado (ejecutando la celda del Paso 0)\n",
        "drive_base_path = '/content/drive/MyDrive/Universidad Distrital/Diplomado_AI/Semana1'\n",
        "output_data_subdir = 'DataSets' # La subcarpeta donde guardaremos los archivos\n",
        "drive_output_path = os.path.join(drive_base_path, output_data_subdir)\n",
        "\n",
        "# Crear la carpeta de destino si no existe\n",
        "if not os.path.exists(drive_output_path):\n",
        "    try:\n",
        "        os.makedirs(drive_output_path)\n",
        "        print(f\"\\nCarpeta de destino en Drive creada: {drive_output_path}\")\n",
        "    except OSError as e:\n",
        "        print(f\"\\nError: No se pudo crear la carpeta en Drive {drive_output_path}. Asegúrate de que Drive esté montado y la ruta base sea válida.\")\n",
        "        drive_output_path = None # Anular la ruta si falla la creación\n",
        "\n",
        "\n",
        "# Asegurarnos de que los embeddings reducidos y las etiquetas estén disponibles\n",
        "try:\n",
        "    # embeddings_pca_3d is from the previous cell\n",
        "    embeddings_pca_3d\n",
        "    # sample_labels_array is from the previous cell\n",
        "    sample_labels_array\n",
        "    if embeddings_pca_3d is not None and sample_labels_array is not None:\n",
        "        print(f\"Embeddings PCA 3D encontrados con forma: {embeddings_pca_3d.shape}\")\n",
        "        print(f\"Etiquetas de la muestra encontradas con forma: {sample_labels_array.shape}\")\n",
        "        # Verificar que el número de embeddings y etiquetas coincida\n",
        "        if embeddings_pca_3d.shape[0] != sample_labels_array.shape[0]:\n",
        "            print(\"Advertencia: El número de embeddings reducidos no coincide con el número de etiquetas.\")\n",
        "            print(\"No se podrán generar los archivos .tsv para el Projector con metadatos correctos.\")\n",
        "            embeddings_pca_3d = None # Anular si no coinciden para evitar errores\n",
        "            sample_labels_array = None\n",
        "    else:\n",
        "         embeddings_pca_3d = None\n",
        "         sample_labels_array = None\n",
        "\n",
        "\n",
        "except NameError:\n",
        "    print(\"Advertencia: Las variables 'embeddings_pca_3d' o 'sample_labels_array' no fueron encontradas.\")\n",
        "    print(\"Por favor, ejecuta la celda anterior (PCA y visualización en Colab) para generarlas.\")\n",
        "    embeddings_pca_3d = None\n",
        "    sample_labels_array = None\n",
        "\n",
        "\n",
        "if embeddings_pca_3d is not None and sample_labels_array is not None and drive_output_path is not None:\n",
        "\n",
        "    print(\"\\nPreparando archivos .tsv para TensorFlow Projector...\")\n",
        "\n",
        "    # --- Preparar y guardar el archivo vectors.tsv ---\n",
        "    # Usaremos los embeddings 3D para el Projector\n",
        "    vectors_file_name = 'transformer_pca_vectors_sample.tsv' # Nombre específico\n",
        "    vectors_file_path = os.path.join(drive_output_path, vectors_file_name)\n",
        "    print(f\"Creando archivo de vectores en Drive: {vectors_file_path}\")\n",
        "\n",
        "    try:\n",
        "        # Guardar la matriz de embeddings en formato .tsv en Drive\n",
        "        np.savetxt(vectors_file_path, embeddings_pca_3d, delimiter='\\t')\n",
        "        print(f\"Archivo '{vectors_file_name}' guardado en Drive.\")\n",
        "        print(f\"Tamaño del archivo de vectores: {os.path.getsize(vectors_file_path)} bytes\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al guardar el archivo de vectores en Drive: {e}\")\n",
        "\n",
        "\n",
        "    # --- Preparar y guardar el archivo metadata.tsv ---\n",
        "    metadata_file_name = 'transformer_pca_metadata_sample.tsv' # Nombre específico\n",
        "    metadata_file_path = os.path.join(drive_output_path, metadata_file_name)\n",
        "    print(f\"\\nCreando archivo de metadatos en Drive: {metadata_file_path}\")\n",
        "\n",
        "    # Para este ejemplo, los metadatos serán simplemente las etiquetas de sentimiento\n",
        "    # En un caso real, podrías incluir el texto original, texto procesado, etc.\n",
        "    df_metadata = pd.DataFrame({'sentiment_label': sample_labels_array})\n",
        "\n",
        "    try:\n",
        "        # Guardar el DataFrame de metadatos en formato .tsv en Drive\n",
        "        # index=False para no escribir el índice de pandas como columna\n",
        "        df_metadata.to_csv(metadata_file_path, sep='\\t', index=False)\n",
        "        print(f\"Archivo '{metadata_file_name}' guardado en Drive.\")\n",
        "        print(f\"Tamaño del archivo de metadatos: {os.path.getsize(metadata_file_path)} bytes\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al guardar el archivo de metadatos en Drive: {e}\")\n",
        "\n",
        "\n",
        "    print(\"\\nArchivos .tsv para TensorFlow Projector generados y guardados.\")\n",
        "    print(\"\\nRecomendación para TensorFlow Projector:\")\n",
        "    print(f\"Los archivos '{vectors_file_name}' y '{metadata_file_name}' ahora están en tu Google Drive en la ruta: {drive_output_path}\")\n",
        "    print(\"Puedes ir a Google Drive, navegar a esa carpeta, y descargar los archivos desde allí.\")\n",
        "    print(\"Luego, ve a https://projector.tensorflow.org/ y sigue estos pasos:\")\n",
        "    print(\"1. Haz clic en el botón 'Load' en el panel izquierdo.\")\n",
        "    print(\"2. Selecciona 'Choose files'.\")\n",
        "    print(f\"3. Carga el archivo de vectores: '{vectors_file_name}'\")\n",
        "    print(f\"4. Carga el archivo de metadatos: '{metadata_file_name}'\")\n",
        "    print(\"\\nUna vez cargados, verás los puntos en el espacio 3D (o 2D si usaste esa opción).\")\n",
        "    print(\"Puedes colorear los puntos según la columna 'sentiment_label' para ver si PCA separó las clases de sentimiento.\")\n",
        "    print(\"Con un dataset más grande, podrías explorar clusters de puntos que correspondan a temas o tipos de reseñas.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudieron crear los archivos .tsv. Asegúrate de que los embeddings reducidos, las etiquetas y la ruta de Drive sean válidos.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb90e0c5"
      },
      "source": [
        "# --- Imports necesarios ---\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "from sklearn.decomposition import PCA\n",
        "import time # Importar time para posibles delays (aunque intentaremos evitarlo)\n",
        "\n",
        "\n",
        "# --- Asegurar que los recursos de NLTK y stop words estén descargados ---\n",
        "# Mover las descargas al inicio de la celda para mayor seguridad\n",
        "print(\"Iniciando verificación/descarga de recursos de NLTK...\")\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    print(\"Descargando recurso 'punkt'...\")\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    print(\"Descargando recurso 'wordnet'...\")\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/omw-1.4')\n",
        "except LookupError:\n",
        "    print(\"Descargando recurso 'omw-1.4'...\")\n",
        "    nltk.download('omw-1.4')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except LookupError:\n",
        "    print(\"Descargando recurso 'averaged_perceptron_tagger'...\")\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    print(\"Descargando recurso 'stopwords'...\")\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "print(\"Verificación/Descarga de recursos de NLTK completada.\")\n",
        "\n",
        "# --- Forzar la carga de recursos de NLTK después de la descarga ---\n",
        "# Intentar usar los recursos inmediatamente después de la descarga para forzar su carga en memoria\n",
        "try:\n",
        "    word_tokenize(\"test sentence.\")\n",
        "    nltk.pos_tag([\"test\"])\n",
        "    stopwords.words('english')\n",
        "    WordNetLemmatizer().lemmatize(\"testing\")\n",
        "    print(\"Recursos de NLTK cargados exitosamente en runtime.\")\n",
        "except LookupError as e:\n",
        "    print(f\"Error: Los recursos de NLTK no están disponibles después de la descarga. {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error inesperado al intentar cargar recursos de NLTK: {e}\")\n",
        "\n",
        "\n",
        "# --- Configuración de rutas en Google Drive ---\n",
        "# Asegúrate de que Google Drive esté montado (ejecutando la celda del Paso 0)\n",
        "drive_base_path = '/content/drive/MyDrive/Universidad Distrital/Diplomado_AI/Semana1'\n",
        "output_data_subdir = 'DataSets'\n",
        "drive_output_path = os.path.join(drive_base_path, output_data_subdir)\n",
        "\n",
        "# Crear la carpeta de destino si no existe\n",
        "if not os.path.exists(drive_output_path):\n",
        "    try:\n",
        "        os.makedirs(drive_output_path)\n",
        "        print(f\"\\nCarpeta de destino en Drive creada: {drive_output_path}\")\n",
        "    except OSError as e:\n",
        "        print(f\"\\nError: No se pudo crear la carpeta en Drive {drive_output_path}. Asegúrate de que Drive esté montado y la ruta base sea válida.\")\n",
        "        drive_output_path = None # Anular la ruta si falla la creación\n",
        "\n",
        "\n",
        "# --- Función de preprocesamiento de texto (limpieza, lematización, stop words) ---\n",
        "# Redefinir la función para asegurar que esté disponible y use los recursos descargados\n",
        "# Las variables stop_words y lemmatizer se inicializan aquí, después de la descarga/carga forzada\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character used by WordNetLemmatizer, handles LookupError\"\"\"\n",
        "    try:\n",
        "        # Asegurar que los recursos necesarios para pos_tag estén disponibles\n",
        "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "        tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "        return tag_dict.get(tag, wordnet.NOUN) # Default to Noun\n",
        "    except LookupError:\n",
        "         # print(\"Recursos de NLTK para POS tagging no encontrados en runtime.\") # Evitar spam en output\n",
        "         return wordnet.NOUN\n",
        "    except IndexError:\n",
        "         # print(\"Error de indexación en pos_tag.\") # Evitar spam en output\n",
        "         return wordnet.NOUN\n",
        "\n",
        "def clean_and_lemmatize_text(text_input):\n",
        "    # Convertir a string de Python si es tensor o bytes\n",
        "    if isinstance(text_input, tf.Tensor):\n",
        "         if tf.size(text_input) > 0 and text_input.dtype == tf.string:\n",
        "             text = text_input.numpy().decode('utf-8', errors='ignore')\n",
        "         else:\n",
        "             return \"\"\n",
        "    elif isinstance(text_input, bytes):\n",
        "         text = text_input.decode('utf-8', errors='ignore')\n",
        "     # Handle potential NaN float input if apply is used on a Series with NaNs\n",
        "    elif isinstance(text_input, float) and pd.isna(text_input):\n",
        "        return \"\" # Return empty string for NaN inputs\n",
        "    else: # Asumir que ya es un string\n",
        "         text = str(text_input)\n",
        "\n",
        "    # Limpieza básica\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenización\n",
        "    try:\n",
        "        tokens = word_tokenize(text)\n",
        "    except LookupError:\n",
        "        # print(\"Recurso 'punkt' de NLTK no encontrado en runtime. No se realizará tokenización.\") # Evitar spam en output\n",
        "        return \"\"\n",
        "\n",
        "    # Eliminar Stop Words y Lematizar\n",
        "    processed_tokens = []\n",
        "    for word in tokens:\n",
        "        # Ensure word is not empty and not a stop word\n",
        "        if word and word not in stop_words:\n",
        "            lemma = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
        "            processed_tokens.append(lemma)\n",
        "\n",
        "    return \" \".join(processed_tokens) # Retornar un string con los tokens unidos\n",
        "\n",
        "\n",
        "# --- Cargar una MUESTRA del dataset original y procesar ---\n",
        "# Usaremos una muestra para que la ejecución sea rápida\n",
        "sample_size_for_processing = 1000 # Tamaño de la muestra para esta sección\n",
        "\n",
        "print(f\"\\nCargando y procesando una muestra de {sample_size_for_processing} reseñas del dataset original...\")\n",
        "\n",
        "try:\n",
        "    # Cargar el dataset si no está ya en memoria\n",
        "    try:\n",
        "        dataset # Check if dataset variable exists\n",
        "        print(\"Utilizando el dataset 'imdb_reviews' cargado previamente.\")\n",
        "    except NameError:\n",
        "        print(\"Dataset 'imdb_reviews' no encontrado. Intentando cargarlo...\")\n",
        "        dataset, info = tfds.load('imdb_reviews', split='train', with_info=True, as_supervised=True)\n",
        "        print(\"Dataset 'imdb_reviews' cargado exitosamente.\")\n",
        "\n",
        "    # Procesar la muestra\n",
        "    processed_texts_sample = []\n",
        "    labels_sample = []\n",
        "\n",
        "    # Iterar sobre una muestra del dataset\n",
        "    for text_tensor, label_tensor in dataset.take(sample_size_for_processing):\n",
        "         processed_text = clean_and_lemmatize_text(text_tensor)\n",
        "         processed_texts_sample.append(processed_text)\n",
        "         labels_sample.append(label_tensor.numpy())\n",
        "\n",
        "    print(f\"Procesamiento de la muestra completo. Número de reseñas procesadas: {len(processed_texts_sample)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar o procesar la muestra del dataset: {e}\")\n",
        "    processed_texts_sample = []\n",
        "    labels_sample = []\n",
        "\n",
        "\n",
        "# --- Guardar la muestra procesada con etiquetas en un CSV en Drive ---\n",
        "if processed_texts_sample and drive_output_path is not None:\n",
        "    sample_csv_filename = 'imdb_reviews_processed_sample_with_labels.csv'\n",
        "    sample_csv_filepath = os.path.join(drive_output_path, sample_csv_filename)\n",
        "\n",
        "    print(f\"\\nGuardando muestra procesada con etiquetas en Drive: {sample_csv_filepath}\")\n",
        "\n",
        "    df_processed_sample = pd.DataFrame({\n",
        "        'processed_text': processed_texts_sample,\n",
        "        'label': labels_sample\n",
        "    })\n",
        "\n",
        "    try:\n",
        "        df_processed_sample.to_csv(sample_csv_filepath, index=False)\n",
        "        print(f\"Archivo '{sample_csv_filename}' guardado exitosamente en Drive.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al guardar el archivo CSV en Drive: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo guardar la muestra procesada en CSV. Asegúrate de que el procesamiento fue exitoso y la ruta de Drive es válida.\")\n",
        "\n",
        "\n",
        "# --- Cargar el CSV de muestra procesada desde Drive ---\n",
        "if drive_output_path is not None:\n",
        "    sample_csv_filename = 'imdb_reviews_processed_sample_with_labels.csv' # Asegurarse de usar el mismo nombre\n",
        "    sample_csv_filepath = os.path.join(drive_output_path, sample_csv_filename)\n",
        "\n",
        "    print(f\"\\nCargando muestra procesada con etiquetas desde Drive: {sample_csv_filepath}\")\n",
        "    try:\n",
        "        df_loaded_sample = pd.read_csv(sample_csv_filepath)\n",
        "        print(f\"Dataset '{sample_csv_filename}' cargado exitosamente.\")\n",
        "        print(f\"Forma del DataFrame cargado: {df_loaded_sample.shape}\")\n",
        "\n",
        "        # Extraer texto y etiquetas\n",
        "        if 'processed_text' in df_loaded_sample.columns and 'label' in df_loaded_sample.columns:\n",
        "            loaded_texts_sample = df_loaded_sample['processed_text'].tolist()\n",
        "            loaded_labels_sample = df_loaded_sample['label'].tolist()\n",
        "            # Manejar posibles NaN en texto si read_csv los introdujo\n",
        "            loaded_texts_sample = [text if isinstance(text, str) else \"\" for text in loaded_texts_sample]\n",
        "            print(f\"Extraídos {len(loaded_texts_sample)} textos y {len(loaded_labels_sample)} etiquetas.\")\n",
        "        else:\n",
        "             print(\"Error: Las columnas esperadas ('processed_text', 'label') no fueron encontradas en el CSV cargado.\")\n",
        "             loaded_texts_sample = []\n",
        "             loaded_labels_sample = []\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: El archivo '{sample_csv_filepath}' no fue encontrado al intentar cargarlo.\")\n",
        "        loaded_texts_sample = []\n",
        "        loaded_labels_sample = []\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar o procesar el archivo CSV desde Drive: {e}\")\n",
        "        loaded_texts_sample = []\n",
        "        loaded_labels_sample = []\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo cargar la muestra procesada desde Drive. Ruta de Drive no válida.\")\n",
        "    loaded_texts_sample = []\n",
        "    loaded_labels_sample = []\n",
        "\n",
        "\n",
        "# --- Generar Embeddings con Transformer para la muestra cargada ---\n",
        "if loaded_texts_sample:\n",
        "    # Cargar un modelo y tokenizador pre-entrenados\n",
        "    model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "    print(f\"\\nCargando tokenizador y modelo Transformer: {model_name}\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModel.from_pretrained(model_name)\n",
        "        print(\"Tokenizador y modelo cargados.\")\n",
        "\n",
        "        # Tokenizar los textos\n",
        "        print(\"\\nTokenizando textos para Transformer...\")\n",
        "        # Use padding=True for dynamic padding, truncation=True to handle long texts\n",
        "        inputs = tokenizer(loaded_texts_sample, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        print(\"Tokenización completa. Forma de los inputs (input_ids):\", inputs['input_ids'].shape)\n",
        "\n",
        "        # Obtener los embeddings del modelo\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.to(device)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        print(f\"\\nObteniendo embeddings del modelo Transformer en {device}...\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # Obtener el embedding del token [CLS]\n",
        "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "        cls_embeddings_np = cls_embeddings.cpu().numpy()\n",
        "        print(\"Obtención de embeddings completa.\")\n",
        "        print(f\"Forma de los embeddings [CLS]: {cls_embeddings_np.shape}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el modelo Transformer o generar embeddings: {e}\")\n",
        "        cls_embeddings_np = None\n",
        "else:\n",
        "    print(\"\\nNo se pudo generar embeddings. No hay textos cargados disponibles.\")\n",
        "    cls_embeddings_np = None\n",
        "\n",
        "\n",
        "# --- Aplicar PCA para Reducción de Dimensionalidad (a 3D para Projector) ---\n",
        "if cls_embeddings_np is not None:\n",
        "    print(\"\\nAplicando PCA para reducir la dimensionalidad a 3 componentes...\")\n",
        "    try:\n",
        "        pca_3d = PCA(n_components=3, random_state=42)\n",
        "        embeddings_pca_3d = pca_3d.fit_transform(cls_embeddings_np)\n",
        "        print(\"Reducción de dimensionalidad con PCA (3D) completa.\")\n",
        "        print(f\"Forma de los embeddings PCA 3D: {embeddings_pca_3d.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al aplicar PCA: {e}\")\n",
        "        embeddings_pca_3d = None\n",
        "else:\n",
        "    print(\"\\nNo se pudo aplicar PCA. Los embeddings no están disponibles.\")\n",
        "    embeddings_pca_3d = None\n",
        "\n",
        "\n",
        "# --- Preparar y guardar archivos .tsv para TensorFlow Projector ---\n",
        "if embeddings_pca_3d is not None and loaded_labels_sample and drive_output_path is not None:\n",
        "    print(\"\\nPreparando archivos .tsv para TensorFlow Projector...\")\n",
        "\n",
        "    # Asegurarse de que el número de embeddings y etiquetas coincida\n",
        "    if embeddings_pca_3d.shape[0] == len(loaded_labels_sample):\n",
        "\n",
        "        # --- Preparar y guardar el archivo vectors.tsv ---\n",
        "        vectors_file_name = 'transformer_pca_vectors_sample.tsv' # Nombre específico\n",
        "        vectors_file_path = os.path.join(drive_output_path, vectors_file_name)\n",
        "        print(f\"Creando archivo de vectores en Drive: {vectors_file_path}\")\n",
        "        try:\n",
        "            np.savetxt(vectors_file_path, embeddings_pca_3d, delimiter='\\t')\n",
        "            print(f\"Archivo '{vectors_file_name}' guardado exitosamente.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al guardar el archivo de vectores en Drive: {e}\")\n",
        "\n",
        "\n",
        "        # --- Preparar y guardar el archivo metadata.tsv ---\n",
        "        metadata_file_name = 'transformer_pca_metadata_sample.tsv' # Nombre específico\n",
        "        metadata_file_path = os.path.join(drive_output_path, metadata_file_name)\n",
        "        print(f\"\\nCreando archivo de metadatos en Drive: {metadata_file_path}\")\n",
        "\n",
        "        # Crear DataFrame de metadatos (texto original, texto procesado, etiqueta, etc.)\n",
        "        # Para este ejemplo, solo incluiremos la etiqueta y el texto procesado\n",
        "        df_metadata_projector = pd.DataFrame({\n",
        "            'sentiment_label': loaded_labels_sample,\n",
        "            'processed_text': loaded_texts_sample # Incluir texto procesado para inspección en Projector\n",
        "            # Puedes añadir otras columnas si las cargas del CSV original de metadatos y las alineas\n",
        "        })\n",
        "\n",
        "        try:\n",
        "            # Guardar el DataFrame de metadatos en formato .tsv en Drive\n",
        "            df_metadata_projector.to_csv(metadata_file_path, sep='\\t', index=False) # index=False para no escribir el índice\n",
        "            print(f\"Archivo '{metadata_file_name}' guardado exitosamente.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al guardar el archivo de metadatos en Drive: {e}\")\n",
        "\n",
        "        print(\"\\nArchivos .tsv para TensorFlow Projector generados y guardados.\")\n",
        "        print(\"\\nRecomendación para TensorFlow Projector:\")\n",
        "        print(f\"Los archivos '{vectors_file_name}' y '{metadata_file_name}' ahora están en tu Google Drive en la ruta: {drive_output_path}\")\n",
        "        print(\"Puedes ir a Google Drive, navegar a esa carpeta, y descargar los archivos desde allí.\")\n",
        "        print(\"Luego, ve a https://projector.tensorflow.org/ y sigue estos pasos:\")\n",
        "        print(\"1. Haz clic en el botón 'Load' en el panel izquierdo.\")\n",
        "        print(\"2. Selecciona 'Choose files'.\")\n",
        "        print(f\"3. Carga el archivo de vectores: '{vectors_file_name}'\")\n",
        "        print(f\"4. Carga el archivo de metadatos: '{metadata_file_name}'\")\n",
        "        print(\"\\nUna vez cargados, verás los puntos en el espacio 3D.\")\n",
        "        print(\"Puedes colorear los puntos según la columna 'sentiment_label' para ver si PCA separó las clases de sentimiento.\")\n",
        "        print(\"Al hacer clic en un punto, podrás ver el 'processed_text' asociado en el panel lateral.\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"Error: El número de embeddings reducidos no coincide con el número de etiquetas cargadas.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudieron crear los archivos .tsv. Asegúrate de que los embeddings reducidos y las etiquetas estén disponibles y la ruta de Drive sea válida.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}